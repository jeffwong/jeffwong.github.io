<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>From First Principles</title>
<link>https://jeffwong.github.io/</link>
<atom:link href="https://jeffwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Data science innovation from first principles.</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Mon, 15 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Errors in Experiments</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/errors_in_experiments/</link>
  <description><![CDATA[ 





<p>An experiment can make many errors. What error do we care about? How do we design an experiment that can make guarantees that those errors are small? While a randomized and controlled experiment lets us report an unbiased estimate of the treatment effect, the reported effect may still</p>
<ol type="1">
<li>Have the wrong sign.</li>
<li>Have an exagerated effect size.</li>
<li>Be a false positive.</li>
</ol>
<p>We’ll describe each case from first principles, and how to design an experiment when one of the errors is especially important.</p>
<section id="sign-error" class="level1">
<h1>Sign Error</h1>
<p><a href="https://sites.stat.columbia.edu/gelman/research/published/retropower_final.pdf">Andrew Gelman</a> says that we should not only plan an experiment for power, we should also plan to control for a type S error as well as an exaggeration factor. A type S error is when we flag stat sig, but the estimated effect has the wrong sign. The probability of a type S error conditions on stat sig, where the denominator is power and the numerator has</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0APower%20&amp;=%20%5CPhi(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20%5C%5C%0AP(%5Ctext%7BSign%20Error%7D%20%7C%20Sig)%20&amp;=%20%5Cfrac%7B%5CPhi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%7D%7B%5CPhi(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%7D%0A%5Cend%7Balign%7D"></p>
<p>Power is a function of the ncp, and <img src="https://latex.codecogs.com/png.latex?%5Calpha">. The probability of a sign error is a function of the same variables. So they can be computed at the same time. Below we will show that the exaggeration ratio can also be computed at the same time.</p>
</section>
<section id="exaggeration-type-m-error" class="level1">
<h1>Exaggeration (Type M Error)</h1>
<p>When we publish only stat sig results, we run into a winner’s curse problem. The results will tend to be exaggerated. The mere fact that a collection of results are all stat sig is correlated with the results all having large effect sizes. Remember that the underlying effect has randomness around it, and can appear in data as a large effect or a small effect. Conditioning our reporting on stat sig results means we are masking the small effect, creating the incorrect perception that the underlying effect is large. The exaggeration factor tries to measure the difference between the reported effect and the underlying effect.</p>
<p>Say that we measure a treatment effect <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5CDelta%7D"> with standard error <img src="https://latex.codecogs.com/png.latex?se(%5Chat%7B%5CDelta%7D)">. Without loss of generality, say <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%3E%200">, and since we condition on stat sig we also have <img src="https://latex.codecogs.com/png.latex?Z%20%5Cgeq%20z_%7B1%20-%20%5Calpha/2%7D">. The estimated effect size can be expressed in terms of standard errors <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5CDelta%7D%20=%20Z%20se(%5Chat%7B%5CDelta%7D)">. The exaggeration is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AM%20&amp;=%20E%5B%5Cfrac%7B%5Chat%7B%5CDelta%7D%7D%7B%5CDelta%7D%20%7C%20Z%20%5Cgeq%20z_%7B1%20-%20%5Calpha/2%7D%5D%5C%5C%0A%20%20&amp;=%20%5Cfrac%7Bse(%5Chat%7B%5CDelta%7D)%7D%7B%5CDelta%7D%20E%5BZ%20%7C%20Z%20%5Cgeq%20z_%7B1%20-%20%5Calpha/2%7D%5D%0A%5Cend%7Balign%7D"></p>
<p>We know that <img src="https://latex.codecogs.com/png.latex?Z%20%5Csim%20N(ncp,%201)"> itself is a normally distributed variable, and the portion of this distribution with <img src="https://latex.codecogs.com/png.latex?Z%20%5Cgeq%20z_%7B1%20-%20%5Calpha/2%7D"> is a truncated normal. The expectation of this truncated normal is <img src="https://latex.codecogs.com/png.latex?ncp%20+%20%5Cfrac%7B%5Cphi(z_%7B1-%5Calpha/2%7D%20-%20ncp)%7D%7B1%20-%20%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20ncp)%7D">. So the final expression for the exaggeration is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7BM%20=%201%20+%20%5Cfrac%7B%5Cphi(z_%7B1-%5Calpha/2%7D%20-%20ncp)%7D%7Bncp(1%20-%20%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20ncp)%7D%7D%0A"></p>
<p>An interesting thing to note is that the exaggeration decreases when <img src="https://latex.codecogs.com/png.latex?%5Calpha"> increases. When we open up <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and allow more results to be flagged as stat sig, then the winner’s curse problem naturally dissipates, reducing the exaggeration.</p>
<p>Type M is also a function of only the ncp and <img src="https://latex.codecogs.com/png.latex?%5Calpha">, so can be computed at the same time as power and type S.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">retrodesign <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(ncp, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>) {</span>
<span id="cb1-2">  crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-3">  power <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ncp)</span>
<span id="cb1-4">  sign_risk <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> power</span>
<span id="cb1-5">  exaggeration_numerator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dnorm</span>(crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ncp)</span>
<span id="cb1-6">  exaggeration_denominator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> ncp <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ncp))</span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(</span>
<span id="cb1-8">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncp =</span> ncp, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> alpha,</span>
<span id="cb1-9">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">power =</span> power, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sign_risk =</span> sign_risk, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">exaggeration =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> exaggeration_numerator <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> exaggeration_denominator</span>
<span id="cb1-10">  )</span>
<span id="cb1-11">}</span></code></pre></div></div>
</div>
</section>
<section id="false-positive-risk" class="level1">
<h1>False Positive Risk</h1>
<p>We can use the formula for <a href="https://exp-platform.com/abtestingintuitionbusters/">false positive risk</a> (FPR) to estimate a probability that a stat sig result is actually a false positive. For a test with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, 80% power, and a 80% chance that the null hypothesis is true, the FPR is 20% This is an important risk to understand, and it is frequently confused with <img src="https://latex.codecogs.com/png.latex?%5Calpha">. Many people think that <img src="https://latex.codecogs.com/png.latex?%5Calpha"> controls the false positive rate among significant results, but actually what it controls is a false positive rate under the null hypothesis. The FPR is actually what most people think of when they think of the false positive risk.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Ctext%7BP(H0%20is%20true%20%7C%20stat%20sig)%7D%20&amp;=%20%5Ctext%7BP(stat%20sig%20%7C%20H0%20is%20true)%7D%20%5Ccdot%20%5Cfrac%7B%5Ctext%7BP(H0%20is%20true)%7D%7D%7B%5Ctext%7BP(stat%20sig)%7D%7D%20%5C%5C%0A%20%20&amp;=%20%5Cfrac%7B%5Calpha%20%5Ctext%7BP(H0%20is%20true)%7D%7D%7B%5Calpha%20%5Ctext%7BP(H0%20is%20true)%7D%20+%20(1%20-%20%5Cbeta)%20(1%20-%20%5Ctext%7BP(H0%20is%20true)%7D)%7D%0A%5Cend%7Balign%7D"></p>
<p>Unlike type S or type M functions, FPR is normally written in terms of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and power. However, power is a function of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and ncp. So we can cast FPR to look like it has similar arguments to type S and type M functions. This casting is going to be very convenient later.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Traditional implementation of fpr</span></span>
<span id="cb2-2">fpr_traditional <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(prior_h0_true, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">power =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>) {</span>
<span id="cb2-3">  numerator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> alpha <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior_h0_true</span>
<span id="cb2-4">  denominator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> alpha <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior_h0_true <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> power <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prior_h0_true)</span>
<span id="cb2-5">  numerator<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>denominator</span>
<span id="cb2-6">}</span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># More convenient implementation of fpr</span></span>
<span id="cb2-8">fpr_alternative <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(ncp, prior_h0_true, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>) {</span>
<span id="cb2-9">  crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-10">  power <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ncp)</span>
<span id="cb2-11">  numerator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> alpha <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior_h0_true</span>
<span id="cb2-12">  denominator <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> alpha <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior_h0_true <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> power <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prior_h0_true)</span>
<span id="cb2-13">  numerator<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>denominator</span>
<span id="cb2-14">}</span>
<span id="cb2-15"></span>
<span id="cb2-16"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(</span>
<span id="cb2-17">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_h0_true =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">from =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">to =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb2-18">) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb2-20">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fpr =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fpr_traditional</span>(prior_h0_true)</span>
<span id="cb2-21">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(</span>
<span id="cb2-23">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> prior_h0_true, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> fpr)</span>
<span id="cb2-24">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-25">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-26">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_bw</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">base_size =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">xlab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Prior that H0 is True"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb2-28">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ylab</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"False Positive Risk"</span>)</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/errors_in_experiments/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="contrast-in-experimental-design" class="level1">
<h1>Contrast in Experimental Design</h1>
<p>So far we have described many different types of errors that can occur while designing an experiment. We tend to take for granted why conventional wisdom says to plan a test to achieve 80% power while holding <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">. We will explore why those values were chosen and what properties do they yield.</p>
<p>Practitioners tend to say we need to design a high powered test. If we wanted to simply achieve high power, we could easily achieve that by setting <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201">. Then every result, regardless of effect size, is flagged as stat sig with power = 100%. However, there would be many false positives, and there would be no clear understanding of which results to report. So power, false positives, sign errors, and exaggeration all need to be balanced. Is pinning <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05"> and power = 80% the right balance? We will explore that.</p>
<p>To make the study simple, we note that power, FPR, sign errors, and exaggeration can all be computed by using the planned ncp, <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and the prior that the null is true. We will scan 3 inputs and study how the 4 design statistics vary.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">exp_design_statistics <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncp =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.8</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_h0_true =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>) {</span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cbind</span>(</span>
<span id="cb3-3">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">retrodesign</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncp =</span> ncp, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> alpha),</span>
<span id="cb3-4">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fpr =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">fpr_alternative</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncp =</span> ncp, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_h0_true =</span> prior_h0_true, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> alpha)</span>
<span id="cb3-5">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb3-6">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb3-7">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">required_n_c =</span> ncp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-8">    )</span>
<span id="cb3-9">}</span></code></pre></div></div>
</div>
<section id="traditional-designs" class="level2">
<h2 class="anchored" data-anchor-id="traditional-designs">Traditional Designs</h2>
<p>Say that we want to be able to detect a effect of 0.1. The standard deviation of the outcome is 1. We pin <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200.2">. Data is randomly split 50/50 between treatment and control.</p>
<p>The amount of data needed was described in <a href="https://jeffwong.github.io/posts/statistical_power/">Foundations of Statistical Power</a></p>
<p><img src="https://latex.codecogs.com/png.latex?n_C%20=%20n_T%20=%202%20%5Cfrac%7B(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%5E2%20%5Csigma%5E2%7D%7BMDE%5E2%7D"></p>
<p>which produces a <img src="https://latex.codecogs.com/png.latex?n_C%20+%20n_T%20%5Capprox%2032%20%5Cfrac%7B1%7D%7B(.1)%5E2%7D%20=%203200"> To confirm this design has 80% power, we can verify in code</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">n_c <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1600</span></span>
<span id="cb4-2">k <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb4-3">delta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb4-4">alpha <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span></span>
<span id="cb4-5">sigma2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">pooled_se <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(sigma2 <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_c <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>k))</span>
<span id="cb4-8">pooled_ncp <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> delta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pooled_se</span>
<span id="cb4-9"></span>
<span id="cb4-10">crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb4-11">power <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pooled_ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> pooled_ncp))</span>
<span id="cb4-12"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">print</span>(power)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8074304</code></pre>
</div>
</div>
<p>We can learn much more about this particular design other than that it achieves 80% power. It also has a <img src="https://latex.codecogs.com/png.latex?%5Capprox%200">% sign error rate, an exaggeration of <img src="https://latex.codecogs.com/png.latex?1.125"> and false positive risk of 6% when the prior for the null is 50/50. This is a good design.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">exp_design_statistics</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ncp =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.8</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prior_h0_true =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>  ncp alpha     power    sign_risk exaggeration        fpr required_n_c
1 2.8  0.05 0.7995569 1.210843e-06     1.125219 0.05885421         1568</code></pre>
</div>
</div>
<p>However there are other configurations that can also yield 80% power. If we allow <img src="https://latex.codecogs.com/png.latex?%5Calpha"> to vary, then any configuration that yields an ncp of <img src="https://latex.codecogs.com/png.latex?(z_%7B1%20-%20%5Calpha/2%7D%20-%20z_%5Cbeta)"> will also achieve 80%. We want to explore other configurations and see how type S errors, exaggeration, and fpr change.</p>
</section>
<section id="alternative-design" class="level2">
<h2 class="anchored" data-anchor-id="alternative-design">Alternative Design</h2>
<p>In this section we explore alternative configurations to experimental design. We will let <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and the ncp vary, and compute the changes to power, FPR, type S error rates, and exaggeration. We will do this with a 50/50 prior that the null is true. The results are charted below.</p>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using method = 'loess' and formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/errors_in_experiments/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using method = 'loess' and formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/errors_in_experiments/index_files/figure-html/unnamed-chunk-6-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using method = 'loess' and formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/errors_in_experiments/index_files/figure-html/unnamed-chunk-6-3.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/errors_in_experiments/index_files/figure-html/unnamed-chunk-6-4.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There are patterns in the charts that are noteworthy:</p>
<ol type="1">
<li>Power increases as ncp increases. Power also increases as <img src="https://latex.codecogs.com/png.latex?%5Calpha"> increases. Power is very sensitive to ncp in the range <img src="https://latex.codecogs.com/png.latex?ncp%20%5Cin%20%5B0.5,%202.5%5D">, small improvements to the ncp can change power by a lot. In order to achieve 80% statistical power, we must plan for <img src="https://latex.codecogs.com/png.latex?ncp%20%5Cgeq%201.5"> or higher depending on <img src="https://latex.codecogs.com/png.latex?%5Calpha">.\</li>
<li>Once <img src="https://latex.codecogs.com/png.latex?ncp%20%5Cgeq%201.5">, sign error rates are fairly small, regardless of <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
<li>FPR decreases as ncp increases. FPR also increases when <img src="https://latex.codecogs.com/png.latex?%5Calpha"> increases, holding ncp constant. When <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is small, the FPR is sensitive to small changes in the ncp, but FPR can be flat when <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is large. To have <img src="https://latex.codecogs.com/png.latex?FPR%20%5Cleq%200.2">, we need <img src="https://latex.codecogs.com/png.latex?ncp%20%5Cgeq%201.1"> or higher depending on <img src="https://latex.codecogs.com/png.latex?%5Calpha">. Similarly to have <img src="https://latex.codecogs.com/png.latex?FPR%20%5Cleq%200.1"> we need <img src="https://latex.codecogs.com/png.latex?ncp%20%5Cgeq%201.9"> or higher.</li>
<li>Exaggeration decreases as ncp increases. It also decreases when <img src="https://latex.codecogs.com/png.latex?%5Calpha"> increases. Achieving <img src="https://latex.codecogs.com/png.latex?M%20%5Cleq%201.1"> can be very hard, requiring ncp to be between 2.0 and 2.8. At high values of ncp, exaggeration is not sensitive to <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
</ol>
<p>All of these charts are nonlinear curves. There are diminishing returns to increasing the ncp. Yet, a doubling of the ncp requires a 4x in the data volume, since <img src="https://latex.codecogs.com/png.latex?ncp%20=%20%5Cfrac%7B%5CDelta%7D%7Bse(%5Chat%7B%5CDelta%7D)%7D">. Below is a table of configurations that range from “good” to “reasonably acceptable” to “bad”, and how data volume vaaries the quality.</p>
<p>The standard practice of <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, and power = 80%, is a good practice. Many others actually even advocate for more data. However, it is important to understand how we arrived here, and whether there is flexibility when designing your own experiment. Understanding the first principles is crucial.</p>
<div class="cell">
<div class="cell-output-display">
<div id="ishjwoznvo" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#ishjwoznvo table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#ishjwoznvo thead, #ishjwoznvo tbody, #ishjwoznvo tfoot, #ishjwoznvo tr, #ishjwoznvo td, #ishjwoznvo th {
  border-style: none;
}

#ishjwoznvo p {
  margin: 0;
  padding: 0;
}

#ishjwoznvo .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#ishjwoznvo .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#ishjwoznvo .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#ishjwoznvo .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#ishjwoznvo .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ishjwoznvo .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ishjwoznvo .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#ishjwoznvo .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#ishjwoznvo .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#ishjwoznvo .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#ishjwoznvo .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#ishjwoznvo .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#ishjwoznvo .gt_spanner_row {
  border-bottom-style: hidden;
}

#ishjwoznvo .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#ishjwoznvo .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#ishjwoznvo .gt_from_md > :first-child {
  margin-top: 0;
}

#ishjwoznvo .gt_from_md > :last-child {
  margin-bottom: 0;
}

#ishjwoznvo .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#ishjwoznvo .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#ishjwoznvo .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#ishjwoznvo .gt_row_group_first td {
  border-top-width: 2px;
}

#ishjwoznvo .gt_row_group_first th {
  border-top-width: 2px;
}

#ishjwoznvo .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ishjwoznvo .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#ishjwoznvo .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#ishjwoznvo .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ishjwoznvo .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#ishjwoznvo .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#ishjwoznvo .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#ishjwoznvo .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#ishjwoznvo .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#ishjwoznvo .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ishjwoznvo .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ishjwoznvo .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#ishjwoznvo .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#ishjwoznvo .gt_left {
  text-align: left;
}

#ishjwoznvo .gt_center {
  text-align: center;
}

#ishjwoznvo .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#ishjwoznvo .gt_font_normal {
  font-weight: normal;
}

#ishjwoznvo .gt_font_bold {
  font-weight: bold;
}

#ishjwoznvo .gt_font_italic {
  font-style: italic;
}

#ishjwoznvo .gt_super {
  font-size: 65%;
}

#ishjwoznvo .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#ishjwoznvo .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#ishjwoznvo .gt_indent_1 {
  text-indent: 5px;
}

#ishjwoznvo .gt_indent_2 {
  text-indent: 10px;
}

#ishjwoznvo .gt_indent_3 {
  text-indent: 15px;
}

#ishjwoznvo .gt_indent_4 {
  text-indent: 20px;
}

#ishjwoznvo .gt_indent_5 {
  text-indent: 25px;
}

#ishjwoznvo .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#ishjwoznvo div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>

<table class="gt_table caption-top table table-sm table-striped small" data-quarto-bootstrap="false">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="gt_heading header">
<td colspan="7" class="gt_heading gt_title gt_font_normal gt_bottom_border">Experimentation Design Statistics</td>
</tr>
<tr class="gt_col_headings gt_spanner_row even">
<th colspan="3" id="Design Inputs" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><div class="gt_column_spanner">
Design Inputs
</div></th>
<th colspan="4" id="Design Statistics" class="gt_center gt_columns_top_border gt_column_spanner_outer" data-quarto-table-cell-role="th" scope="colgroup"><div class="gt_column_spanner">
Design Statistics
</div></th>
</tr>
<tr class="gt_col_headings header">
<th id="ncp" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">ncp</th>
<th id="alpha" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">alpha</th>
<th id="required_n_c" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">required_n_c</th>
<th id="power" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" style="border-left-width: 2px; border-left-style: solid; border-left-color: black" scope="col">power</th>
<th id="sign_risk" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">sign_risk</th>
<th id="exaggeration" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">exaggeration</th>
<th id="fpr" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">fpr</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_right" headers="ncp">3.4</td>
<td class="gt_row gt_right" headers="alpha">0.01</td>
<td class="gt_row gt_right" headers="required_n_c">2312</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black; background-color: #00FF00; font-weight: bold">0.80</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration" style="background-color: #00FF00; font-weight: bold">1.11</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.01</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="ncp">2.8</td>
<td class="gt_row gt_right" headers="alpha">0.05</td>
<td class="gt_row gt_right" headers="required_n_c">1568</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black; background-color: #00FF00; font-weight: bold">0.80</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.13</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.06</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="ncp">1.5</td>
<td class="gt_row gt_right" headers="alpha">0.05</td>
<td class="gt_row gt_right" headers="required_n_c">450</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black">0.32</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.74</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.13</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="ncp">1.5</td>
<td class="gt_row gt_right" headers="alpha">0.10</td>
<td class="gt_row gt_right" headers="required_n_c">450</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black">0.44</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.59</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.18</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="ncp">1.5</td>
<td class="gt_row gt_right" headers="alpha">0.20</td>
<td class="gt_row gt_right" headers="required_n_c">450</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black">0.59</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.44</td>
<td class="gt_row gt_right" headers="fpr">0.25</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="ncp">1.5</td>
<td class="gt_row gt_right" headers="alpha">0.50</td>
<td class="gt_row gt_right" headers="required_n_c">450</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black; background-color: #00FF00; font-weight: bold">0.81</td>
<td class="gt_row gt_right" headers="sign_risk">0.02</td>
<td class="gt_row gt_right" headers="exaggeration">1.24</td>
<td class="gt_row gt_right" headers="fpr">0.38</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="ncp">2.0</td>
<td class="gt_row gt_right" headers="alpha">0.05</td>
<td class="gt_row gt_right" headers="required_n_c">800</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black">0.52</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.39</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.09</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="ncp">2.0</td>
<td class="gt_row gt_right" headers="alpha">0.10</td>
<td class="gt_row gt_right" headers="required_n_c">800</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black">0.64</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.29</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.14</td>
</tr>
<tr class="odd">
<td class="gt_row gt_right" headers="ncp">2.0</td>
<td class="gt_row gt_right" headers="alpha">0.20</td>
<td class="gt_row gt_right" headers="required_n_c">800</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black; background-color: #00FF00; font-weight: bold">0.76</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration">1.20</td>
<td class="gt_row gt_right" headers="fpr" style="background-color: #00FF00; font-weight: bold">0.21</td>
</tr>
<tr class="even">
<td class="gt_row gt_right" headers="ncp">2.0</td>
<td class="gt_row gt_right" headers="alpha">0.50</td>
<td class="gt_row gt_right" headers="required_n_c">800</td>
<td class="gt_row gt_right" headers="power" style="border-left-width: 2px; border-left-style: solid; border-left-color: black; background-color: #00FF00; font-weight: bold">0.91</td>
<td class="gt_row gt_right" headers="sign_risk" style="background-color: #00FF00; font-weight: bold">0.00</td>
<td class="gt_row gt_right" headers="exaggeration" style="background-color: #00FF00; font-weight: bold">1.09</td>
<td class="gt_row gt_right" headers="fpr">0.35</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2020/09/LuQiuDeng-BJMSP2019.pdf">A Note on Type S/M Errors in Hypothesis Testing</a> \</li>
<li><a href="https://sites.stat.columbia.edu/gelman/research/published/retropower_final.pdf">Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors</a> \</li>
<li><a href="https://exp-platform.com/abtestingintuitionbusters/">AB Testing Intuition Busters</a></li>
</ol>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/errors_in_experiments/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Posthoc MDE</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/posthoc_mde/</link>
  <description><![CDATA[ 





<section id="references" class="level1">

<ol type="1">
<li>https://blogs.worldbank.org/en/impactevaluations/why-ex-post-power-using-estimated-effect-sizes-bad-ex-post-mde-not</li>
</ol>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/posthoc_mde/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Statistical Power under Noncompliance</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/power_noncompliance/</link>
  <description><![CDATA[ 





<p>This is an extension of a previous post on statistical power. In that post we derived the formula for statistical power using</p>
<ol type="1">
<li>The effect size</li>
<li>The standard error of the effect. Alternatively, this can be stated as the variance and the sample size.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
</ol>
<p>However, there is an assumption of perfect compliance. That is, 100% of the treatment units are actually treated, and 100% of the control units are actually withheld.</p>
<p>In this post we discuss the case when <img src="https://latex.codecogs.com/png.latex?p_1">% of treatment units are treated, and <img src="https://latex.codecogs.com/png.latex?p_0">% of control units are withheld.</p>
<section id="effect-size" class="level1">
<h1>Effect size</h1>
<p>Say that treatment assignment is determined by the variable <img src="https://latex.codecogs.com/png.latex?Z">, so <img src="https://latex.codecogs.com/png.latex?Z%20=%201"> means we intend to provide treatment, and <img src="https://latex.codecogs.com/png.latex?Z%20=%200"> means we intend to withhold. Let <img src="https://latex.codecogs.com/png.latex?X"> be the treatment that was actually received.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0Ap_1%20&amp;=%20P(X%20=%201%20%7C%20Z%20=%201)%20%5C%5C%0Ap_0%20&amp;=%20P(X%20=%200%20%7C%20Z%20=%200)%0A%5Cend%7Balign%7D"></p>
<p>The effect size we care about is <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%20%5Cmu_1%20-%20%5Cmu_0%20=%20E%5By%20%7C%20X%20=%201%5D%20-%20E%5By%20%7C%20X%20=%200%5D">. There is a difference between what we care about and what we will observe. From data, we will observe a dilution in the treatment effect. Noncompliance will decrease the effect size and will subsequently decrease the power.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmu_1%20%7C%20Z%20=%201%20&amp;=%20p_1%20%5Cmu_1%20+%20(1%20-%20p_1)%20%5Cmu_0%20%5C%5C%0A%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20p_0%20%5Cmu_0%20+%20(1%20-%20p_0)%20%5Cmu_1%20%5C%5C%0A%5Cmu_1%20%7C%20Z%20=%201%20-%20%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20(p_1%20+%20p_0%20-%201)%20%5CDelta%0A%5Cend%7Balign%7D"></p>
</section>
<section id="variance" class="level1">
<h1>Variance</h1>
<p>The variance of <img src="https://latex.codecogs.com/png.latex?y"> is broken into</p>
<p><img src="https://latex.codecogs.com/png.latex?Var(y)%20=%20E%5Bvar(y%20%20%7C%20X)%5D%20+%20Var(E%5By%20%7C%20X%5D)"></p>
<p>The first term can be simplified and reduced to just <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. We can argue that the variance of <img src="https://latex.codecogs.com/png.latex?y"> is not a function of the artificially generated assignment variable, <img src="https://latex.codecogs.com/png.latex?Z">. It is only a function of whether or not the unit actually received the treatment. For simplicity, we say that the variance is independent of the treatment received.</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5Bvar(y%20%7C%20X%20=%201)%5D%20+%20E%5Bvar(y%20%7C%20X%20=%200)%5D%20=%20%5Csigma%5E2."> The conditioning on a binary X is like a mixture of bernoulli random variables. In this case the mixture has 2 equal components so it reduces to <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</p>
<p>Next, is the variance of a mixture of bernoulli variables: <img src="https://latex.codecogs.com/png.latex?E%5By%20%7C%20X%20=%200%5D%20=%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?E%5By%20%7C%20X%20=%201%5D%20=%20%5Cmu_1%20=%20%5Cmu_0%20+%20%5CDelta">. The variance of this mixture is a function of the mixing probabilities and the difference in the individual means, which in this case will be <img src="https://latex.codecogs.com/png.latex?%5CDelta">. Then we have the key pieces we need to measure statistical power based on the observed data</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmu_1%20%7C%20Z%20=%201%20&amp;=%20p_1%20%5Cmu_1%20+%20(1%20-%20p_1)%20%5Cmu_0%20%5C%5C%0A%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20p_0%20%5Cmu_0%20+%20(1%20-%20p_0)%20%5Cmu_1%20%5C%5C%0A%5Cmu_1%20%7C%20Z%20=%201%20-%20%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20(p_1%20+%20p_0%20-%201)%20%5CDelta%20%5C%5C%0AVar(y%20%7C%20Z%20=%201)%20&amp;=%20%5Csigma%5E2%20+%20p_1%20(1%20-%20p_1)%20%5CDelta%5E2%20%5Capprox%20%5Csigma%5E2%20%5C%5C%0AVar(y%20%7C%20Z%20=%200)%20&amp;=%20%5Csigma%5E2%20+%20p_0%20(1%20-%20p_0)%20%5CDelta%5E2%20%5Capprox%20%5Csigma%5E2%0A%5Cend%7Balign%7D"></p>
<p>The approximation in <img src="https://latex.codecogs.com/png.latex?Var(y%20%7C%20Z%20=%201)%20%5Capprox%20%5Csigma%5E2"> comes from the pattern that variance (not normalized by <img src="https://latex.codecogs.com/png.latex?n">) tends to be large while effect size tends to be small. Thus variance under noncompliance is not much different than variance under full compliance. The change in power will largely come from the dilution in the treatment effect.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Ctext%7BPower%7D%20=%20%5CPhi(%5Cdelta'%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta'%20-%20z_%7B1-%5Calpha/2%7D)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cdelta'%20=%20%5Cdelta%20(p_1%20+%20p_0%20-%201)"> is a dilution on the treatment effect.</p>
<p>Using some simple numbers, if <img src="https://latex.codecogs.com/png.latex?p_1%20=%20p_0%20=%200.9">, then <img src="https://latex.codecogs.com/png.latex?%5Cdelta'%20=%200.8%20%5Cdelta">. By changing the effect size by 0.8, we change the sample size necessary by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B.8%5E2%7D%20=%2056%5C%25">!</p>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/power_noncompliance/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Foundations in Statistical Power</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/statistical_power/</link>
  <description><![CDATA[ 





<p>Statistical power is a concept that helps us plan for an effective experiment. Power is typically described in terms of a purely randomized and controlled trial, where we can take advantage of certain properties like independence. It is also typically described in the context of measuring the difference in means, which also has properties like the Central Limit Theorem that it takes advantage of. Finally, an assumption in typical description is that all units that are assigned to the treatment group are successfully treated, and all units assigned to the control are successfully withheld.</p>
<p>But not all experiments are so perfectly planned and executed. It is important to understand the history of how statistical power was derived, from first principles, so that we can adapt it and innovate. For example, some experiments do not have perfect compliance. Many experiments will try to give treatment to a subject, but that subject refuses, or forgets, to use the treatment. This complexity can add layers to the question: “what should we measure? The average effect or the average effect among the compliers?” With this complication we will need to revisit our formula for statistical power. We will do this innovation exercise in the next post. For now, let us understand the history of statistical power.</p>
<hr>
<p>Power is always associated with a hypothesis test. <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BPower%7D%20=%201%20-%20%5Cbeta%20=%20P(%5Ctext%7BReject%20%7DH_0%20%7C%20H_A)"> where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the type 1 error rate, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the type 2 error rate.</p>
<p>So to derive power, we must first identify the rejection rules of the hypothesis test, then evaluate the probability of rejection if the alternative is true.</p>
<p>To illustrate, we specify a very generic hypothesis test, the difference in means. While frequently associated with the t test, it is not necessarily tied to it. The hypothesis about the difference in means is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AH_0:%20%5Cmu_1%20-%20%5Cmu_0%20&amp;=%200%20%5C%5C%0AH_A:%20%5Cmu_1%20-%20%5Cmu_0%20&amp;%5Cneq%200%0A%5Cend%7Balign%7D"></p>
<section id="central-limit-theorem" class="level1">
<h1>Central Limit Theorem</h1>
<p>From the Central Limit Theorem, we know the distribution of a sample mean is <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%20%5Csim%20N(%5Cmu,%20%5Csigma%5E2%20/%20n)"> and the standard error on <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D"> is <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7BX%7D)%20=%20%5Csigma/%5Csqrt%7Bn%7D">. The Z statistic is a transformation with <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Cfrac%7B%5Cbar%20X%20-%20%5Cmu%7D%7Bse(%5Cbar%7BX%7D)%7D%20%5Csim%20N(0,%201)">. Now we apply information from the hypothesis test, where the mean in question is the difference in two means.</p>
</section>
<section id="rejection-rule" class="level1">
<h1>Rejection Rule</h1>
<p>In order to reject the null, we must first assume that the null is true. Then we must show that the sample mean for <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is sufficiently different from 0, even when the true governing parameter is 0. This is conveniently done by working with the normalized Z statistic. In the case of the null, the sample mean <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5CDelta%7D%20%5Csim%20N(%5CDelta,%20%5Csigma%5E2)"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%200">. Converting to a normalized Z statistic we have <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Cfrac%7B%5Cbar%7B%5CDelta%7D%7D%7B%7BSE(%5Cbar%7B%5CDelta%7D)%7D%7D%20%5Csim%20N(0,%201)">. Now, we can state the rejection rule: reject if <img src="https://latex.codecogs.com/png.latex?%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D">. Equivalently <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cleft%7C%5Cfrac%7B%5Cbar%7B%5Cmu%7D_1%20-%20%5Cbar%7B%5Cmu%7D_0%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D%5Cright%7C%20&amp;%5Cgeq%20z_%7B1-%5Calpha/2%7D%0A%5Cend%7Balign%7D"> where <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7B%5CDelta%7D)%20=%20%5Csqrt%7B%5Cfrac%7B%5Csigma%5E2%7D%7Bn_T%7D%20+%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_C%7D%7D"> under randomization. If <img src="https://latex.codecogs.com/png.latex?n_T%20=%20k%20%5Ccdot%20n_C">, then we can use the reduction</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AVar(%5Cbar%7By%7D_c)%20&amp;=%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_C%7D%20%5C%5C%0Ase(%5Cbar%7B%5CDelta%7D)%20&amp;=%20%5Csqrt%7BVar(%5Cbar%7By%7D_c)%20(1%20+%20%5Cfrac%7B1%7D%7Bk%7D)%7D%0A%5Cend%7Balign%7D"></p>
</section>
<section id="probability-under-the-alternative" class="level1">
<h1>Probability under the Alternative</h1>
<p>Power is the probability of triggering the rejection rules, which were derived under the null <img src="https://latex.codecogs.com/png.latex?H_0:%20%5CDelta%20=%200">, when the true data generating process has <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cneq%200">. It is important to understand which governing parameter is in play here. The rejection rules will be derived using <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%200">, and those rules will be fixed. Then, we toggle the governing parameter to have <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cneq%200">, and benchmark the probability that data under this governing parameter will hit the rejection rule.</p>
<p>Say that <img src="https://latex.codecogs.com/png.latex?H_A"> is true and there are two distinct means <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_1"> with <img src="https://latex.codecogs.com/png.latex?%5Cmu_1%20-%20%5Cmu_0%20=%20k%20%5Cneq%200">. Under <img src="https://latex.codecogs.com/png.latex?H_A"> we cannot claim the usual Z statistic is distributed N(0, 1). Instead, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AZ%20%7C%20H_A%20&amp;=%20%5Cfrac%7B%5Cbar%7B%5CDelta%7D%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D%20%5Csim%20N(%5Cfrac%7Bk%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D,1)%20%5C%5C%0A%5Cend%7Balign%7D"></p>
<p>Now we revisit the rejection rule: reject when <img src="https://latex.codecogs.com/png.latex?%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D">. Power is the probability of triggering the rejection rule when <img src="https://latex.codecogs.com/png.latex?H_A"> is true, so</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0APower%20&amp;=%20P(%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D%20%7C%20Z%20%5Csim%20N(%5Cfrac%7Bk%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D,%201))%0A%5Cend%7Balign%7D"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20%5Cfrac%7B%5CDelta%20%7C%20H_A%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D"> be the noncentrality parameter (ncp), using the anticipated effect size we would like to detect under <img src="https://latex.codecogs.com/png.latex?H_A">. The ncp is a parameter we will revisit many times. The final solution for power is <img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Ctext%7BPower%7D%20=%20%5CPhi(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%7D%0A"></p>
<p>Using first principles, we can implement code as</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">treatment_effect <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">01</span></span>
<span id="cb1-2">sigma2_treatment <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> sigma2_control <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> sigma2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-3">n <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e3</span></span>
<span id="cb1-4">alpha <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span></span>
<span id="cb1-5">treatment_share <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-6">control_share <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-7">n_treatment <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> treatment_share</span>
<span id="cb1-8">n_control <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> control_share</span>
<span id="cb1-9"></span>
<span id="cb1-10">pooled_se <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(sigma2_treatment <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_treatment <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> sigma2_control <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_control)</span>
<span id="cb1-11">pooled_ncp <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> treatment_effect <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pooled_se</span>
<span id="cb1-12"></span>
<span id="cb1-13">crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-14"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pooled_ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> pooled_ncp))</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05143313</code></pre>
</div>
</div>
<p>This matches the native implementation in R</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">power.t.test</span>(</span>
<span id="cb3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> treatment_share,</span>
<span id="cb3-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">delta =</span> treatment_effect,</span>
<span id="cb3-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(sigma2),</span>
<span id="cb3-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">strict =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span></span>
<span id="cb3-6">)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample t test power calculation 

              n = 500
          delta = 0.01
             sd = 1.414214
      sig.level = 0.05
          power = 0.05143037
    alternative = two.sided

NOTE: n is number in *each* group</code></pre>
</div>
</div>
</section>
<section id="how-power-changes" class="level1">
<h1>How Power Changes</h1>
<p>A lot of research in experimentation goes into methods to increase statistical power. Using the formula, the variables that drive variance are:</p>
<ol type="1">
<li>The treatment effect, <img src="https://latex.codecogs.com/png.latex?%5Cmu_1%20-%20%5Cmu_0">.</li>
<li>The critical value, determined by <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
<li>Sample size, <img src="https://latex.codecogs.com/png.latex?n">.</li>
<li>Variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</li>
</ol>
<p>By changing the treatment effect that we want to detect, we can change power. However, that may jeopardize the practical value of an experiment. If we only power our experiment to detect large treatment effects that in practice do not happen, then the experiment is useless. Increasing <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is also a simple change, but it also increases the false positive rate. Increasing sample size decreases the standard error, but it will take more time and resources to accumulate the extra data.</p>
<p>Finally, the last path to improve power has a lot of subtlety. The role variance plays in the power formula is not fixed, it can vary. Its role is more precisely called <strong>“unexplained variance”</strong>. If there is a model that relates the metric, <img src="https://latex.codecogs.com/png.latex?y">, with the treatment variable and exogeneous covariates, <img src="https://latex.codecogs.com/png.latex?X">, then <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> is actually <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BVar%7D(y%20%7C%20X)">, or equivalently the variance of the residuals after we use <img src="https://latex.codecogs.com/png.latex?X"> to explain part of the variance. Using a good set of <img src="https://latex.codecogs.com/png.latex?X"> variables, we can make the unexplained variance small.</p>
<p>Which of these levers should we pursue to get maximal power? How much will power change? To answer this, we combine <img src="https://latex.codecogs.com/png.latex?n"> and variance into the more general variable, standard error. We merge the standard error and the effect size using the more general variable <img src="https://latex.codecogs.com/png.latex?%5Cdelta">. Below we plot power as <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha"> vary. If we wanted to unpack the specific sensitivity to standard error, we could say the sensitivity of power to standard error is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cfrac%7Bd%20%5Ctext%7BPower%7D%7D%7BdSE%7D%20&amp;=%20%5Cfrac%7Bd%20%5Ctext%7BPower%7D%7D%7Bd%20%5Cdelta%7D%20%5Cfrac%7Bd%20%5Cdelta%7D%7BdSE%7D%20%5C%5C%0A&amp;%20-%5Cfrac%7B%5Cmu_0%20-%20%5Cmu_1%7D%7BSE%5E2%7D%20%5CBigl%5B%20%5Cphi%20(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20-%20%5Cphi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20%5CBigr%5D%0A%5Cend%7Balign%7D"></p>
<p>It is a function of ncp and <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and the sensitivity is charted below.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">power <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(delta, se, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>) {</span>
<span id="cb5-2">  pooled_ncp <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> delta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> se</span>
<span id="cb5-3">  crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb5-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pooled_ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> pooled_ncp))</span>
<span id="cb5-5">}</span></code></pre></div></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/statistical_power/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="mde-as-the-inverse-of-statistical-power" class="level1">
<h1>MDE as the Inverse of Statistical Power</h1>
<p>MDE is the minimum detectable effect. It operates as the inverse to statistical power. Instead of asking: how much power do I have given an effect size and standard error, it asks: what is the effect size I can detect given a fixed amount of power and standard error.</p>
<p>To derive the MDE from first principles, we go back to the definition of power and how it is framed in terms of the rejection rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BPower%7D%20=%201%20-%20%5Cbeta%20=%20P(%5Cdelta%20%5Cgeq%20z_%7B1-%5Calpha/2%7D%20%7C%20H_A)%0A"></p>
<p>This equation about the CDF can be simplified. The noncentrality parameter is distributed <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Csim%20N(0,%201)">, so it being greater than or equal to <img src="https://latex.codecogs.com/png.latex?z_%7B1-%5Calpha/2%7D"> is actually just <img src="https://latex.codecogs.com/png.latex?1%20-%20%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20%5Cdelta)">. Then the derivation for the MDE is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20%5Cdelta)%20&amp;=%20%5Cbeta%20%5C%5C%0Az_%7B1-%5Calpha/2%7D%20-%20%5Cdelta%20&amp;=%20z_%7B%5Cbeta%7D%20%5C%5C%0A%5Cdelta%20&amp;=%20z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D%20%5C%5C%0A%5CDelta%20&amp;=%20%5Cboxed%7B(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%20%5Ccdot%20SE%7D%0A%5Cend%7Balign%7D"></p>
<p>Using common numbers, <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200.2">, the rule of thumb for MDE is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AMDE%20=%202.8%20%5Ccdot%20SE%0A"> or equivalently we need a ncp of <img src="https://latex.codecogs.com/png.latex?2.8">.</p>
<p>Let’s unpack this common rule of thumb explicitly. It says that the smallest effect size we can detect while ensuring an 80% power is <img src="https://latex.codecogs.com/png.latex?2.8%20%5Ccdot%20se(%5Cbar%7B%5CDelta%7D)">. (The SE here is the residual standard error after controlling for other variables.)</p>
<p>This looks like a similar rule of thumb related to rejecting the null: reject if the effect size is larger than 1.96 SE. This separate rule of thumb can be very confusing - why are there two different constants of 1.96 and 2.8? This rule describes when can we flag a result as stat sig while ensuring a different property: that under the null hypothesis there is less than a 5% chance of generating this effect by random. This rule of thumb is offering a different guarantee, not one about power. Ensuring 80% power at a specific effect size simply says that there is a good chance we can detect these effects, but it does not make it impossible. Even using power = 50% we will correctly flag some results as stat sig. (See multiple online discussions like this <a href="https://stats.stackexchange.com/questions/538590/reconciling-the-minimum-detectable-effect-calculation-with-the-experiment-result">one</a>) 1.96 SE is a rejection rule after the test executes, while 2.8 SE is a rule for planning to ensure 80% power. However, it is noteworthy that the scientific community is advocating for rejecting at 2.8 SE, or equivalently a p value of 0.005. (See <a href="https://escholarship.org/content/qt1jx6091g/qt1jx6091g.pdf">Redefine Statistical Significance</a>)</p>
</section>
<section id="sample-size-calculator" class="level1">
<h1>Sample Size Calculator</h1>
<p>It’s easier for a business to think about designing an experiment around the MDE. It’s a more natural discussion: what is the smallest effect that the business would still care about?</p>
<p>Given an MDE, we pivot the problem again. If <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7B%5CDelta%7D)%20=%20%20%5Csqrt%7B%5Cfrac%7B%5Csigma_T%5E2%7D%7Bn_T%7D%20+%20%5Cfrac%7B%5Csigma_C%5E2%7D%7Bn_C%7D%7D%20=%20%5Csqrt%7BVar(%5Cbar%7By%7D_c)%20(1%20+%20%5Cfrac%7B1%7D%7Bk%7D)%7D">, and <img src="https://latex.codecogs.com/png.latex?n_T%20=%20k%20%5Ccdot%20n_C">, then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AMDE%20&amp;=%20(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%20%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%7Bn_C%7D%7D%20%5Csqrt%7B1%20+%20%5Cfrac%7B1%7D%7Bk%7D%7D%20%5C%5C%0An_C%20&amp;=%20%5Cfrac%7B(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%5E2%20%5Csigma%5E2%20(%5Cfrac%7B1%7D%7Bk%7D%20+%201)%7D%7BMDE%5E2%7D%20%5C%5C%0An_T%20&amp;=%20k%20%5Ccdot%20n_C%0A%5Cend%7Balign%7D"></p>
<p>Using common numbers, <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200.2">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201"> the rule of thumb for <img src="https://latex.codecogs.com/png.latex?n_c"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7Bn_C%20=%20n_T%20=%20%5Cfrac%7B16%20%5Csigma%5E2%7D%7BMDE%5E2%7D%7D%0A"></p>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/statistical_power/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Mathematics of the Elastic Net</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/elastic_net/</link>
  <description><![CDATA[ 





<p><strong>Author’s note: This post is largely a rehash of many of the original elastic net and glmnet papers.</strong> I hope that having another voice describe the elegance of the elastic net will help others understand it. I have linked to all of the original documents to the best I can.</p>
<p>The elastic net adds L1 and L2 penalties to OLS, and is used to shrink coefficients towards zero. This can help with overfitting, as well as building an interpretive model from many features. When there is structure in coefficient-specific penalties, regularization can mimic a hierarchical model.</p>
<p>We start with a feature matrix, <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20p%7D">, a response vector, <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5En">, and a given <img src="https://latex.codecogs.com/png.latex?%5Calpha">. The elastic net formulates the problem</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D%20=%20%5Carg%5Cmin%20%5Csum_%7Bi=1%7D%5En%20(y_i%20-%5Cbeta_0%20-x_i%5ET%20%5Cbeta)%5E2%20+%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20(0.5(1-%5Calpha)%5Cbeta_j%5E2%20+%20%5Calpha%20%7C%5Cbeta_j%7C)."></p>
<p>The first term is the usual OLS term and the second term is a combination of L1 and L2 regularization.</p>
<section id="physical-interpretation-of-the-regularization" class="level1">
<h1>Physical Interpretation of the Regularization</h1>
<p>The 2 norm on <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> incentivizes the program to return coefficients that are small in magnitude. Likewise, the 1 norm incentivizes coefficients that are exactly zero. This prevents the exaggeration of effects in a model, while simultaneously serving as a form of model selection and interpretation.</p>
<p>Regularization is also similar to a prior. L2 regularization is similar to OLS with a gaussian prior on the parameters, that has a prior mean of 0 and a prior variance of <img src="https://latex.codecogs.com/png.latex?1/%5Clambda">. L1 regularization is similar to a laplacian prior. The relationship is explained <a href="https://papers.nips.cc/paper/1976-adaptive-sparseness-using-jeffreys-prior.pdf">here</a> with a compact stack overflow description <a href="https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior">here</a>.</p>
</section>
<section id="solving-the-program" class="level1">
<h1>Solving the Program</h1>
<p>When <img src="https://latex.codecogs.com/png.latex?X"> is centered and scaled to have zero mean and unit variance, the optimization problem can be solved using <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">coordinate descent</a>, with the update step:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D_j%20=%20%5Cfrac%7BS(%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(x_%7Bi,j%7D%5Cvarepsilon_i%20+%20%5Cbeta%5E%7B(%5Clambda)%7D_j),%20%5Clambda%20%5Calpha)%7D%7B1%20+%20%5Clambda(1%20-%20%5Calpha)%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?S(x,%20%5Clambda)%20=%20%5Ctext%7Bsign%7D(x)%20%5Ccdot%20(%7Cx%7C%20-%20%5Clambda)_+"> is the soft thresholding function.</p>
<p>This produces an algorithm with the form</p>
<pre><code># Given X, y, lambda, alpha.
for cycle in 1:max_cycles
  for j in 1:p
    for it in 1:max_iters
      beta_j = &lt;do update step above&gt;</code></pre>
</section>
<section id="searching-lambda" class="level1">
<h1>Searching <img src="https://latex.codecogs.com/png.latex?%5Clambda"></h1>
<p>The amount of regularization to use is always a question when fitting the elastic net. More regularization will more aggressively shrink the coefficients to zero. From the physical interpretation section above, regularization is like a prior, and careful thought also goes into choosing the prior. Usually, we cross validate and search for an optimal <img src="https://latex.codecogs.com/png.latex?%5Clambda"> that minimizes an out-of-sample metric. Fortunately there is a smart strategy for how to pick a starting set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> to explore (<a href="https://web.stanford.edu/~hastie/TALKS/glmnet.pdf">talk</a>, <a href="https://stats.stackexchange.com/questions/166630/glmnet-compute-maximal-lambda-value">stack overflow</a>).</p>
<p>Say a good set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> ranges from <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D">, and is logarithmically spaced apart, where <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> is the smallest <img src="https://latex.codecogs.com/png.latex?%5Clambda"> such that the coefficient vector is the zero vector and <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D"> is some multiple of <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D">.</p>
<p>When <img src="https://latex.codecogs.com/png.latex?X"> is centered and scaled to have zero mean and unit variance, and <img src="https://latex.codecogs.com/png.latex?y"> is centered to have zero mean, then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D%20=%20%5Cfrac%7B%5Cmax(%7CX%5ET%20y%7C)%7D%7Bn%20%5Calpha%7D."></p>
<p>In <code>glmnet::glmnet</code>, <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D%20=%20.0001%20%5Clambda_%7Bmax%7D"> if <img src="https://latex.codecogs.com/png.latex?n%20%3E%20p">. It should be noted that when <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200">, <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> does not exist, so <code>glmnet</code> intercepts <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and pretends it is 0.001.</p>
<p>Adding this layer to search for <img src="https://latex.codecogs.com/png.latex?%5Clambda"> means the optimization algorithm above gains a fourth nested for loop.</p>
<pre><code># Given X, y, alpha.
for cycle in 1:max_cycles
  for j in 1:p
    for l in lambda: 
      for it in 1:max_iters
        beta_j = &lt;do update step above&gt;</code></pre>
<p>This sounds like it is untractable, but there are several optimizations that can make the algorithm fast.</p>
</section>
<section id="computational-performance" class="level1">
<h1>Computational performance</h1>
<p>The above two sections are sufficient enough to build a lightweight elastic net solver. This section describes specific optimizations that make the algorithm faster, but ultimately are not relevant for how to use the elastic net as an end user.</p>
<section id="updates-via-covariance" class="level2">
<h2 class="anchored" data-anchor-id="updates-via-covariance">Updates via Covariance</h2>
<p>Note that the <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i"> term can be decomposed into <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D(y_%7Bi%7D%20-%20x_%7Bi%7D%5ET%20%5Cbeta)">. This can be computed very efficiently from a few vectorized operations that are computed just once outside of all of the loops. We first compute and store <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> and <img src="https://latex.codecogs.com/png.latex?X%5ET%20y">. When <img src="https://latex.codecogs.com/png.latex?X"> is sparse the linear algebra can be optimized. Then <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20=%20(X%5ET%20y)%5Bj%5D%20-%20(X%5ET%20X)%5B,j%5D%5ET%5Cbeta">, i.e. the j-th component of <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> and the dot product between the j-th column of <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</p>
</section>
<section id="reuse-xt-y-from-searching-lambda" class="level2">
<h2 class="anchored" data-anchor-id="reuse-xt-y-from-searching-lambda">Reuse <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> from searching <img src="https://latex.codecogs.com/png.latex?%5Clambda"></h2>
<p>When a smart set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is initialized, we can store the product <img src="https://latex.codecogs.com/png.latex?X%5ET%20y">, which is then used as part of the covariance update strategy.</p>
</section>
<section id="pathwise-coordinate-descent" class="level2">
<h2 class="anchored" data-anchor-id="pathwise-coordinate-descent">Pathwise Coordinate Descent</h2>
<p>The elastic net algorithm can compute the coefficient vector for several values of <img src="https://latex.codecogs.com/png.latex?%5Clambda">. Suppose we have a monotonically decreasing sequence for <img src="https://latex.codecogs.com/png.latex?%5Clambda">, <img src="https://latex.codecogs.com/png.latex?%7B%5Clambda%7D%20=%20%7B%5Clambda_%7Bmax%7D,%20%5Clambda_2,%20%5Cldots%7D">. By definition, the coefficient vector for <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> is the zero vector. The next <img src="https://latex.codecogs.com/png.latex?%5Clambda"> in the sequence will have the update step <img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D_j%20=%200"> as long as <img src="https://latex.codecogs.com/png.latex?%7CX%5ETy%5Bj%5D%7C%20%3C%20%5Clambda%20%5Calpha%20n">. This check is a simple lookup since <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> is cached, and can bypass several update steps.</p>
</section>
<section id="active-sets" class="level2">
<h2 class="anchored" data-anchor-id="active-sets">Active Sets</h2>
<p>After doing one pass on the outermost loop that iterates on <code>cycles</code>, we check which coefficients are nonzero. In the second cycle, instead of iterating on the <img src="https://latex.codecogs.com/png.latex?p"> coefficients, we iterate only on the nonzero ones. These are the active sets. Finally, at the end we do one last cycle iterating on all coefficients. If the nonzeros have not changed, we conclude the algorithm.</p>
</section>
<section id="centering-and-scaling" class="level2">
<h2 class="anchored" data-anchor-id="centering-and-scaling">Centering and Scaling</h2>
<p>Much of the elastic net algorithm assumes <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> have been centered and scaled. Say we start with a feature matrix <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D"> which is not centered or scaled. Centering <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D"> makes it become dense, and many sparse linear algebra optimizations are lost.</p>
<p>Instead, we leverage the formula that centering and scaling can be written as</p>
<p><img src="https://latex.codecogs.com/png.latex?X%20=%20(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D."></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cmu_%5Ctilde%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%5Ctilde%7Bx%7D"> column vectors containing the column means and column standard deviations of <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D">, and likewise for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">.</p>
<p>The key computations can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AX%5ET%20y%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20(%5Ctilde%7By%7D%20-%201%5Cmu_%5Ctilde%7By%7D).%5C%5C%0AX%5ET%20X%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%20n%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET.%0A%5Cend%7Balign%7D"></p>
</section>
</section>
<section id="elastic-net-with-weights" class="level1">
<h1>Elastic Net with Weights</h1>
<p>This section discusses the extension of elastic net to use weights, similar to weighted least squares.</p>
<section id="coordinate-descent-with-weights" class="level2">
<h2 class="anchored" data-anchor-id="coordinate-descent-with-weights">Coordinate Descent with Weights</h2>
<p>Assume that <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> have been centered and scaled <strong>without weights</strong>, so that their unweighted means are 0 and unweighted variances are 1. The update step for weighted elastic net is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%5E%7B(%5Clambda)%7D%20=%20%5Cfrac%7BS(%5Csum_%7Bi=1%7D%5En%20(w_i%20x_%7Bi,j%7D(%5Cvarepsilon_i%20+%20x_%7Bi,j%7D%5Cbeta_j%5E%7B(%5Clambda)%7D)),%20%5Clambda%20%5Calpha)%7D%7B%5Csum_i%20w_i%20x_%7Bi,j%7D%5E2%20+%20%5Clambda(1%20-%20%5Calpha)%7D"></p>
<p>Though it looks more complex than before, using <img src="https://latex.codecogs.com/png.latex?w_i%20=%201/n"> will reduce the update step to the original unweighted update step.</p>
<p>Now suppose that <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> were centered and scaled <strong>with weights</strong>, so that their weighted means are 0 and weighted variances are 1. By taking advantage of the definition <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20w_i%20x_%7Bi,j%7D%5E2%20=%20%5Csum_i%20w_i"> we can recover the more familiar formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%5E%7B(%5Clambda)%7D%20=%20%5Cfrac%7BS(%5Csum_%7Bi=1%7D%5En%20(w_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20+%20%5Cbeta_j%5E%7B(%5Clambda)%7D),%20%5Clambda%20%5Calpha)%7D%7B%5Csum_i%20w_i%20+%20%5Clambda(1%20-%20%5Calpha)%7D."></p>
<p>Like before, this update step can use vectorized operations. The key computations can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AX%5ET%20W%20y%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5Ctext%7BDiagonal%7D(w)%20(%7B%5Ctilde%7By%7D%7D)%20%5C%5C%0A%20%20&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20(%7B%5Ctilde%7By%7D%7D)%20-%0A%20%20%20%20%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Cmu_%7B%5Ctilde%7BX%7D%7D%20w%5ET%20%5Ctilde%7By%7D.%20%5C%5C%0AX%5ET%20W%20X%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5B(%7B%5Ctilde%7BX%7D%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%0A%20%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20w%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20-%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20w%5ET%20%5Ctilde%7BX%7D%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20+%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20%5Csum_i%20w_i%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20%5Csum_i%20w_i.%20%5C%5C%0A%5Clambda_%7Bmax%7D%20&amp;=%20%5Cmax%20%5Cfrac%7B%7CX%5ET%20W%20y%7C%7D%7B%5Calpha%7D.%0A%5Cend%7Balign%7D"></p>
</section>
<section id="vectorizing-for-multiple-outcome-variables" class="level2">
<h2 class="anchored" data-anchor-id="vectorizing-for-multiple-outcome-variables">Vectorizing for Multiple Outcome Variables</h2>
<p>Many applications will track multiple outcome variables, so that <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20o%7D"> is a matrix of <img src="https://latex.codecogs.com/png.latex?o"> outcomes per observation. When the outcomes are independent, there is a fast way to fit multiple OLS regressions to the same feature matrix. Likewise, there is a fast way to do this for multiple elastic nets.</p>
<p>The bulk of the computation for a single <img src="https://latex.codecogs.com/png.latex?y"> is in the covariance update step</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20=%20(X%5ET%20y)%5Bj%5D%20-%20(X%5ET%20X)%5B,j%5D%5ET%5Cbeta."></p>
<p><img src="https://latex.codecogs.com/png.latex?y"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are column vectors. It is possible to update the j-th coefficient for all outcomes simultaneously. We vectorize over <img src="https://latex.codecogs.com/png.latex?o"> outcomes to produce and cache the intermediate matrix <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bp%20%5Ctimes%20o%7D">, and reuse <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> across outcomes.</p>
<p>However, different outcome variables can reach convergence differently. When updating the j-th coefficient, we would like to subset the columns of <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y"> to those outcomes which have not converged yet. This subsetting creates a deep copy of the matrix, and can be counter productive to the vectorization over multiple outcomes.</p>
<p>In practice, it may be easier to implement a job coordinator that computes <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y"> and <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> apriori. These intermediates are stored in shared memory. Then, the coordinator assigns the task of estimating <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> for a single outcome to a worker, which reads the intermediates from shared memory.</p>
</section>
</section>
<section id="extensions" class="level1">
<h1>Extensions</h1>
<section id="differential-shrinkage" class="level2">
<h2 class="anchored" data-anchor-id="differential-shrinkage">Differential Shrinkage</h2>
<p>The standard description of the elastic net assumes a constant penalty across all coefficients, as seen in</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D%20=%20%5Carg%5Cmin%20%5Csum_%7Bi=1%7D%5En%20(y_i%20-%5Cbeta_0%20-x_i%5ET%20%5Cbeta)%5E2%20+%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20(0.5(1-%5Calpha)%5Cbeta_j%5E2%20+%20%5Calpha%20%7C%5Cbeta_j%7C)."></p>
<p>Sometimes we want to augment the penalty for different coefficients. The library <code>glmnet</code> introduces the parameter <code>penalty.factor</code>, which multiplies the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> term by a <img src="https://latex.codecogs.com/png.latex?%5Cgamma_j%20%5Cgeq%200"> that varies for different coefficients. The algorithm for solving elastic net is flexible for differential shrinkage, where the loop over coefficients scales the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> penalty term by <img src="https://latex.codecogs.com/png.latex?%5Cgamma_j">. In addition, the initialization of the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> path should use</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D%20=%20%5Cmax%20%5Ctext%7BDiagonal%7D(1/%5Cgamma)%20%5Cfrac%7B%7CX%5ET%20W%20y%7C%7D%7Bn%20%5Calpha%7D."></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li>https://web.stanford.edu/~hastie/TALKS/glmnet.pdf</li>
<li>https://web.stanford.edu/~hastie/Papers/glmnet.pdf</li>
<li>https://stats.stackexchange.com/questions/166630/glmnet-compute-maximal-lambda-value</li>
<li>https://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet</li>
<li>https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</li>
</ol>


</section>

 ]]></description>
  <category>mathematical statistics</category>
  <guid>https://jeffwong.github.io/posts/elastic_net/</guid>
  <pubDate>Wed, 07 Apr 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Tikhonov Regularization and Gaussian Priors</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/l2_gaussian_prior/</link>
  <description><![CDATA[ 





<p>In this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Throughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, <img src="https://latex.codecogs.com/png.latex?y">, that is normally distributed and its mean is a linear combination of features, <img src="https://latex.codecogs.com/png.latex?x">, so that <img src="https://latex.codecogs.com/png.latex?Y%20%5Csim%20N(x%5ET%5Cbeta,%20%5CSigma)">.</p>
<p>Optionally, the parameter vector <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> can have a prior on it, in the form <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20N(%5Cmu_0,%20%5CSigma_0)">.</p>
</section>
<section id="maximum-likelihood-for-normally-distributed-data" class="level1">
<h1>Maximum Likelihood for Normally Distributed Data</h1>
<p>In frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Cbeta)%20=%20%5Cprod_i%20N(y_i%20%7C%20x_i,%20%5Cbeta,%20%5CSigma)%20=%20%5Cprod_i%20%5Cfrac%7B1%7D%7B%5Csqrt%7B(2%20%5Cpi)%5Ek%20%7C%5CSigma%7C%7D%7D%0Aexp(%7B-%5Cfrac%7B1%7D%7B2%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%7D)."> The MLE estimate for <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> will maximize the log-likelihood with respect to <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, by differentiating it and finding its root. This produces the MLE estimate</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMLE%7D%20=%20(X%5ET%20X)%5E%7B-1%7D%20X%5ET%20y."></p>
</section>
<section id="maximum-a-posteriori" class="level1">
<h1>Maximum a Posteriori</h1>
<p>When there is a gaussian prior in the form <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20N(%5Cmu_0,%20%5CSigma_0)">, we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0Ap(%5Cbeta%7Cy,%20x,%20%5CSigma)%20&amp;=%20%5Cprod_i%20N(y_i%20%7C%20x_i,%20%5Cbeta,%20%5CSigma)%20%5Ccdot%20N(%5Cbeta%20%7C%20%5Cmu_0,%20%5CSigma_0)%20%5C%5C%0A%20%20&amp;%5Cpropto%0A%20%20%5Cprod_i%20%5Cfrac%7B1%7D%7B%7C%5CSigma%7C%7D%0A%20%20exp(%7B-%5Cfrac%7B1%7D%7B2%7D%20%5Cbig((y_i%20-%20x_i%5ET%20%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%20-%20(%5Cbeta%20-%20%5Cmu_0)%5ET%20%5CSigma_0%5E%7B-1%7D%20(%5Cbeta%20-%20%5Cmu_0)%5Cbig)%7D).%0A%5Cend%7Balign%7D"></p>
<p>The posterior turns out to be another normal distribution, <img src="https://latex.codecogs.com/png.latex?N(%5Cmu_1,%20%5CSigma_1)"> (<a href="https://en.wikipedia.org/wiki/Conjugate_prior">wiki</a>), where</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5CSigma_1%20&amp;=%20(%5CSigma_0%5E%7B-1%7D%20+%20n%20%5CSigma%5E%7B-1%7D)%5E%7B-1%7D%20%5C%5C%0A%5Cmu_1%20&amp;=%20%5CSigma_1%20(%5CSigma_0%5E%7B-1%7D%20%5Cmu_0%20+%20%5CSigma%5E%7B-1%7D%20%5Csum_i%7By_i%7D)%0A%5Cend%7Balign%7D"></p>
<p>The maximum a-posteriori estimator (<a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">wiki</a>) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMAP%7D%20=%20%5Carg%20max_%7B%5Cbeta%7D%0A-%20(y-X%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y-X%5Cbeta)%0A-%20(%5Cbeta-%5Cbeta_0)%5ET%20%5CSigma_0%5E%7B-1%7D%20(%5Cbeta-%5Cbeta_0)."> Recall that <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is fixed, and <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?%5CSigma_0"> are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMAP%7D%20=%20(X%5ET%20X%20+%20%5CSigma_0)%5E%7B-1%7D%20(X%5ET%20y%20+%20%5CSigma_0%20%5Cmu_0)."></p>
</section>
<section id="equivalence-between-mle-and-map" class="level1">
<h1>Equivalence between MLE and MAP</h1>
<p>When the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution.</p>


</section>

 ]]></description>
  <category>mathematical statistics</category>
  <guid>https://jeffwong.github.io/posts/l2_gaussian_prior/</guid>
  <pubDate>Sun, 27 Dec 2020 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
