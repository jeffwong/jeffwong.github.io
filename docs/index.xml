<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>From First Principles</title>
<link>https://jeffwong.github.io/</link>
<atom:link href="https://jeffwong.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Data science innovation from first principles.</description>
<generator>quarto-1.8.24</generator>
<lastBuildDate>Mon, 15 Sep 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>Statistical Power under Noncompliance</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/power_noncompliance/</link>
  <description><![CDATA[ 





<p>This is an extension of a previous post on statistical power. In that post we derived the formula for statistical power using</p>
<ol type="1">
<li>The effect size</li>
<li>The standard error of the effect. Alternatively, this can be stated as the variance and the sample size.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
</ol>
<p>However, there is an assumption of perfect compliance. That is, 100% of the treatment units are actually treated, and 100% of the control units are actually withheld.</p>
<p>In this post we discuss the case when <img src="https://latex.codecogs.com/png.latex?p_1">% of treatment units are treated, and <img src="https://latex.codecogs.com/png.latex?p_0">% of control units are withheld.</p>
<section id="effect-size" class="level1">
<h1>Effect size</h1>
<p>Say that treatment assignment is determined by the variable <img src="https://latex.codecogs.com/png.latex?Z">, so <img src="https://latex.codecogs.com/png.latex?Z%20=%201"> means we intend to provide treatment, and <img src="https://latex.codecogs.com/png.latex?Z%20=%200"> means we intend to withhold. Let <img src="https://latex.codecogs.com/png.latex?X"> be the treatment that was actually received.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0Ap_1%20&amp;=%20P(X%20=%201%20%7C%20Z%20=%201)%20%5C%5C%0Ap_0%20&amp;=%20P(X%20=%200%20%7C%20Z%20=%200)%0A%5Cend%7Balign%7D"></p>
<p>The effect size we care about is <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%20%5Cmu_1%20-%20%5Cmu_0%20=%20E%5By%20%7C%20X%20=%201%5D%20-%20E%5By%20%7C%20X%20=%200%5D">. There is a difference between what we care about and what we will observe. From data, we will observe a dilution in the treatment effect. Noncompliance will decrease the effect size and will subsequently decrease the power.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmu_1%20%7C%20Z%20=%201%20&amp;=%20p_1%20%5Cmu_1%20+%20(1%20-%20p_1)%20%5Cmu_0%20%5C%5C%0A%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20p_0%20%5Cmu_0%20+%20(1%20-%20p_0)%20%5Cmu_1%20%5C%5C%0A%5Cmu_1%20%7C%20Z%20=%201%20-%20%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20(p_1%20+%20p_0%20-%201)%20%5CDelta%0A%5Cend%7Balign%7D"></p>
</section>
<section id="variance" class="level1">
<h1>Variance</h1>
<p>The variance of <img src="https://latex.codecogs.com/png.latex?y"> is broken into</p>
<p><img src="https://latex.codecogs.com/png.latex?Var(y)%20=%20E%5Bvar(y%20%20%7C%20X)%5D%20+%20Var(E%5By%20%7C%20X%5D)"></p>
<p>The first term can be simplified and reduced to just <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">. We can argue that the variance of <img src="https://latex.codecogs.com/png.latex?y"> is not a function of the artificially generated assignment variable, <img src="https://latex.codecogs.com/png.latex?Z">. It is only a function of whether or not the unit actually received the treatment. For simplicity, we say that the variance is independent of the treatment received.</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5Bvar(y%20%7C%20X%20=%201)%5D%20+%20E%5Bvar(y%20%7C%20X%20=%200)%5D%20=%20%5Csigma%5E2."> The conditioning on a binary X is like a mixture of bernoulli random variables. In this case the mixture has 2 equal components so it reduces to <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</p>
<p>Next, is the variance of a mixture of bernoulli variables: <img src="https://latex.codecogs.com/png.latex?E%5By%20%7C%20X%20=%200%5D%20=%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?E%5By%20%7C%20X%20=%201%5D%20=%20%5Cmu_1%20=%20%5Cmu_0%20+%20%5CDelta">. The variance of this mixture is a function of the mixing probabilities and the difference in the individual means, which in this case will be <img src="https://latex.codecogs.com/png.latex?%5CDelta">. Then we have the key pieces we need to measure statistical power based on the observed data</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cmu_1%20%7C%20Z%20=%201%20&amp;=%20p_1%20%5Cmu_1%20+%20(1%20-%20p_1)%20%5Cmu_0%20%5C%5C%0A%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20p_0%20%5Cmu_0%20+%20(1%20-%20p_0)%20%5Cmu_1%20%5C%5C%0A%5Cmu_1%20%7C%20Z%20=%201%20-%20%5Cmu_0%20%7C%20Z%20=%200%20&amp;=%20(p_1%20+%20p_0%20-%201)%20%5CDelta%20%5C%5C%0AVar(y%20%7C%20Z%20=%201)%20&amp;=%20%5Csigma%5E2%20+%20p_1%20(1%20-%20p_1)%20%5CDelta%5E2%20%5Capprox%20%5Csigma%5E2%20%5C%5C%0AVar(y%20%7C%20Z%20=%200)%20&amp;=%20%5Csigma%5E2%20+%20p_0%20(1%20-%20p_0)%20%5CDelta%5E2%20%5Capprox%20%5Csigma%5E2%0A%5Cend%7Balign%7D"></p>
<p>The approximation in <img src="https://latex.codecogs.com/png.latex?Var(y%20%7C%20Z%20=%201)%20%5Capprox%20%5Csigma%5E2"> comes from the pattern that variance (not normalized by <img src="https://latex.codecogs.com/png.latex?n">) tends to be large while effect size tends to be small. Thus variance under noncompliance is not much different than variance under full compliance. The change in power will largely come from the dilution in the treatment effect.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Ctext%7BPower%7D%20=%20%5CPhi(%5Cdelta'%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta'%20-%20z_%7B1-%5Calpha/2%7D)%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cdelta'%20=%20%5Cdelta%20(p_1%20+%20p_0%20-%201)"> is a dilution on the treatment effect.</p>
<p>Using some simple numbers, if <img src="https://latex.codecogs.com/png.latex?p_1%20=%20p_0%20=%200.9">, then <img src="https://latex.codecogs.com/png.latex?%5Cdelta'%20=%200.8%20%5Cdelta">. By changing the effect size by 0.8, we change the sample size necessary by <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B.8%5E2%7D%20=%2056%5C%25">!</p>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/power_noncompliance/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Foundations in Statistical Power</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/statistical_power/</link>
  <description><![CDATA[ 





<p>Statistical power is a concept that helps us plan for an effective experiment. Power is typically described in terms of a purely randomized and controlled trial, where we can take advantage of certain properties like independence. It is also typically described in the context of measuring the difference in means, which also has properties like the Central Limit Theorem that it takes advantage of. Finally, an assumption in typical description is that all units that are assigned to the treatment group are successfully treated, and all units assigned to the control are successfully withheld.</p>
<p>But not all experiments are so perfectly planned and executed. It is important to understand the history of how statistical power was derived, from first principles, so that we can adapt it and innovate. For example, some experiments do not have perfect compliance. Many experiments will try to give treatment to a subject, but that subject refuses, or forgets, to use the treatment. This complexity can add layers to the question: “what should we measure? The average effect or the average effect among the compliers?” With this complication we will need to revisit our formula for statistical power. We will do this innovation exercise in the next post. For now, let us understand the history of statistical power.</p>
<hr>
<p>Power is always associated with a hypothesis test. <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BPower%7D%20=%201%20-%20%5Cbeta%20=%20P(%5Ctext%7BReject%20%7DH_0%20%7C%20H_A)"> where <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the type 1 error rate, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the type 2 error rate.</p>
<p>So to derive power, we must first identify the rejection rules of the hypothesis test, then evaluate the probability of rejection if the alternative is true.</p>
<p>To illustrate, we specify a very generic hypothesis test, the difference in means. While frequently associated with the t test, it is not necessarily tied to it. The hypothesis about the difference in means is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AH_0:%20%5Cmu_1%20-%20%5Cmu_0%20&amp;=%200%20%5C%5C%0AH_A:%20%5Cmu_1%20-%20%5Cmu_0%20&amp;%5Cneq%200%0A%5Cend%7Balign%7D"></p>
<section id="central-limit-theorem" class="level1">
<h1>Central Limit Theorem</h1>
<p>From the Central Limit Theorem, we know the distribution of a sample mean is <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%20%5Csim%20N(%5Cmu,%20%5Csigma%5E2%20/%20n)"> and the standard error on <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D"> is <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7BX%7D)%20=%20%5Csigma/%5Csqrt%7Bn%7D">. The Z statistic is a transformation with <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Cfrac%7B%5Cbar%20X%20-%20%5Cmu%7D%7Bse(%5Cbar%7BX%7D)%7D%20%5Csim%20N(0,%201)">. Now we apply information from the hypothesis test, where the mean in question is the difference in two means.</p>
</section>
<section id="rejection-rule" class="level1">
<h1>Rejection Rule</h1>
<p>In order to reject the null, we must first assume that the null is true. Then we must show that the sample mean for <img src="https://latex.codecogs.com/png.latex?%5CDelta"> is sufficiently different from 0, even when the true governing parameter is 0. This is conveniently done by working with the normalized Z statistic. In the case of the null, the sample mean <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5CDelta%7D%20%5Csim%20N(%5CDelta,%20%5Csigma%5E2)"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%200">. Converting to a normalized Z statistic we have <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Cfrac%7B%5Cbar%7B%5CDelta%7D%7D%7B%7BSE(%5Cbar%7B%5CDelta%7D)%7D%7D%20%5Csim%20N(0,%201)">. Now, we can state the rejection rule: reject if <img src="https://latex.codecogs.com/png.latex?%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D">. Equivalently <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cleft%7C%5Cfrac%7B%5Cbar%7B%5Cmu%7D_1%20-%20%5Cbar%7B%5Cmu%7D_0%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D%5Cright%7C%20&amp;%5Cgeq%20z_%7B1-%5Calpha/2%7D%0A%5Cend%7Balign%7D"> where <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7B%5CDelta%7D)%20=%20%5Csqrt%7B%5Cfrac%7B%5Csigma%5E2%7D%7Bn_T%7D%20+%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_C%7D%7D"> under randomization. If <img src="https://latex.codecogs.com/png.latex?n_T%20=%20k%20%5Ccdot%20n_C">, then we can use the reduction</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AVar(%5Cbar%7By%7D_c)%20&amp;=%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_C%7D%20%5C%5C%0Ase(%5Cbar%7B%5CDelta%7D)%20&amp;=%20%5Csqrt%7BVar(%5Cbar%7By%7D_c)%20(1%20+%20%5Cfrac%7B1%7D%7Bk%7D)%7D%0A%5Cend%7Balign%7D"></p>
</section>
<section id="probability-under-the-alternative" class="level1">
<h1>Probability under the Alternative</h1>
<p>Power is the probability of triggering the rejection rules, which were derived under the null <img src="https://latex.codecogs.com/png.latex?H_0:%20%5CDelta%20=%200">, when the true data generating process has <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cneq%200">. It is important to understand which governing parameter is in play here. The rejection rules will be derived using <img src="https://latex.codecogs.com/png.latex?%5CDelta%20=%200">, and those rules will be fixed. Then, we toggle the governing parameter to have <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cneq%200">, and benchmark the probability that data under this governing parameter will hit the rejection rule.</p>
<p>Say that <img src="https://latex.codecogs.com/png.latex?H_A"> is true and there are two distinct means <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_1"> with <img src="https://latex.codecogs.com/png.latex?%5Cmu_1%20-%20%5Cmu_0%20=%20k%20%5Cneq%200">. Under <img src="https://latex.codecogs.com/png.latex?H_A"> we cannot claim the usual Z statistic is distributed N(0, 1). Instead, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AZ%20%7C%20H_A%20&amp;=%20%5Cfrac%7B%5Cbar%7B%5CDelta%7D%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D%20%5Csim%20N(%5Cfrac%7Bk%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D,1)%20%5C%5C%0A%5Cend%7Balign%7D"></p>
<p>Now we revisit the rejection rule: reject when <img src="https://latex.codecogs.com/png.latex?%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D">. Power is the probability of triggering the rejection rule when <img src="https://latex.codecogs.com/png.latex?H_A"> is true, so</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0APower%20&amp;=%20P(%7CZ%7C%20%5Cgeq%20z_%7B1-%5Calpha/2%7D%20%7C%20Z%20%5Csim%20N(%5Cfrac%7Bk%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D,%201))%0A%5Cend%7Balign%7D"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20%5Cfrac%7B%5CDelta%20%7C%20H_A%7D%7Bse(%5Cbar%7B%5CDelta%7D)%7D"> be the noncentrality parameter, using the effect size we would like to detect under <img src="https://latex.codecogs.com/png.latex?H_A">. The final solution for power is <img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7B%5Ctext%7BPower%7D%20=%20%5CPhi(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20+%20%5CPhi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%7D%0A"></p>
<p>Using first principles, we can implement code as</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">treatment_effect <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">01</span></span>
<span id="cb1-2">sigma2_treatment <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> sigma2_control <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> sigma2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-3">n <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e3</span></span>
<span id="cb1-4">alpha <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span></span>
<span id="cb1-5">treatment_share <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-6">control_share <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-7">n_treatment <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> treatment_share</span>
<span id="cb1-8">n_control <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> control_share</span>
<span id="cb1-9"></span>
<span id="cb1-10">pooled_se <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(sigma2_treatment <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_treatment <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> sigma2_control <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> n_control)</span>
<span id="cb1-11">pooled_ncp <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> treatment_effect <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> pooled_se</span>
<span id="cb1-12"></span>
<span id="cb1-13">crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-14"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pooled_ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> pooled_ncp))</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05143313</code></pre>
</div>
</div>
<p>This matches the native implementation in R</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">power.t.test</span>(</span>
<span id="cb3-2">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> n <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> treatment_share,</span>
<span id="cb3-3">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">delta =</span> treatment_effect,</span>
<span id="cb3-4">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">sd =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sqrt</span>(sigma2),</span>
<span id="cb3-5">  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">strict =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span></span>
<span id="cb3-6">)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample t test power calculation 

              n = 500
          delta = 0.01
             sd = 1.414214
      sig.level = 0.05
          power = 0.05143037
    alternative = two.sided

NOTE: n is number in *each* group</code></pre>
</div>
</div>
</section>
<section id="how-power-changes" class="level1">
<h1>How Power Changes</h1>
<p>A lot of research in experimentation goes into methods to increase statistical power. Using the formula, the variables that drive variance are:</p>
<ol type="1">
<li>The treatment effect, <img src="https://latex.codecogs.com/png.latex?%5Cmu_1%20-%20%5Cmu_0">.</li>
<li>The critical value, determined by <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</li>
<li>Sample size, <img src="https://latex.codecogs.com/png.latex?n">.</li>
<li>Variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</li>
</ol>
<p>By changing the treatment effect that we want to detect, we can change power. However, that may jeopardize the practical value of an experiment. If we only power our experiment to detect large treatment effects that in practice do not happen, then the experiment is useless. Increasing <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is also a simple change, but it also increases the false positive rate. Increasing sample size decreases the standard error, but it will take more time and resources to accumulate the extra data.</p>
<p>Finally, the last path to improve power has a lot of subtlety. The role variance plays in the power formula is not fixed, it can vary. Its role is more precisely called <strong>“unexplained variance”</strong>. If there is a model that relates the metric, <img src="https://latex.codecogs.com/png.latex?y">, with the treatment variable and exogeneous covariates, <img src="https://latex.codecogs.com/png.latex?X">, then <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2"> is actually <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BVar%7D(y%20%7C%20X)">, or equivalently the variance of the residuals after we use <img src="https://latex.codecogs.com/png.latex?X"> to explain part of the variance. Using a good set of <img src="https://latex.codecogs.com/png.latex?X"> variables, we can make the unexplained variance small.</p>
<p>Which of these levers should we pursue to get maximal power? How much will power change? To answer this, we combine <img src="https://latex.codecogs.com/png.latex?n"> and variance into the more general variable, standard error. We merge the standard error and the effect size using the more general variable <img src="https://latex.codecogs.com/png.latex?%5Cdelta">. Below we plot power as <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha"> vary. If we wanted to unpack the specific sensitivity to standard error, we could say the sensitivity of power to standard error is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cfrac%7Bd%20%5Ctext%7BPower%7D%7D%7BdSE%7D%20&amp;=%20%5Cfrac%7Bd%20%5Ctext%7BPower%7D%7D%7Bd%20%5Cdelta%7D%20%5Cfrac%7Bd%20%5Cdelta%7D%7BdSE%7D%20%5C%5C%0A&amp;%20-%5Cfrac%7B%5Cmu_0%20-%20%5Cmu_1%7D%7BSE%5E2%7D%20%5CBigl%5B%20%5Cphi%20(%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20-%20%5Cphi(-%5Cdelta%20-%20z_%7B1-%5Calpha/2%7D)%20%5CBigr%5D%0A%5Cend%7Balign%7D"></p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">power <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(delta, se, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>) {</span>
<span id="cb5-2">  pooled_ncp <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> delta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> se</span>
<span id="cb5-3">  crit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb5-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pooled_ncp) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pnorm</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>crit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> pooled_ncp))</span>
<span id="cb5-5">}</span></code></pre></div></div>
</div>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">delta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb6-2">se <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">from =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">to =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">01</span>)</span>
<span id="cb6-3">alpha <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(.<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">005</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">01</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">05</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, .<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb6-4"></span>
<span id="cb6-5">params <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">expand.grid</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">delta =</span> delta, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">se =</span> se, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">alpha =</span> alpha)</span>
<span id="cb6-6">output <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pmap_dbl</span>(params <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.data.frame</span>(), <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">function</span>(delta, se, alpha) {</span>
<span id="cb6-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">power</span>(delta, se, alpha)</span>
<span id="cb6-8">})</span>
<span id="cb6-9"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cbind</span>(params, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">power =</span> output) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb6-11">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">t =</span> delta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> se,</span>
<span id="cb6-12">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">crit =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">qnorm</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb6-13">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-14">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb6-15">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">is_stat_sig =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">abs</span>(delta<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>se) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> crit</span>
<span id="cb6-16">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-17">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(alpha, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">desc</span>(se)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-18">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(alpha) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-19">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb6-20">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">power_prev =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lag</span>(power)</span>
<span id="cb6-21">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-22">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(</span>
<span id="cb6-23">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">incremental_power =</span> power <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> power_prev</span>
<span id="cb6-24">  ) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb6-25">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">is.na</span>(incremental_power)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> </span>
<span id="cb6-26">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ggplot</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">aes</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> t, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> power, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">color =</span> is_stat_sig)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-27">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">geom_point</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-28">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> alpha) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span></span>
<span id="cb6-29">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme_bw</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">base_size =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>)</span></code></pre></div></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://jeffwong.github.io/posts/statistical_power/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="mde-as-the-inverse-of-statistical-power" class="level1">
<h1>MDE as the Inverse of Statistical Power</h1>
<p>MDE is the minimum detectable effect. It operates as the inverse to statistical power. Instead of asking: how much power do I have given an effect size and standard error, it asks: what is the effect size I can detect given a fixed amount of power and standard error.</p>
<p>To derive the MDE from first principles, we go back to the definition of power and how it is framed in terms of the rejection rule:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BPower%7D%20=%201%20-%20%5Cbeta%20=%20P(%5Cdelta%20%5Cgeq%20z_%7B1-%5Calpha/2%7D%20%7C%20H_A)%0A"></p>
<p>This equation about the CDF can be simplified. The noncentrality parameter is distributed <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Csim%20N(0,%201)">, so it being greater than or equal to <img src="https://latex.codecogs.com/png.latex?z_%7B1-%5Calpha/2%7D"> is actually just <img src="https://latex.codecogs.com/png.latex?1%20-%20%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20%5Cdelta)">. Then the derivation for the MDE is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5CPhi(z_%7B1-%5Calpha/2%7D%20-%20%5Cdelta)%20&amp;=%20%5Cbeta%20%5C%5C%0Az_%7B1-%5Calpha/2%7D%20-%20%5Cdelta%20&amp;=%20z_%7B%5Cbeta%7D%20%5C%5C%0A%5Cdelta%20&amp;=%20z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D%20%5C%5C%0A%5CDelta%20&amp;=%20%5Cboxed%7B(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%20%5Ccdot%20SE%7D%0A%5Cend%7Balign%7D"></p>
<p>Using common numbers, <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200.2">, the rule of thumb for MDE is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AMDE%20=%202.8%20%5Ccdot%20SE%0A"></p>
<p>Let’s unpack this common rule of thumb explicitly. It says that the smallest effect size we can detect while ensuring an 80% power is <img src="https://latex.codecogs.com/png.latex?2.8%20%5Ccdot%20se(%5Cbar%7B%5CDelta%7D)">. (The SE here is the residual standard error after controlling for other variables.)</p>
<p>This looks like a similar rule of thumb related to rejecting the null: reject if the effect size is larger than 1.96 SE. This separate rule of thumb can be very confusing. It describes when can we flag a result as stat sig while ensuring a different property: that under the null hypothesis there is less than a 5% chance of generating this effect by random. This rule of thumb is offering a different guarantee, not one about power. Ensuring 80% power at a specific effect size simply says that there is a good chance we can detect these effects, but it does not make it impossible. Even using power = 50% we will correctly flag some results as stat sig. (See multiple online discussions like this <a href="https://stats.stackexchange.com/questions/538590/reconciling-the-minimum-detectable-effect-calculation-with-the-experiment-result">one</a>) This is why we can reject at 1.96 SE, even though the minimum detectable effect using power = 80% is 2.8 SE. However, it is noteworthy that the scientific community is advocating for rejecting at 2.8 SE (See <a href="https://escholarship.org/content/qt1jx6091g/qt1jx6091g.pdf">Redefine Statistical Significance</a>)</p>
</section>
<section id="sample-size-calculator" class="level1">
<h1>Sample Size Calculator</h1>
<p>It’s easier for a business to think about designing an experiment around the MDE. It’s a more natural discussion: what is the smallest effect that the business would still care about?</p>
<p>Given an MDE, we pivot the problem again. If <img src="https://latex.codecogs.com/png.latex?se(%5Cbar%7B%5CDelta%7D)%20=%20%20%5Csqrt%7B%5Cfrac%7B%5Csigma_T%5E2%7D%7Bn_T%7D%20+%20%5Cfrac%7B%5Csigma_C%5E2%7D%7Bn_C%7D%7D%20=%20%5Csqrt%7BVar(%5Cbar%7By%7D_c)%20(1%20+%20%5Cfrac%7B1%7D%7Bk%7D)%7D">, and <img src="https://latex.codecogs.com/png.latex?n_T%20=%20k%20%5Ccdot%20n_C">, then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AMDE%20&amp;=%20(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%20%5Cfrac%7B%5Csigma%7D%7B%5Csqrt%7Bn_C%7D%7D%20%5Csqrt%7B1%20+%20%5Cfrac%7B1%7D%7Bk%7D%7D%20%5C%5C%0An_C%20&amp;=%20%5Cfrac%7B(z_%7B1-%5Calpha/2%7D%20-%20z_%7B%5Cbeta%7D)%5E2%20%5Csigma%5E2%20(%5Cfrac%7B1%7D%7Bk%7D%20+%201)%7D%7BMDE%5E2%7D%20%5C%5C%0An_T%20&amp;=%20k%20%5Ccdot%20n_C%0A%5Cend%7Balign%7D"></p>
<p>Using common numbers, <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200.2">, and <img src="https://latex.codecogs.com/png.latex?k%20=%201"> the rule of thumb for <img src="https://latex.codecogs.com/png.latex?n_c"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboxed%7Bn_C%20=%20n_T%20=%20%5Cfrac%7B16%20%5Csigma%5E2%7D%7BMDE%5E2%7D%7D%0A"></p>


</section>

 ]]></description>
  <category>experimentation</category>
  <category>statistics</category>
  <guid>https://jeffwong.github.io/posts/statistical_power/</guid>
  <pubDate>Mon, 15 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Mathematics of the Elastic Net</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/elastic_net/</link>
  <description><![CDATA[ 





<p><strong>Author’s note: This post is largely a rehash of many of the original elastic net and glmnet papers.</strong> I hope that having another voice describe the elegance of the elastic net will help others understand it. I have linked to all of the original documents to the best I can.</p>
<p>The elastic net adds L1 and L2 penalties to OLS, and is used to shrink coefficients towards zero. This can help with overfitting, as well as building an interpretive model from many features. When there is structure in coefficient-specific penalties, regularization can mimic a hierarchical model.</p>
<p>We start with a feature matrix, <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20p%7D">, a response vector, <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5En">, and a given <img src="https://latex.codecogs.com/png.latex?%5Calpha">. The elastic net formulates the problem</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D%20=%20%5Carg%5Cmin%20%5Csum_%7Bi=1%7D%5En%20(y_i%20-%5Cbeta_0%20-x_i%5ET%20%5Cbeta)%5E2%20+%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20(0.5(1-%5Calpha)%5Cbeta_j%5E2%20+%20%5Calpha%20%7C%5Cbeta_j%7C)."></p>
<p>The first term is the usual OLS term and the second term is a combination of L1 and L2 regularization.</p>
<section id="physical-interpretation-of-the-regularization" class="level1">
<h1>Physical Interpretation of the Regularization</h1>
<p>The 2 norm on <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> incentivizes the program to return coefficients that are small in magnitude. Likewise, the 1 norm incentivizes coefficients that are exactly zero. This prevents the exaggeration of effects in a model, while simultaneously serving as a form of model selection and interpretation.</p>
<p>Regularization is also similar to a prior. L2 regularization is similar to OLS with a gaussian prior on the parameters, that has a prior mean of 0 and a prior variance of <img src="https://latex.codecogs.com/png.latex?1/%5Clambda">. L1 regularization is similar to a laplacian prior. The relationship is explained <a href="https://papers.nips.cc/paper/1976-adaptive-sparseness-using-jeffreys-prior.pdf">here</a> with a compact stack overflow description <a href="https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior">here</a>.</p>
</section>
<section id="solving-the-program" class="level1">
<h1>Solving the Program</h1>
<p>When <img src="https://latex.codecogs.com/png.latex?X"> is centered and scaled to have zero mean and unit variance, the optimization problem can be solved using <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">coordinate descent</a>, with the update step:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D_j%20=%20%5Cfrac%7BS(%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(x_%7Bi,j%7D%5Cvarepsilon_i%20+%20%5Cbeta%5E%7B(%5Clambda)%7D_j),%20%5Clambda%20%5Calpha)%7D%7B1%20+%20%5Clambda(1%20-%20%5Calpha)%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?S(x,%20%5Clambda)%20=%20%5Ctext%7Bsign%7D(x)%20%5Ccdot%20(%7Cx%7C%20-%20%5Clambda)_+"> is the soft thresholding function.</p>
<p>This produces an algorithm with the form</p>
<pre><code># Given X, y, lambda, alpha.
for cycle in 1:max_cycles
  for j in 1:p
    for it in 1:max_iters
      beta_j = &lt;do update step above&gt;</code></pre>
</section>
<section id="searching-lambda" class="level1">
<h1>Searching <img src="https://latex.codecogs.com/png.latex?%5Clambda"></h1>
<p>The amount of regularization to use is always a question when fitting the elastic net. More regularization will more aggressively shrink the coefficients to zero. From the physical interpretation section above, regularization is like a prior, and careful thought also goes into choosing the prior. Usually, we cross validate and search for an optimal <img src="https://latex.codecogs.com/png.latex?%5Clambda"> that minimizes an out-of-sample metric. Fortunately there is a smart strategy for how to pick a starting set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> to explore (<a href="https://web.stanford.edu/~hastie/TALKS/glmnet.pdf">talk</a>, <a href="https://stats.stackexchange.com/questions/166630/glmnet-compute-maximal-lambda-value">stack overflow</a>).</p>
<p>Say a good set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> ranges from <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D">, and is logarithmically spaced apart, where <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> is the smallest <img src="https://latex.codecogs.com/png.latex?%5Clambda"> such that the coefficient vector is the zero vector and <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D"> is some multiple of <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D">.</p>
<p>When <img src="https://latex.codecogs.com/png.latex?X"> is centered and scaled to have zero mean and unit variance, and <img src="https://latex.codecogs.com/png.latex?y"> is centered to have zero mean, then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D%20=%20%5Cfrac%7B%5Cmax(%7CX%5ET%20y%7C)%7D%7Bn%20%5Calpha%7D."></p>
<p>In <code>glmnet::glmnet</code>, <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmin%7D%20=%20.0001%20%5Clambda_%7Bmax%7D"> if <img src="https://latex.codecogs.com/png.latex?n%20%3E%20p">. It should be noted that when <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200">, <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> does not exist, so <code>glmnet</code> intercepts <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and pretends it is 0.001.</p>
<p>Adding this layer to search for <img src="https://latex.codecogs.com/png.latex?%5Clambda"> means the optimization algorithm above gains a fourth nested for loop.</p>
<pre><code># Given X, y, alpha.
for cycle in 1:max_cycles
  for j in 1:p
    for l in lambda: 
      for it in 1:max_iters
        beta_j = &lt;do update step above&gt;</code></pre>
<p>This sounds like it is untractable, but there are several optimizations that can make the algorithm fast.</p>
</section>
<section id="computational-performance" class="level1">
<h1>Computational performance</h1>
<p>The above two sections are sufficient enough to build a lightweight elastic net solver. This section describes specific optimizations that make the algorithm faster, but ultimately are not relevant for how to use the elastic net as an end user.</p>
<section id="updates-via-covariance" class="level2">
<h2 class="anchored" data-anchor-id="updates-via-covariance">Updates via Covariance</h2>
<p>Note that the <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i"> term can be decomposed into <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D(y_%7Bi%7D%20-%20x_%7Bi%7D%5ET%20%5Cbeta)">. This can be computed very efficiently from a few vectorized operations that are computed just once outside of all of the loops. We first compute and store <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> and <img src="https://latex.codecogs.com/png.latex?X%5ET%20y">. When <img src="https://latex.codecogs.com/png.latex?X"> is sparse the linear algebra can be optimized. Then <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20=%20(X%5ET%20y)%5Bj%5D%20-%20(X%5ET%20X)%5B,j%5D%5ET%5Cbeta">, i.e. the j-th component of <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> and the dot product between the j-th column of <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</p>
</section>
<section id="reuse-xt-y-from-searching-lambda" class="level2">
<h2 class="anchored" data-anchor-id="reuse-xt-y-from-searching-lambda">Reuse <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> from searching <img src="https://latex.codecogs.com/png.latex?%5Clambda"></h2>
<p>When a smart set of <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is initialized, we can store the product <img src="https://latex.codecogs.com/png.latex?X%5ET%20y">, which is then used as part of the covariance update strategy.</p>
</section>
<section id="pathwise-coordinate-descent" class="level2">
<h2 class="anchored" data-anchor-id="pathwise-coordinate-descent">Pathwise Coordinate Descent</h2>
<p>The elastic net algorithm can compute the coefficient vector for several values of <img src="https://latex.codecogs.com/png.latex?%5Clambda">. Suppose we have a monotonically decreasing sequence for <img src="https://latex.codecogs.com/png.latex?%5Clambda">, <img src="https://latex.codecogs.com/png.latex?%7B%5Clambda%7D%20=%20%7B%5Clambda_%7Bmax%7D,%20%5Clambda_2,%20%5Cldots%7D">. By definition, the coefficient vector for <img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D"> is the zero vector. The next <img src="https://latex.codecogs.com/png.latex?%5Clambda"> in the sequence will have the update step <img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D_j%20=%200"> as long as <img src="https://latex.codecogs.com/png.latex?%7CX%5ETy%5Bj%5D%7C%20%3C%20%5Clambda%20%5Calpha%20n">. This check is a simple lookup since <img src="https://latex.codecogs.com/png.latex?X%5ET%20y"> is cached, and can bypass several update steps.</p>
</section>
<section id="active-sets" class="level2">
<h2 class="anchored" data-anchor-id="active-sets">Active Sets</h2>
<p>After doing one pass on the outermost loop that iterates on <code>cycles</code>, we check which coefficients are nonzero. In the second cycle, instead of iterating on the <img src="https://latex.codecogs.com/png.latex?p"> coefficients, we iterate only on the nonzero ones. These are the active sets. Finally, at the end we do one last cycle iterating on all coefficients. If the nonzeros have not changed, we conclude the algorithm.</p>
</section>
<section id="centering-and-scaling" class="level2">
<h2 class="anchored" data-anchor-id="centering-and-scaling">Centering and Scaling</h2>
<p>Much of the elastic net algorithm assumes <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> have been centered and scaled. Say we start with a feature matrix <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D"> which is not centered or scaled. Centering <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D"> makes it become dense, and many sparse linear algebra optimizations are lost.</p>
<p>Instead, we leverage the formula that centering and scaling can be written as</p>
<p><img src="https://latex.codecogs.com/png.latex?X%20=%20(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D."></p>
<p>with <img src="https://latex.codecogs.com/png.latex?%5Cmu_%5Ctilde%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_%5Ctilde%7Bx%7D"> column vectors containing the column means and column standard deviations of <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BX%7D">, and likewise for <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7By%7D">.</p>
<p>The key computations can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AX%5ET%20y%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20(%5Ctilde%7By%7D%20-%201%5Cmu_%5Ctilde%7By%7D).%5C%5C%0AX%5ET%20X%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%5Ctilde%7Bx%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%20n%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET.%0A%5Cend%7Balign%7D"></p>
</section>
</section>
<section id="elastic-net-with-weights" class="level1">
<h1>Elastic Net with Weights</h1>
<p>This section discusses the extension of elastic net to use weights, similar to weighted least squares.</p>
<section id="coordinate-descent-with-weights" class="level2">
<h2 class="anchored" data-anchor-id="coordinate-descent-with-weights">Coordinate Descent with Weights</h2>
<p>Assume that <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> have been centered and scaled <strong>without weights</strong>, so that their unweighted means are 0 and unweighted variances are 1. The update step for weighted elastic net is</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%5E%7B(%5Clambda)%7D%20=%20%5Cfrac%7BS(%5Csum_%7Bi=1%7D%5En%20(w_i%20x_%7Bi,j%7D(%5Cvarepsilon_i%20+%20x_%7Bi,j%7D%5Cbeta_j%5E%7B(%5Clambda)%7D)),%20%5Clambda%20%5Calpha)%7D%7B%5Csum_i%20w_i%20x_%7Bi,j%7D%5E2%20+%20%5Clambda(1%20-%20%5Calpha)%7D"></p>
<p>Though it looks more complex than before, using <img src="https://latex.codecogs.com/png.latex?w_i%20=%201/n"> will reduce the update step to the original unweighted update step.</p>
<p>Now suppose that <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?y"> were centered and scaled <strong>with weights</strong>, so that their weighted means are 0 and weighted variances are 1. By taking advantage of the definition <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20w_i%20x_%7Bi,j%7D%5E2%20=%20%5Csum_i%20w_i"> we can recover the more familiar formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%5E%7B(%5Clambda)%7D%20=%20%5Cfrac%7BS(%5Csum_%7Bi=1%7D%5En%20(w_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20+%20%5Cbeta_j%5E%7B(%5Clambda)%7D),%20%5Clambda%20%5Calpha)%7D%7B%5Csum_i%20w_i%20+%20%5Clambda(1%20-%20%5Calpha)%7D."></p>
<p>Like before, this update step can use vectorized operations. The key computations can be written as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AX%5ET%20W%20y%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5Ctext%7BDiagonal%7D(w)%20(%7B%5Ctilde%7By%7D%7D)%20%5C%5C%0A%20%20&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20(%7B%5Ctilde%7By%7D%7D)%20-%0A%20%20%20%20%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Cmu_%7B%5Ctilde%7BX%7D%7D%20w%5ET%20%5Ctilde%7By%7D.%20%5C%5C%0AX%5ET%20W%20X%20&amp;=%20%5B(%5Ctilde%7BX%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%5ET%20%5B(%7B%5Ctilde%7BX%7D%7D%20-%201%5Cmu_%7B%5Ctilde%7BX%7D%7D%5ET)%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%5D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%0A%20%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20w%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20-%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20w%5ET%20%5Ctilde%7BX%7D%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20+%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20%5Csum_i%20w_i%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20%5Ctilde%7BX%7D%5ET%20%5Ctext%7BDiagonal%7D(w)%20%5Ctilde%7BX%7D%20%5Cbegin%7Bbmatrix%7D%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%201%7D%20&amp;%20&amp;%20%5C%5C%20&amp;%20%5Cddots%20&amp;%20%5C%5C%20&amp;%20&amp;%201/%5Csigma_%7B%5Ctilde%7Bx%7D,%20p%7D%20%5Cend%7Bbmatrix%7D%20-%0A%20%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%20(%5Cfrac%7B%5Cmu_%5Ctilde%7Bx%7D%7D%7B%5Csigma_%5Ctilde%7Bx%7D%7D)%5ET%20%5Csum_i%20w_i.%20%5C%5C%0A%5Clambda_%7Bmax%7D%20&amp;=%20%5Cmax%20%5Cfrac%7B%7CX%5ET%20W%20y%7C%7D%7B%5Calpha%7D.%0A%5Cend%7Balign%7D"></p>
</section>
<section id="vectorizing-for-multiple-outcome-variables" class="level2">
<h2 class="anchored" data-anchor-id="vectorizing-for-multiple-outcome-variables">Vectorizing for Multiple Outcome Variables</h2>
<p>Many applications will track multiple outcome variables, so that <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20o%7D"> is a matrix of <img src="https://latex.codecogs.com/png.latex?o"> outcomes per observation. When the outcomes are independent, there is a fast way to fit multiple OLS regressions to the same feature matrix. Likewise, there is a fast way to do this for multiple elastic nets.</p>
<p>The bulk of the computation for a single <img src="https://latex.codecogs.com/png.latex?y"> is in the covariance update step</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_%7Bi,j%7D%5Cvarepsilon_i%20=%20(X%5ET%20y)%5Bj%5D%20-%20(X%5ET%20X)%5B,j%5D%5ET%5Cbeta."></p>
<p><img src="https://latex.codecogs.com/png.latex?y"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are column vectors. It is possible to update the j-th coefficient for all outcomes simultaneously. We vectorize over <img src="https://latex.codecogs.com/png.latex?o"> outcomes to produce and cache the intermediate matrix <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bp%20%5Ctimes%20o%7D">, and reuse <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> across outcomes.</p>
<p>However, different outcome variables can reach convergence differently. When updating the j-th coefficient, we would like to subset the columns of <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y"> to those outcomes which have not converged yet. This subsetting creates a deep copy of the matrix, and can be counter productive to the vectorization over multiple outcomes.</p>
<p>In practice, it may be easier to implement a job coordinator that computes <img src="https://latex.codecogs.com/png.latex?X%5ET%20Y"> and <img src="https://latex.codecogs.com/png.latex?X%5ET%20X"> apriori. These intermediates are stored in shared memory. Then, the coordinator assigns the task of estimating <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> for a single outcome to a worker, which reads the intermediates from shared memory.</p>
</section>
</section>
<section id="extensions" class="level1">
<h1>Extensions</h1>
<section id="differential-shrinkage" class="level2">
<h2 class="anchored" data-anchor-id="differential-shrinkage">Differential Shrinkage</h2>
<p>The standard description of the elastic net assumes a constant penalty across all coefficients, as seen in</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta%5E%7B(%5Clambda)%7D%20=%20%5Carg%5Cmin%20%5Csum_%7Bi=1%7D%5En%20(y_i%20-%5Cbeta_0%20-x_i%5ET%20%5Cbeta)%5E2%20+%20%5Clambda%20%5Csum_%7Bj=1%7D%5Ep%20(0.5(1-%5Calpha)%5Cbeta_j%5E2%20+%20%5Calpha%20%7C%5Cbeta_j%7C)."></p>
<p>Sometimes we want to augment the penalty for different coefficients. The library <code>glmnet</code> introduces the parameter <code>penalty.factor</code>, which multiplies the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> term by a <img src="https://latex.codecogs.com/png.latex?%5Cgamma_j%20%5Cgeq%200"> that varies for different coefficients. The algorithm for solving elastic net is flexible for differential shrinkage, where the loop over coefficients scales the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> penalty term by <img src="https://latex.codecogs.com/png.latex?%5Cgamma_j">. In addition, the initialization of the <img src="https://latex.codecogs.com/png.latex?%5Clambda"> path should use</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda_%7Bmax%7D%20=%20%5Cmax%20%5Ctext%7BDiagonal%7D(1/%5Cgamma)%20%5Cfrac%7B%7CX%5ET%20W%20y%7C%7D%7Bn%20%5Calpha%7D."></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li>https://web.stanford.edu/~hastie/TALKS/glmnet.pdf</li>
<li>https://web.stanford.edu/~hastie/Papers/glmnet.pdf</li>
<li>https://stats.stackexchange.com/questions/166630/glmnet-compute-maximal-lambda-value</li>
<li>https://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet</li>
<li>https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</li>
</ol>


</section>

 ]]></description>
  <category>mathematical statistics</category>
  <guid>https://jeffwong.github.io/posts/elastic_net/</guid>
  <pubDate>Wed, 07 Apr 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Tikhonov Regularization and Gaussian Priors</title>
  <dc:creator>Jeffrey Wong</dc:creator>
  <link>https://jeffwong.github.io/posts/l2_gaussian_prior/</link>
  <description><![CDATA[ 





<p>In this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Throughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, <img src="https://latex.codecogs.com/png.latex?y">, that is normally distributed and its mean is a linear combination of features, <img src="https://latex.codecogs.com/png.latex?x">, so that <img src="https://latex.codecogs.com/png.latex?Y%20%5Csim%20N(x%5ET%5Cbeta,%20%5CSigma)">.</p>
<p>Optionally, the parameter vector <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> can have a prior on it, in the form <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20N(%5Cmu_0,%20%5CSigma_0)">.</p>
</section>
<section id="maximum-likelihood-for-normally-distributed-data" class="level1">
<h1>Maximum Likelihood for Normally Distributed Data</h1>
<p>In frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Cbeta)%20=%20%5Cprod_i%20N(y_i%20%7C%20x_i,%20%5Cbeta,%20%5CSigma)%20=%20%5Cprod_i%20%5Cfrac%7B1%7D%7B%5Csqrt%7B(2%20%5Cpi)%5Ek%20%7C%5CSigma%7C%7D%7D%0Aexp(%7B-%5Cfrac%7B1%7D%7B2%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%7D)."> The MLE estimate for <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> will maximize the log-likelihood with respect to <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, by differentiating it and finding its root. This produces the MLE estimate</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMLE%7D%20=%20(X%5ET%20X)%5E%7B-1%7D%20X%5ET%20y."></p>
</section>
<section id="maximum-a-posteriori" class="level1">
<h1>Maximum a Posteriori</h1>
<p>When there is a gaussian prior in the form <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%5Csim%20N(%5Cmu_0,%20%5CSigma_0)">, we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0Ap(%5Cbeta%7Cy,%20x,%20%5CSigma)%20&amp;=%20%5Cprod_i%20N(y_i%20%7C%20x_i,%20%5Cbeta,%20%5CSigma)%20%5Ccdot%20N(%5Cbeta%20%7C%20%5Cmu_0,%20%5CSigma_0)%20%5C%5C%0A%20%20&amp;%5Cpropto%0A%20%20%5Cprod_i%20%5Cfrac%7B1%7D%7B%7C%5CSigma%7C%7D%0A%20%20exp(%7B-%5Cfrac%7B1%7D%7B2%7D%20%5Cbig((y_i%20-%20x_i%5ET%20%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y_i%20-%20x_i%5ET%20%5Cbeta)%20-%20(%5Cbeta%20-%20%5Cmu_0)%5ET%20%5CSigma_0%5E%7B-1%7D%20(%5Cbeta%20-%20%5Cmu_0)%5Cbig)%7D).%0A%5Cend%7Balign%7D"></p>
<p>The posterior turns out to be another normal distribution, <img src="https://latex.codecogs.com/png.latex?N(%5Cmu_1,%20%5CSigma_1)"> (<a href="https://en.wikipedia.org/wiki/Conjugate_prior">wiki</a>), where</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5CSigma_1%20&amp;=%20(%5CSigma_0%5E%7B-1%7D%20+%20n%20%5CSigma%5E%7B-1%7D)%5E%7B-1%7D%20%5C%5C%0A%5Cmu_1%20&amp;=%20%5CSigma_1%20(%5CSigma_0%5E%7B-1%7D%20%5Cmu_0%20+%20%5CSigma%5E%7B-1%7D%20%5Csum_i%7By_i%7D)%0A%5Cend%7Balign%7D"></p>
<p>The maximum a-posteriori estimator (<a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">wiki</a>) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMAP%7D%20=%20%5Carg%20max_%7B%5Cbeta%7D%0A-%20(y-X%5Cbeta)%5ET%20%5CSigma%5E%7B-1%7D%20(y-X%5Cbeta)%0A-%20(%5Cbeta-%5Cbeta_0)%5ET%20%5CSigma_0%5E%7B-1%7D%20(%5Cbeta-%5Cbeta_0)."> Recall that <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is fixed, and <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?%5CSigma_0"> are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%5E%7BMAP%7D%20=%20(X%5ET%20X%20+%20%5CSigma_0)%5E%7B-1%7D%20(X%5ET%20y%20+%20%5CSigma_0%20%5Cmu_0)."></p>
</section>
<section id="equivalence-between-mle-and-map" class="level1">
<h1>Equivalence between MLE and MAP</h1>
<p>When the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution.</p>


</section>

 ]]></description>
  <category>mathematical statistics</category>
  <guid>https://jeffwong.github.io/posts/l2_gaussian_prior/</guid>
  <pubDate>Sun, 27 Dec 2020 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
