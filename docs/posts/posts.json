[
  {
    "path": "posts/2021-04-09-r-is-a-great-research-environment/",
    "title": "R is a great Research Environment",
    "description": "A thoughtful exposition on how R makes data scientists more productive.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\n\n\n\nR is a data science language, and carries a unique spirit in providing data scientists tools to increase their productivity, and interactively analyze data. Other than being an open source standard, it also receives robust development from RStudio, which has been contributing several data science projects for the R community, such as tidyverse, ggplot, and tidymodels, and sets a high bar for data science software development. The R community has made admirable improvements to data science by improving functionality, performance, design, and both user and developer experience.\nHere are some of the highlights.\nAPI design in the community focuses on individual primitives that are lightweight and easy to map to a single thought, yet can be composed into more complex analyses.\nHigh performance computing through C++.\nRStudio maximizes developer flow by having data import, data modeling, data visualization, the code editor, package development tools, and git all in one IDE.\nThe makers within the R community have a great experience developing packages, including testing, writing and posting online documentation, creating interactive apps, and teaching. That creates a healthy environment for downstream users to always be aligned to best practices.\nFocus on Grammars for Data Analysis\nR has a deep focus on grammars for data analysis that feature very rich and thoughtful API design patterns. The grammars allow data scientists to compose arbitrary analyses in an intuitive way with a few, very robust primitives. This allows data scientists full creative freedom to do the analysis they want, without being blocked by functions that do not stack together. The developer experience becomes aligned and elegant, from the way that a data scientist thinks about an analysis, including inspecting data and intermediate outputs, to the way they communicate it for reproducible research, to the way the analysis is programmed. The grammars span across all critical components of data science: in data wrangling (dplyr & tidyr), data modeling (tidymodels), and data visualization (ggplot2).\nggplot2 - a Grammar for Graphics\n\n\n\nThe most elegant example of a grammar is ggplot2. ggplot composes plots through very simple primitives like geom_bar, geom_point, facet_grid, and coord_flip. Nearly all aspects of the plot are customizeable through ggplot primitives, and they are each scoped in a way that is intuitive to the user. By reading ggplot commands, you immediately have an understanding of what the user is plotting; a geom_bar means the viz will contain a barchart, and coord_flip means the plot will be rotated. Similarly, when you need to write ggplot commands, there is a natural intuition for how to change this plot: for example swapping geom_bar with geom_line will make this a linechart. ggplot2 is an effective way to communicate your visualization structure to a machine and to another person.\nThe code below shows the elegance of dplyr, ggplot, and the pipe, to walk through a filter and aggregation of the starwars dataset, and then to compose a bar plot of average mass per species.\n\n\nrequire(dplyr)\nrequire(ggplot2)\n\nstarwars %>%\n  group_by(species) %>%\n  summarise(\n    n = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  ungroup() %>%\n  filter(n > 1, mass > 50) %>%\n  ggplot(aes(x = species, y = mass, size = n)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() +\n  theme_bw(base_size = 16)\n\n\n\n\ndplyr & tidyr - a Grammar for Data Manipulation\n\n\n\ndplyr and tidyr have emerged as a grammar for data manipulation and data cleaning. They provide fundamental primitives in data manipulation with dplyr::group_by, dplyr::summarise, dplyr::filter, and dplyr::tally, but also provide primitives that handle much more complex cases, like tidyr::pivot_longer and tidyr::pivot_wider.\nThe grammar also has great ergonomics to minimize the amount of repetition when manipulating data. For example, many datasets name their variables with a specific convention. We may want to manipulate all variables from a specific group. In the example below, dplyr lets us aggregate all columns that have the suffix agg_me. We can even apply multiple aggregations easily.\n\n\nrequire(magrittr)\n\ndata = data.frame(group = sample(LETTERS, 100, replace = TRUE),\n                  y1_agg_me_not = 1:100,\n                  y2_agg_me = (1:100)^2,\n                  y3_agg_me = rnorm(100))\ndata %>% head() %>% kable\n\n\ngroup\ny1_agg_me_not\ny2_agg_me\ny3_agg_me\nW\n1\n1\n-0.0204550\nE\n2\n4\n0.0214538\nE\n3\n9\n-0.0458914\nJ\n4\n16\n1.4529694\nY\n5\n25\n-0.2254822\nL\n6\n36\n-0.6198267\n\n\n\nrequire(dplyr)\nrequire(tidyselect)\ndata %>%\n  filter(group != 'D') %>%\n  group_by(group) %>%\n  summarise(across(ends_with(\"agg_me\"), list(\"sum\" = sum, \"mean\" = mean))) %>%\n  ungroup() %>%\n  head() %>%\n  kable\n\n\ngroup\ny2_agg_me_sum\ny2_agg_me_mean\ny3_agg_me_sum\ny3_agg_me_mean\nA\n16748\n4187.000\n0.9162618\n0.2290654\nB\n14141\n4713.667\n-1.1713720\n-0.3904573\nC\n23225\n5806.250\n1.1870365\n0.2967591\nE\n8942\n2235.500\n3.7416921\n0.9354230\nF\n10935\n2733.750\n0.8967162\n0.2241790\nH\n23439\n2929.875\n3.8536697\n0.4817087\n\nWe can also pivot a “long” dataset into a “wide” one, especially for dynamic aggregation. Below, we have a common, “long” dataset listing impressions and clicks on various items for a user. Say we want to generate click through rates on each item, and then want to use those rates as features for a model.\n\n\ndata = data.frame(user = c(rep(\"J\", 3), rep(\"C\", 3), rep(\"N\", 3)),\n                  item = rep(c(\"Item1\", \"Item2\", \"Item3\"), 3),\n                  impressions = 1:9,\n                  clicks = 1:9)\ndata %>% head %>% kable\n\n\nuser\nitem\nimpressions\nclicks\nJ\nItem1\n1\n1\nJ\nItem2\n2\n2\nJ\nItem3\n3\n3\nC\nItem1\n4\n4\nC\nItem2\n5\n5\nC\nItem3\n6\n6\n\nUsing the long dataset it’s easy to create the per-user per-item click through rate.\n\n\ndata = data %>%\n  mutate(click_through_rate = clicks/impressions)\ndata %>% kable\n\n\nuser\nitem\nimpressions\nclicks\nclick_through_rate\nJ\nItem1\n1\n1\n1\nJ\nItem2\n2\n2\n1\nJ\nItem3\n3\n3\n1\nC\nItem1\n4\n4\n1\nC\nItem2\n5\n5\n1\nC\nItem3\n6\n6\n1\nN\nItem1\n7\n7\n1\nN\nItem2\n8\n8\n1\nN\nItem3\n9\n9\n1\n\nThen, we pivot the user-item level data into a user dataset, so that we have 3 user variables for the 3 click through rates.\n\n\nrequire(tidyr)\nrequire(tidyselect)\ndata %>%\n  pivot_wider(names_from = item, values_from = c(impressions, clicks, click_through_rate)) %>%\n  select(user, starts_with(\"click_through_rate\")) %>%\n  kable\n\n\nuser\nclick_through_rate_Item1\nclick_through_rate_Item2\nclick_through_rate_Item3\nJ\n1\n1\n1\nC\n1\n1\n1\nN\n1\n1\n1\n\ndplyr’s interface is consistent across many inputs, not just R dataframes. The input can be a spark dataframe, or even a parquet file, making it easy to transition from prototype code to production code. All of the above dplyr code can be executed agnostically, adding to the elegance and thoughtful thinking that goes into R grammars.\nRecipes - a Grammar for Data Preprocessing\n\n\n\nrecipes is the most recent grammar added to R, and is targeted towards improving data preprocessing, a universal challenge for all data scientists. The grammar lets you define several data preprocessing steps that can be composed into a recipe that transforms a data.frame. The recipe can then be applied to other datasets, giving complete reproducibility to data. For example, the recipe below trains an imputation step, which is then baked from a data.frame called data.\n\n\nrequire(recipes)\n\nn = 100\ndata = data.frame(\n  y = rnorm(n),\n  categorical = sample(LETTERS, n, replace = TRUE),\n  numerical = rnorm(n),\n  constant = 1\n)\n\nrec = recipe(y ~ categorical + numerical + constant, data = data) %>%\n  step_meanimpute(all_numeric()) %>%\n  step_modeimpute(all_nominal()) %>%\n  prep(data) %>%\n  bake(data)\n\n\n\nThis recipe can be reapplied to any dataframe, including an out of sample dataset, and will reapply the same imputation logic from data. For machine learning, you can preprocess a test set in the exact same way you preprocess the training set.\nHigh Performance Computing\n\n\n\nRcpp provides a boost to data science productivity by making functions faster. It offers rich integration between R and C++. Very similar to cython and pybind11 in Python, Rcpp can compile low level C and C++ code that can be orders of magnitude faster than interactive languages like R. For example, for loops can be made significantly faster by compiling them in C++. OpenMP and Intel TBB can make C++ algorithms multithreaded, instead of having to rely on Unix like forks. Memory allocation can be controlled better, reducing the memory footprint of an algorithm while simultaneously making it faster.\nJust like the beautiful ergonomics in dplyr, Rcpp takes care of many barriers of entry into high performance computing. The package will compile C++ source code, load the C symbol table into R, and convert between R types and C++ types for you. Using Rcpp::sourceCpp or devtools::load_all, you can have an editor open, write C++ code, and immediately bring optimized functions into R without restarting the session or managing a Makefile. This combines the performance of C++ with the interactivity of R.\nBecause of Rcpp’s ease of use, there are over 1000 R packages using Rcpp. This aspect of computing is one of the biggest reasons why R has changed so much in recent years.\nMultilingual Data Science\n\n\n\nRStudio has been pivoting itself from not only developing R software, but also developing data science tools in general. For example, see its page on the Essentials of Data Science and Advanced Data Science.\nReticulate is the most noteworthy package. It allows an R user to invoke Python functions while still inside an R session. The package takes care of converting Python and R data types for you: it is possible to invoke a python function that uses an R dataframe as input, and under the hood the dataframe will be converted to a pandas type. Data scientists that are multilingual in Python and R do not need to have two notebooks running in parallel. At the same time, broader data science collaboration is enabled.\nReticulate’s integration is broad. It is embedded in RStudio and RMarkdown, where you can develop Python scripts just as easily as R scripts. You can even create RStudio Notebooks that weave native Python code cells and R code cells together, where RStudio will transfer data types between languages for you.\nThrough Python and C++ bindings, R is able to elegantly integrate with Tensorflow and Keras, which were primarily developed in Python and C++. This allows RStudio to act as a primary hub for multiple data science languages.\nPackage Development\n\n\n\nDeveloping a package in R that can be distributed to others is an easy and uniquely interactive experience. The package devtools provides all the tools a developer will need, and you will not need to exit the R session to go to bash or any other environment. This minimizes distractions and creates a productive developer workflow. Having a good package development experience is crucial for growing a productive community of open source developers.\nTo create a package, a user collects a series of .R files into an /R/ directory as if they were going to be sourced. To import the package into the R session, just run devtools::load_all(). During the development process, modify the source files directly in RStudio, and reload the package again using devtools::load_all(). You will not need to restart the R session. To create manual pages according to docstrings, simply run devtools::document(). To test the package, simply run devtools::test(). To release a package to the open source simply run devtools::release(). All of this is done without toggling environments; the developer can stay inside RStudio.\nRunning tests interactively is a major productivity boost to debugging software. Tests are written in a self contained script, so you can run lines one by one to prepare data and run your functions. R is meant to be interactive, so when a test fails you can use the current R session to rerun a test and use debugger tools to figure out what is wrong. You will not need separate test infrastructure, so debugging a unit test truly feels like combing your code line by line. Being able to stay inside one single R session without restarts or other third party infrastructure streamlines rapid iteration of editing source code, running unit tests, running a debugger, and releasing code.\nThe R community has thousands of packages and open source developers. To maintain a healthy ecosystem where packages are compatible with each other, developers can make use of devtools::revdep(). This function will lookup all packages that depend on package X, and run their unit tests against the development version of package X, allowing developers to make safe releases into the community.\nRStudio\n\n\n\nRStudio is an IDE, notebook, debugger, and profiler all in one. You do not need separate UIs or terminals to iterate and produce high quality code. This creates great developer flow.\nFor better productivity, you can run R on remote servers that are always on and always available. Combined with AWS’s customizeable hardware, this lets you do data science from anywhere with any amount of resources. RStudio Server lets you connect to RStudio on a remote machine using your browser. If you don’t want to maintain the hardware, you can also use RStudio Cloud. This is one of the most advanced browser based data science IDEs that exists.\nPackage Documentation\n\n\n\nDocumenting software is a crucial part of developing successful open source software. In order to produce high quality software, the experience of writing documentation needs to be frictionless. R makes developing packages with high quality documentation easy. To document a function, simply add a docstring next to the function’s source code. To create documentation on larger topics, you can use RMarkdown to create a notebook vignette that contains any combination of text, HTML, LaTeX and R/Python/C++ code cells. These notebooks can become part of the package documentation. As seen many times before, R takes common workflows - for example creating a Notebook to demo code - and turns them into paths for writing documentation.\nUsing pkgdown and pkgdown::build_site() you can build beautiful html websites containing documentation and vignettes from your package. The function builds the entire website, html files, and other assets that can be simply uploaded to github and turned into a website using github pages.\nUsing distill you can create an entire data science blog, such as this one!\n\n\n\n",
    "preview": "https://github.com/tidyverse/ggplot2/raw/master/man/figures/logo.png",
    "last_modified": "2021-04-09T14:35:19-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-09-the-mathematics-of-the-elastic-net/",
    "title": "The Mathematics of the Elastic Net",
    "description": "OLS with L1 and L2 penalties.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2021-04-07",
    "categories": [],
    "contents": "\nAuthor’s note: This post is largely a rehash of many of the original elastic net and glmnet papers. I hope that having another voice describe the elegance of the elastic net will help others understand it. I have linked to all of the original documents to the best I can.\nThe elastic net adds L1 and L2 penalties to OLS, and is used to shrink coefficients towards zero. This can help with overfitting, as well as building an interpretive model from many features. When there is structure in coefficient-specific penalties, regularization can mimic a hierarchical model.\nWe start with a feature matrix, \\(X \\in \\mathbb{R}^{n \\times p}\\), a response vector, \\(y \\in \\mathbb{R}^n\\), and a given \\(\\alpha\\). The elastic net formulates the problem\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nThe first term is the usual OLS term and the second term is a combination of L1 and L2 regularization.\nPhysical Interpretation of the Regularization\nThe 2 norm on \\(\\beta\\) incentivizes the program to return coefficients that are small in magnitude. Likewise, the 1 norm incentivizes coefficients that are exactly zero. This prevents the exaggeration of effects in a model, while simultaneously serving as a form of model selection and interpretation.\nRegularization is also similar to a prior. L2 regularization is similar to OLS with a gaussian prior on the parameters, that has a prior mean of 0 and a prior variance of \\(1/\\lambda\\). L1 regularization is similar to a laplacian prior. The relationship is explained here with a compact stack overflow description here.\nSolving the Program\nWhen \\(X\\) is centered and scaled to have zero mean and unit variance, the optimization problem can be solved using coordinate descent, with the update step:\n\\[\\beta^{(\\lambda)}_j = \\frac{S(\\frac{1}{n} \\sum_{i=1}^n (x_{i,j}\\varepsilon_i + \\beta^{(\\lambda)}_j), \\lambda \\alpha)}{1 + \\lambda(1 - \\alpha)}\\]\nwhere \\(S(x, \\lambda) = \\text{sign}(x) \\cdot (|x| - \\lambda)_+\\) is the soft thresholding function.\nThis produces an algorithm with the form\n# Given X, y, lambda, alpha.\nfor cycle in 1:max_cycles\n  for j in 1:p\n    for it in 1:max_iters\n      beta_j = <do update step above>\nSearching \\(\\lambda\\)\nThe amount of regularization to use is always a question when fitting the elastic net. More regularization will more aggressively shrink the coefficients to zero. From the physical interpretation section above, regularization is like a prior, and careful thought also goes into choosing the prior. Usually, we cross validate and search for an optimal \\(\\lambda\\) that minimizes an out-of-sample metric. Fortunately there is a smart strategy for how to pick a starting set of \\(\\lambda\\) to explore (talk, stack overflow).\nSay a good set of \\(\\lambda\\) ranges from \\(\\lambda_{max}\\) to \\(\\lambda_{min}\\), and is logarithmically spaced apart, where \\(\\lambda_{max}\\) is the smallest \\(\\lambda\\) such that the coefficient vector is the zero vector and \\(\\lambda_{min}\\) is some multiple of \\(\\lambda_{max}\\).\nWhen \\(X\\) is centered and scaled to have zero mean and unit variance, and \\(y\\) is centered to have zero mean, then\n\\[\\lambda_{max} = \\frac{\\max(|X^T y|)}{n \\alpha}.\\]\nIn glmnet::glmnet, \\(\\lambda_{min} = .0001 \\lambda_{max}\\) if \\(n > p\\). It should be noted that when \\(\\alpha = 0\\), \\(\\lambda_{max}\\) does not exist, so glmnet intercepts \\(\\alpha\\) and pretends it is 0.001.\nAdding this layer to search for \\(\\lambda\\) means the optimization algorithm above gains a fourth nested for loop.\n# Given X, y, alpha.\nfor cycle in 1:max_cycles\n  for j in 1:p\n    for l in lambda: \n      for it in 1:max_iters\n        beta_j = <do update step above>\nThis sounds like it is untractable, but there are several optimizations that can make the algorithm fast.\nComputational performance\nThe above two sections are sufficient enough to build a lightweight elastic net solver. This section describes specific optimizations that make the algorithm faster, but ultimately are not relevant for how to use the elastic net as an end user.\nUpdates via Covariance\nNote that the \\(\\sum_i x_{i,j}\\varepsilon_i\\) term can be decomposed into \\(\\sum_i x_{i,j}(y_{i} - x_{i}^T \\beta)\\). This can be computed very efficiently from a few vectorized operations that are computed just once outside of all of the loops. We first compute and store \\(X^T X\\) and \\(X^T y\\). When \\(X\\) is sparse the linear algebra can be optimized. Then \\(\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta\\), i.e. the j-th component of \\(X^T y\\) and the dot product between the j-th column of \\(X^T X\\) and \\(\\beta\\).\nReuse \\(X^T y\\) from searching \\(\\lambda\\)\nWhen a smart set of \\(\\lambda\\) is initialized, we can store the product \\(X^T y\\), which is then used as part of the covariance update strategy.\nPathwise Coordinate Descent\nThe elastic net algorithm can compute the coefficient vector for several values of \\(\\lambda\\). Suppose we have a monotonically decreasing sequence for \\(\\lambda\\), \\({\\lambda} = {\\lambda_{max}, \\lambda_2, \\ldots}\\). By definition, the coefficient vector for \\(\\lambda_{max}\\) is the zero vector. The next \\(\\lambda\\) in the sequence will have the update step \\(\\beta^{(\\lambda)}_j = 0\\) as long as \\(|X^Ty[j]| < \\lambda \\alpha n\\). This check is a simple lookup since \\(X^T y\\) is cached, and can bypass several update steps.\nActive Sets\nAfter doing one pass on the outermost loop that iterates on cycles, we check which coefficients are nonzero. In the second cycle, instead of iterating on the \\(p\\) coefficients, we iterate only on the nonzero ones. These are the active sets. Finally, at the end we do one last cycle iterating on all coefficients. If the nonzeros have not changed, we conclude the algorithm.\nCentering and Scaling\nMuch of the elastic net algorithm assumes \\(X\\) and \\(y\\) have been centered and scaled. Say we start with a feature matrix \\(\\tilde{X}\\) which is not centered or scaled. Centering \\(\\tilde{X}\\) makes it become dense, and many sparse linear algebra optimizations are lost.\nInstead, we leverage the formula that centering and scaling can be written as\n\\[X = (\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}.\\]\nwith \\(\\mu_\\tilde{x}\\) and \\(\\sigma_\\tilde{x}\\) column vectors containing the column means and column standard deviations of \\(\\tilde{X}\\), and likewise for \\(\\tilde{y}\\).\nThe key computations can be written as:\n\\[\\begin{align}\nX^T y &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T (\\tilde{y} - 1\\mu_\\tilde{y}).\\\\\nX^T X &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} - n (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T.\n\\end{align}\\]\nElastic Net with Weights\nThis section discusses the extension of elastic net to use weights, similar to weighted least squares.\nCoordinate Descent with Weights\nAssume that \\(X\\) and \\(y\\) have been centered and scaled without weights, so that their unweighted means are 0 and unweighted variances are 1. The update step for weighted elastic net is\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}(\\varepsilon_i + x_{i,j}\\beta_j^{(\\lambda)})), \\lambda \\alpha)}{\\sum_i w_i x_{i,j}^2 + \\lambda(1 - \\alpha)}\\]\nThough it looks more complex than before, using \\(w_i = 1/n\\) will reduce the update step to the original unweighted update step.\nNow suppose that \\(X\\) and \\(y\\) were centered and scaled with weights, so that their weighted means are 0 and weighted variances are 1. By taking advantage of the definition \\(\\sum_i w_i x_{i,j}^2 = \\sum_i w_i\\) we can recover the more familiar formula\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}\\varepsilon_i + \\beta_j^{(\\lambda)}), \\lambda \\alpha)}{\\sum_i w_i + \\lambda(1 - \\alpha)}.\\]\nLike before, this update step can use vectorized operations. The key computations can be written as:\n\\[\\begin{align}\nX^T W y &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T \\text{Diagonal}(w) ({\\tilde{y}}) \\\\\n  &= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) ({\\tilde{y}}) - \n     \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\mu_{\\tilde{X}} w^T \\tilde{y}. \\\\\nX^T W X &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [({\\tilde{X}} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T w (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) w^T \\tilde{X}\\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} +\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i. \\\\\n\\lambda_{max} &= \\max \\frac{|X^T W y|}{\\alpha}.\n\\end{align}\\]\nVectorizing for Multiple Outcome Variables\nMany applications will track multiple outcome variables, so that \\(Y \\in \\mathbb{R}^{n \\times o}\\) is a matrix of \\(o\\) outcomes per observation. When the outcomes are independent, there is a fast way to fit multiple OLS regressions to the same feature matrix. Likewise, there is a fast way to do this for multiple elastic nets.\nThe bulk of the computation for a single \\(y\\) is in the covariance update step\n\\[\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta.\\]\n\\(y\\) and \\(\\beta\\) are column vectors. It is possible to update the j-th coefficient for all outcomes simultaneously. We vectorize over \\(o\\) outcomes to produce and cache the intermediate matrix \\(X^T Y \\in \\mathbb{R}^{p \\times o}\\), and reuse \\(X^T X\\) across outcomes.\nHowever, different outcome variables can reach convergence differently. When updating the j-th coefficient, we would like to subset the columns of \\(X^T Y\\) to those outcomes which have not converged yet. This subsetting creates a deep copy of the matrix, and can be counter productive to the vectorization over multiple outcomes.\nIn practice, it may be easier to implement a job coordinator that computes \\(X^T Y\\) and \\(X^T X\\) apriori. These intermediates are stored in shared memory. Then, the coordinator assigns the task of estimating \\(\\beta\\) for a single outcome to a worker, which reads the intermediates from shared memory.\nExtensions\nDifferential Shrinkage\nThe standard description of the elastic net assumes a constant penalty across all coefficients, as seen in\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nSometimes we want to augment the penalty for different coefficients. The library glmnet introduces the parameter penalty.factor, which multiplies the \\(\\lambda\\) term by a \\(\\gamma_j \\geq 0\\) that varies for different coefficients. The algorithm for solving elastic net is flexible for differential shrinkage, where the loop over coefficients scales the \\(\\lambda\\) penalty term by \\(\\gamma_j\\). In addition, the initialization of the \\(\\lambda\\) path should use\n\\[\\lambda_{max} = \\max \\text{Diagonal}(1/\\gamma) \\frac{|X^T W y|}{n \\alpha}.\\]\nReferences\nhttps://web.stanford.edu/~hastie/TALKS/glmnet.pdf\nhttps://web.stanford.edu/~hastie/Papers/glmnet.pdf\nhttps://stats.stackexchange.com/questions/166630/glmnet-compute-maximal-lambda-value\nhttps://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet\nhttps://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-09T14:26:09-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-27-equivalence-between-tikhonov-regularization-and-gaussian-priors/",
    "title": "The Special Equivalence between Tikhonov Regularization and Gaussian Priors",
    "description": "Straddling Bayesian and Frequentist Statistics.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2020-12-27",
    "categories": [],
    "contents": "\nIn this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.\nIntroduction\nThroughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, \\(y\\), that is normally distributed and its mean is a linear combination of features, \\(x\\), so that \\(Y \\sim N(x^T\\beta, \\Sigma)\\).\nOptionally, the parameter vector \\(\\beta\\) can have a prior on it, in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\).\nMaximum Likelihood for Normally Distributed Data\nIn frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of \\(\\beta\\) is\n\\[ L(\\beta) = \\prod_i N(y_i | x_i, \\beta, \\Sigma) = \\prod_i \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}} \nexp({-\\frac{1}{2} (y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta)}).\\] The MLE estimate for \\(\\beta\\) will maximize the log-likelihood with respect to \\(\\beta\\), by differentiating it and finding its root. This produces the MLE estimate\n\\[\\hat{\\beta}^{MLE} = (X^T X)^{-1} X^T y.\\]\nMaximum a Posteriori\nWhen there is a gaussian prior in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\), we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of \\(\\beta\\). Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.\n\\[\\begin{align}\np(\\beta|y, x, \\Sigma) &= \\prod_i N(y_i | x_i, \\beta, \\Sigma) \\cdot N(\\beta | \\mu_0, \\Sigma_0) \\\\\n  &\\propto\n  \\prod_i \\frac{1}{|\\Sigma|} \n  exp({-\\frac{1}{2} \\big((y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta) - (\\beta - \\mu_0)^T \\Sigma_0^{-1} (\\beta - \\mu_0)\\big)}).\n\\end{align}\\]\nThe posterior turns out to be another normal distribution, \\(N(\\mu_1, \\Sigma_1)\\) (wiki), where\n\\[\\begin{align}\n\\Sigma_1 &= (\\Sigma_0^{-1} + n \\Sigma^{-1})^{-1} \\\\\n\\mu_1 &= \\Sigma_1 (\\Sigma_0^{-1} \\mu_0 + \\Sigma^{-1} \\sum_i{y_i})\n\\end{align}\\]\nThe maximum a-posteriori estimator (wiki) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives\n\\[\\hat{\\beta}^{MAP} = \\arg max_{\\beta} \n- (y-X\\beta)^T \\Sigma^{-1} (y-X\\beta)\n- (\\beta-\\beta_0)^T \\Sigma_0^{-1} (\\beta-\\beta_0).\\] Recall that \\(\\Sigma\\) is fixed, and \\(\\mu_0\\) and \\(\\Sigma_0\\) are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.\n\\[\\hat{\\beta}^{MAP} = (X^T X + \\Sigma_0)^{-1} (X^T y + \\Sigma_0 \\mu_0).\\]\nEquivalence between MLE and MAP\nWhen the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-07T13:25:22-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-25-choose-distill-for-writing-a-tech-blog/",
    "title": "Choose RMarkdown + Distill for writing a Tech Blog",
    "description": "RMarkdown and Distill are powerful tools for scientific communication.",
    "author": [
      {
        "name": "Jeffrey C. Wong",
        "url": {}
      }
    ],
    "date": "2020-12-25",
    "categories": [],
    "contents": "\nRMarkdown is a Champion for Scientific Communication\nRMarkdown has had a long history in scientific communication. Using RMarkdown, you can create beautifully rendered technical documents that can be hosted online or sent as pdfs. The documents weave together the output from code, latex, as well as other web tools like html, css, and javascript. By putting data analysis code right next to text, the author can describe and discuss individual steps in analysis, and show how to reproduce it using software. RMarkdown allows authors to construct an entire scientific narrative with the spirit of the scientific method. It facilitates this approach to scientific communication by creating great development flow between code and discussion: the code you write for development is the same code that you share to others to let them read and follow along.\nHere are examples of how RMarkdown embeds essential parts of scientific communication into one environment. First, we can write latex.\n\\[ \\int_0^1 x dx = \\frac{1}{2}.\\]\nNext, we can illustrate source code for functions\n\n\nfoo = function(x, y) {\n  (x-y)^2 %>% sum() %>% sqrt()\n}\n\n\n\nFinally, we can execute code inline, and can also generate visualizations.\n\n\na = 5\nb = a^2\nprint(b)\n\n\n[1] 25\n\n\n\n\nDistill is a Better Publishing Platform for Research\nDistill is both a publishing framework and machine learning research journal. It is advancing scientific communication by breaking the barriers of traditional pdf documents. Distill articles are html pages that allow publishing new kinds of scientific narratives, including those that have interactive visualizations, videos, and demos. It also follows scientific writing, listing the authors, date published, references, footnotes, and an appendix.\nDevelopers at RStudio have integrated Distill articles into RMarkdown with the #rstats Distill package. RMarkdown users can create Distill articles that can be submitted to the research journal. In addition, they’ve made it easy to collate Distill html articles into an online blog. This further enhances scientific communication because your development environment can also be used for online publishing and content management, creating a single end-to-end environment for development, sharing, and publishing. The Distill package creates the listing page that indexes all blog posts, adds a search bar to the blog, and adds comments and share links to each blog post. You can also link to the underlying RMarkdown source code to show how the blog post was generated. In this way, the blog post turns into a technical document that embodies RMarkdown’s reproducible research features.\nA Simple End-to-End Tech Stack\nA full Distill blog can be hosted using as few as two components, that are completely free. Publishing can be done without maintaining a server or a database.\nRMarkdown, and Distill to build the blog. When the blog is built, a series of .html files are generated that can be uploaded to a webhost.\ngithub pages can host the blog online for free.\n\n\n\n",
    "preview": "posts/2020-12-25-choose-distill-for-writing-a-tech-blog/choose-distill-for-writing-a-tech-blog_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-07T13:25:22-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
