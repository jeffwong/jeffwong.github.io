[
  {
    "objectID": "posts/statistical_power/index.html",
    "href": "posts/statistical_power/index.html",
    "title": "Foundations in Statistical Power",
    "section": "",
    "text": "Statistical power is a concept that helps us plan for an effective experiment. Power is typically described in terms of a purely randomized and controlled trial, where we can take advantage of certain properties like independence. It is also typically described in the context of measuring the difference in means, which also has properties like the Central Limit Theorem that it takes advantage of. Finally, an assumption in typical description is that all units that are assigned to the treatment group are successfully treated, and all units assigned to the control are successfully withheld.\nBut not all experiments are so perfectly planned and executed. It is important to understand the history of how statistical power was derived, from first principles, so that we can adapt it and innovate. For example, some experiments do not have perfect compliance. Many experiments will try to give treatment to a subject, but that subject refuses, or forgets, to use the treatment. This complexity can add layers to the question: “what should we measure? The average effect or the average effect among the compliers?” With this complication we will need to revisit our formula for statistical power. We will do this innovation exercise in the next post. For now, let us understand the history of statistical power.\n\nPower is always associated with a hypothesis test. \\[\\text{Power} = 1 - \\beta = P(\\text{Reject }H_0 | H_A)\\] where \\(\\alpha\\) is the type 1 error rate, and \\(\\beta\\) is the type 2 error rate.\nSo to derive power, we must first identify the rejection rules of the hypothesis test, then evaluate the probability of rejection if the alternative is true.\nTo illustrate, we specify a very generic hypothesis test, the difference in means. While frequently associated with the t test, it is not necessarily tied to it. The hypothesis about the difference in means is\n\\[\\begin{align}\nH_0: \\mu_1 - \\mu_0 &= 0 \\\\\nH_A: \\mu_1 - \\mu_0 &\\neq 0\n\\end{align}\\]\n\nCentral Limit Theorem\nFrom the Central Limit Theorem, we know the distribution of a sample mean is \\(\\bar{X} \\sim N(\\mu, \\sigma^2 / n)\\) and the standard error on \\(\\bar{X}\\) is \\(se(\\bar{X}) = \\sigma/\\sqrt{n}\\). The Z statistic is a transformation with \\(Z = \\frac{\\bar X - \\mu}{se(\\bar{X})} \\sim N(0, 1)\\). Now we apply information from the hypothesis test, where the mean in question is the difference in two means.\n\n\nRejection Rule\nIn order to reject the null, we must first assume that the null is true. Then we must show that the sample mean for \\(\\Delta\\) is sufficiently different from 0, even when the true governing parameter is 0. This is conveniently done by working with the normalized Z statistic. In the case of the null, the sample mean \\(\\bar{\\Delta} \\sim N(\\Delta, \\sigma^2)\\) and \\(\\Delta = 0\\). Converting to a normalized Z statistic we have \\(Z = \\frac{\\bar{\\Delta}}{{SE(\\bar{\\Delta})}} \\sim N(0, 1)\\). Now, we can state the rejection rule: reject if \\(|Z| \\geq z_{1-\\alpha/2}\\). Equivalently \\[\\begin{align}\n\\left|\\frac{\\bar{\\mu}_1 - \\bar{\\mu}_0}{se(\\bar{\\Delta})}\\right| &\\geq z_{1-\\alpha/2}\n\\end{align}\\] where \\(se(\\bar{\\Delta}) = \\sqrt{\\frac{\\sigma^2}{n_T} + \\frac{\\sigma^2}{n_C}}\\) under randomization. If \\(n_T = k \\cdot n_C\\), then we can use the reduction\n\\[\\begin{align}\nVar(\\bar{y}_c) &= \\frac{\\sigma^2}{n_C} \\\\\nse(\\bar{\\Delta}) &= \\sqrt{Var(\\bar{y}_c) (1 + \\frac{1}{k})}\n\\end{align}\\]\n\n\nProbability under the Alternative\nPower is the probability of triggering the rejection rules, which were derived under the null \\(H_0: \\Delta = 0\\), when the true data generating process has \\(\\Delta \\neq 0\\). It is important to understand which governing parameter is in play here. The rejection rules will be derived using \\(\\Delta = 0\\), and those rules will be fixed. Then, we toggle the governing parameter to have \\(\\Delta \\neq 0\\), and benchmark the probability that data under this governing parameter will hit the rejection rule.\nSay that \\(H_A\\) is true and there are two distinct means \\(\\mu_0\\) and \\(\\mu_1\\) with \\(\\mu_1 - \\mu_0 = k \\neq 0\\). Under \\(H_A\\) we cannot claim the usual Z statistic is distributed N(0, 1). Instead, we have\n\\[\\begin{align}\nZ | H_A &= \\frac{\\bar{\\Delta}}{se(\\bar{\\Delta})} \\sim N(\\frac{k}{se(\\bar{\\Delta})},1) \\\\\n\\end{align}\\]\nNow we revisit the rejection rule: reject when \\(|Z| \\geq z_{1-\\alpha/2}\\). Power is the probability of triggering the rejection rule when \\(H_A\\) is true, so\n\\[\\begin{align}\nPower &= P(|Z| \\geq z_{1-\\alpha/2} | Z \\sim N(\\frac{k}{se(\\bar{\\Delta})}, 1))\n\\end{align}\\]\nLet \\(\\delta = \\frac{\\Delta | H_A}{se(\\bar{\\Delta})}\\) be the noncentrality parameter, using the effect size we would like to detect under \\(H_A\\). The final solution for power is \\[\n\\boxed{\\text{Power} = \\Phi(\\delta - z_{1-\\alpha/2}) + \\Phi(-\\delta - z_{1-\\alpha/2})}\n\\]\nUsing first principles, we can implement code as\n\ntreatment_effect = .01\nsigma2_treatment = sigma2_control = sigma2 = 2\nn = 1e3\nalpha = .05\ntreatment_share = .5\ncontrol_share = .5\nn_treatment = n * treatment_share\nn_control = n * control_share\n\npooled_se = sqrt(sigma2_treatment / n_treatment + sigma2_control / n_control)\npooled_ncp = treatment_effect / pooled_se\n\ncrit = qnorm(1-alpha/2)\npnorm(-crit - pooled_ncp) + (pnorm(-crit + pooled_ncp))\n\n[1] 0.05143313\n\n\nThis matches the native implementation in R\n\npower.t.test(\n  n = n * treatment_share,\n  delta = treatment_effect,\n  sd = sqrt(sigma2),\n  strict = TRUE\n)\n\n\n     Two-sample t test power calculation \n\n              n = 500\n          delta = 0.01\n             sd = 1.414214\n      sig.level = 0.05\n          power = 0.05143037\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nHow Power Changes\nA lot of research in experimentation goes into methods to increase statistical power. Using the formula, the variables that drive variance are:\n\nThe treatment effect, \\(\\mu_1 - \\mu_0\\).\nThe critical value, determined by \\(\\alpha\\).\nSample size, \\(n\\).\nVariance, \\(\\sigma^2\\).\n\nBy changing the treatment effect that we want to detect, we can change power. However, that may jeopardize the practical value of an experiment. If we only power our experiment to detect large treatment effects that in practice do not happen, then the experiment is useless. Increasing \\(\\alpha\\) is also a simple change, but it also increases the false positive rate. Increasing sample size decreases the standard error, but it will take more time and resources to accumulate the extra data.\nFinally, the last path to improve power has a lot of subtlety. The role variance plays in the power formula is not fixed, it can vary. Its role is more precisely called “unexplained variance”. If there is a model that relates the metric, \\(y\\), with the treatment variable and exogeneous covariates, \\(X\\), then \\(\\sigma^2\\) is actually \\(\\text{Var}(y | X)\\), or equivalently the variance of the residuals after we use \\(X\\) to explain part of the variance. Using a good set of \\(X\\) variables, we can make the unexplained variance small.\nWhich of these levers should we pursue to get maximal power? How much will power change? To answer this, we combine \\(n\\) and variance into the more general variable, standard error. We merge the standard error and the effect size using the more general variable \\(\\delta\\). Below we plot power as \\(\\delta\\) and \\(\\alpha\\) vary. If we wanted to unpack the specific sensitivity to standard error, we could say the sensitivity of power to standard error is\n\\[\\begin{align}\n\\frac{d \\text{Power}}{dSE} &= \\frac{d \\text{Power}}{d \\delta} \\frac{d \\delta}{dSE} \\\\\n& -\\frac{\\mu_0 - \\mu_1}{SE^2} \\Bigl[ \\phi (\\delta - z_{1-\\alpha/2}) - \\phi(-\\delta - z_{1-\\alpha/2}) \\Bigr]\n\\end{align}\\]\n\npower = function(delta, se, alpha = .05) {\n  pooled_ncp = delta / se\n  crit = qnorm(1-alpha/2)\n  pnorm(-crit - pooled_ncp) + (pnorm(-crit + pooled_ncp))\n}\n\n\ndelta = .5\nse = seq(from = .1, to = 1, by = .01)\nalpha = c(.005, .01, .05, .1, .2, .3)\n\nparams = expand.grid(delta = delta, se = se, alpha = alpha)\noutput = pmap_dbl(params %&gt;% as.data.frame(), function(delta, se, alpha) {\n  power(delta, se, alpha)\n})\ncbind(params, power = output) %&gt;%\n  mutate(\n    t = delta / se,\n    crit = qnorm(1-alpha/2)\n  ) %&gt;%\n  mutate(\n    is_stat_sig = abs(delta/se) &gt;= crit\n  ) %&gt;%\n  arrange(alpha, desc(se)) %&gt;%\n  group_by(alpha) %&gt;%\n  mutate(\n    power_prev = lag(power)\n  ) %&gt;%\n  mutate(\n    incremental_power = power - power_prev\n  ) %&gt;%\n  filter(!is.na(incremental_power)) %&gt;% \n  ggplot(aes(x = t, y = power, color = is_stat_sig)) +\n  geom_point() +\n  facet_wrap(~ alpha) +\n  theme_bw(base_size = 16)\n\n\n\n\n\n\n\n\n\n\nMDE as the Inverse of Statistical Power\nMDE is the minimum detectable effect. It operates as the inverse to statistical power. Instead of asking: how much power do I have given an effect size and standard error, it asks: what is the effect size I can detect given a fixed amount of power and standard error.\nTo derive the MDE from first principles, we go back to the definition of power and how it is framed in terms of the rejection rule:\n\\[\n\\text{Power} = 1 - \\beta = P(\\delta \\geq z_{1-\\alpha/2} | H_A)\n\\]\nThis equation about the CDF can be simplified. The noncentrality parameter is distributed \\(\\delta \\sim N(0, 1)\\), so it being greater than or equal to \\(z_{1-\\alpha/2}\\) is actually just \\(1 - \\Phi(z_{1-\\alpha/2} - \\delta)\\). Then the derivation for the MDE is\n\\[\\begin{align}\n\\Phi(z_{1-\\alpha/2} - \\delta) &= \\beta \\\\\nz_{1-\\alpha/2} - \\delta &= z_{\\beta} \\\\\n\\delta &= z_{1-\\alpha/2} - z_{\\beta} \\\\\n\\Delta &= \\boxed{(z_{1-\\alpha/2} - z_{\\beta}) \\cdot SE}\n\\end{align}\\]\nUsing common numbers, \\(\\alpha = 0.05\\), \\(\\beta = 0.2\\), the rule of thumb for MDE is\n\\[\nMDE = 2.8 \\cdot SE\n\\]\nLet’s unpack this common rule of thumb explicitly. It says that the smallest effect size we can detect while ensuring an 80% power is \\(2.8 \\cdot se(\\bar{\\Delta})\\). (The SE here is the residual standard error after controlling for other variables.)\nThis looks like a similar rule of thumb related to rejecting the null: reject if the effect size is larger than 1.96 SE. This separate rule of thumb can be very confusing. It describes when can we flag a result as stat sig while ensuring a different property: that under the null hypothesis there is less than a 5% chance of generating this effect by random. This rule of thumb is offering a different guarantee, not one about power. Ensuring 80% power at a specific effect size simply says that there is a good chance we can detect these effects, but it does not make it impossible. Even using power = 50% we will correctly flag some results as stat sig. (See multiple online discussions like this one) This is why we can reject at 1.96 SE, even though the minimum detectable effect using power = 80% is 2.8 SE. However, it is noteworthy that the scientific community is advocating for rejecting at 2.8 SE (See Redefine Statistical Significance)\n\n\nSample Size Calculator\nIt’s easier for a business to think about designing an experiment around the MDE. It’s a more natural discussion: what is the smallest effect that the business would still care about?\nGiven an MDE, we pivot the problem again. If \\(se(\\bar{\\Delta}) =  \\sqrt{\\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C}} = \\sqrt{Var(\\bar{y}_c) (1 + \\frac{1}{k})}\\), and \\(n_T = k \\cdot n_C\\), then\n\\[\\begin{align}\nMDE &= (z_{1-\\alpha/2} - z_{\\beta}) \\frac{\\sigma}{\\sqrt{n_C}} \\sqrt{1 + \\frac{1}{k}} \\\\\nn_C &= \\frac{(z_{1-\\alpha/2} - z_{\\beta})^2 \\sigma^2 (\\frac{1}{k} + 1)}{MDE^2} \\\\\nn_T &= k \\cdot n_C\n\\end{align}\\]\nUsing common numbers, \\(\\alpha = 0.05\\), \\(\\beta = 0.2\\), and \\(k = 1\\) the rule of thumb for \\(n_c\\) is\n\\[\n\\boxed{n_C = n_T = \\frac{16 \\sigma^2}{MDE^2}}\n\\]"
  },
  {
    "objectID": "posts/l2_gaussian_prior/index.html",
    "href": "posts/l2_gaussian_prior/index.html",
    "title": "Tikhonov Regularization and Gaussian Priors",
    "section": "",
    "text": "In this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.\n\nIntroduction\nThroughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, \\(y\\), that is normally distributed and its mean is a linear combination of features, \\(x\\), so that \\(Y \\sim N(x^T\\beta, \\Sigma)\\).\nOptionally, the parameter vector \\(\\beta\\) can have a prior on it, in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\).\n\n\nMaximum Likelihood for Normally Distributed Data\nIn frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of \\(\\beta\\) is\n\\[ L(\\beta) = \\prod_i N(y_i | x_i, \\beta, \\Sigma) = \\prod_i \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}}\nexp({-\\frac{1}{2} (y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta)}).\\] The MLE estimate for \\(\\beta\\) will maximize the log-likelihood with respect to \\(\\beta\\), by differentiating it and finding its root. This produces the MLE estimate\n\\[\\hat{\\beta}^{MLE} = (X^T X)^{-1} X^T y.\\]\n\n\nMaximum a Posteriori\nWhen there is a gaussian prior in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\), we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of \\(\\beta\\). Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.\n\\[\\begin{align}\np(\\beta|y, x, \\Sigma) &= \\prod_i N(y_i | x_i, \\beta, \\Sigma) \\cdot N(\\beta | \\mu_0, \\Sigma_0) \\\\\n  &\\propto\n  \\prod_i \\frac{1}{|\\Sigma|}\n  exp({-\\frac{1}{2} \\big((y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta) - (\\beta - \\mu_0)^T \\Sigma_0^{-1} (\\beta - \\mu_0)\\big)}).\n\\end{align}\\]\nThe posterior turns out to be another normal distribution, \\(N(\\mu_1, \\Sigma_1)\\) (wiki), where\n\\[\\begin{align}\n\\Sigma_1 &= (\\Sigma_0^{-1} + n \\Sigma^{-1})^{-1} \\\\\n\\mu_1 &= \\Sigma_1 (\\Sigma_0^{-1} \\mu_0 + \\Sigma^{-1} \\sum_i{y_i})\n\\end{align}\\]\nThe maximum a-posteriori estimator (wiki) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives\n\\[\\hat{\\beta}^{MAP} = \\arg max_{\\beta}\n- (y-X\\beta)^T \\Sigma^{-1} (y-X\\beta)\n- (\\beta-\\beta_0)^T \\Sigma_0^{-1} (\\beta-\\beta_0).\\] Recall that \\(\\Sigma\\) is fixed, and \\(\\mu_0\\) and \\(\\Sigma_0\\) are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.\n\\[\\hat{\\beta}^{MAP} = (X^T X + \\Sigma_0)^{-1} (X^T y + \\Sigma_0 \\mu_0).\\]\n\n\nEquivalence between MLE and MAP\nWhen the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From First Principles",
    "section": "",
    "text": "Absolute truth in data is impossible. What is important is to build a communal understanding of data with others around us. In doing so, there is communal trust, and a path forward in how to make data driven decisions.\nCommunication is key, and it is crucial to not assume partners have the same knowledge of statistical models as other data scientists. This blog “From First Principles” unpacks concepts in statistics and explains them from their first principles using math and software. By building this background together, we will be well positioned to explain concepts in data, gain trust with partners, understand history, and innovate in algorihms to make intelligent systems."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "From First Principles",
    "section": "",
    "text": "Absolute truth in data is impossible. What is important is to build a communal understanding of data with others around us. In doing so, there is communal trust, and a path forward in how to make data driven decisions.\nCommunication is key, and it is crucial to not assume partners have the same knowledge of statistical models as other data scientists. This blog “From First Principles” unpacks concepts in statistics and explains them from their first principles using math and software. By building this background together, we will be well positioned to explain concepts in data, gain trust with partners, understand history, and innovate in algorihms to make intelligent systems."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "From First Principles",
    "section": "Recent posts",
    "text": "Recent posts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a local expert in causal inference and computational methods for causal inference. I have 15+ years experience doing data science research to improve business operations, promotion, and product, as well as writing software for data science systems.\nMy two main research areas are in measuring causal effects in large systems, such as AB testing, as well as optimizing products and systems based on causal effects, for example in personalization and algorithmic decision making. My work has led to two significant changes within the promotion and experimentation industries:\n\n\nIncrementality is the science of measuring the effectiveness of product promotion. This science motivates ROI analysis, audience targeting and recommendation systems. Through well designed experiments and causal effects models, we measure the effect of promotion on conversion, factoring in heterogeneous effects as well as time dynamic effects. For example, different promotions will have different causal effects on different devices, and the causal effect of a promotion can fade away over time. We are able to offer a causal effects solution to the multitouch attribution (MTA) problem that can credit a conversion to different promotion channels from a business.\n\n\n\nCausal Effect Measurement is a series of work aimed to make the measurement of average effects (ATE), heterogeneous effects (HTE), and time-dynamic effects (DTE) scalable enough to analyze a hundred million users across a hundred different experiments. By doing so, companies can gather better insights on how users interact with products, and therefore can design and implement changes to improve user joy. The combination of ATE-HTE-DTE provides deep insights into segmentation and trend analysis, and with roots in causal effects. Causal Effect Measurement is a significant milestone for product analysis. This interdisciplinary work across software engineering, data science research, and computational methods eliminated the stereotype that causal effects models could not scale to large engineering systems.\n\n\n\nI am interested in promoting an interdisciplinary community around causal inference and software for causal inference. I have given featured presentations on the topics of causal inference and attribution for different organizations, including the World Bank, Lyft, Apple, and Instacart. Please reach out to me if there is a chance to discuss these topics with your organization:\n\nIncrementality Bidding and Attribution\nComputational Causal Inference\n\n\n\n\n\nComputational Causal Inference\nIncrementality Bidding and Attribution\nEfficient Computation for Linear Model Treatment Effects\nEngineering for a science-centric experimentation platform\nReimagining Experimentation Analysis at Netflix\nSuccess Stories from a Democratized Experimentation Platform"
  },
  {
    "objectID": "about.html#promotion-incrementality-based-attribution.",
    "href": "about.html#promotion-incrementality-based-attribution.",
    "title": "About Me",
    "section": "",
    "text": "Incrementality is the science of measuring the effectiveness of product promotion. This science motivates ROI analysis, audience targeting and recommendation systems. Through well designed experiments and causal effects models, we measure the effect of promotion on conversion, factoring in heterogeneous effects as well as time dynamic effects. For example, different promotions will have different causal effects on different devices, and the causal effect of a promotion can fade away over time. We are able to offer a causal effects solution to the multitouch attribution (MTA) problem that can credit a conversion to different promotion channels from a business."
  },
  {
    "objectID": "about.html#experimentation-causal-effect-measurement.",
    "href": "about.html#experimentation-causal-effect-measurement.",
    "title": "About Me",
    "section": "",
    "text": "Causal Effect Measurement is a series of work aimed to make the measurement of average effects (ATE), heterogeneous effects (HTE), and time-dynamic effects (DTE) scalable enough to analyze a hundred million users across a hundred different experiments. By doing so, companies can gather better insights on how users interact with products, and therefore can design and implement changes to improve user joy. The combination of ATE-HTE-DTE provides deep insights into segmentation and trend analysis, and with roots in causal effects. Causal Effect Measurement is a significant milestone for product analysis. This interdisciplinary work across software engineering, data science research, and computational methods eliminated the stereotype that causal effects models could not scale to large engineering systems."
  },
  {
    "objectID": "posts/elastic_net/index.html",
    "href": "posts/elastic_net/index.html",
    "title": "The Mathematics of the Elastic Net",
    "section": "",
    "text": "Author’s note: This post is largely a rehash of many of the original elastic net and glmnet papers. I hope that having another voice describe the elegance of the elastic net will help others understand it. I have linked to all of the original documents to the best I can.\nThe elastic net adds L1 and L2 penalties to OLS, and is used to shrink coefficients towards zero. This can help with overfitting, as well as building an interpretive model from many features. When there is structure in coefficient-specific penalties, regularization can mimic a hierarchical model.\nWe start with a feature matrix, \\(X \\in \\mathbb{R}^{n \\times p}\\), a response vector, \\(y \\in \\mathbb{R}^n\\), and a given \\(\\alpha\\). The elastic net formulates the problem\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nThe first term is the usual OLS term and the second term is a combination of L1 and L2 regularization."
  },
  {
    "objectID": "posts/elastic_net/index.html#updates-via-covariance",
    "href": "posts/elastic_net/index.html#updates-via-covariance",
    "title": "The Mathematics of the Elastic Net",
    "section": "Updates via Covariance",
    "text": "Updates via Covariance\nNote that the \\(\\sum_i x_{i,j}\\varepsilon_i\\) term can be decomposed into \\(\\sum_i x_{i,j}(y_{i} - x_{i}^T \\beta)\\). This can be computed very efficiently from a few vectorized operations that are computed just once outside of all of the loops. We first compute and store \\(X^T X\\) and \\(X^T y\\). When \\(X\\) is sparse the linear algebra can be optimized. Then \\(\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta\\), i.e. the j-th component of \\(X^T y\\) and the dot product between the j-th column of \\(X^T X\\) and \\(\\beta\\)."
  },
  {
    "objectID": "posts/elastic_net/index.html#reuse-xt-y-from-searching-lambda",
    "href": "posts/elastic_net/index.html#reuse-xt-y-from-searching-lambda",
    "title": "The Mathematics of the Elastic Net",
    "section": "Reuse \\(X^T y\\) from searching \\(\\lambda\\)",
    "text": "Reuse \\(X^T y\\) from searching \\(\\lambda\\)\nWhen a smart set of \\(\\lambda\\) is initialized, we can store the product \\(X^T y\\), which is then used as part of the covariance update strategy."
  },
  {
    "objectID": "posts/elastic_net/index.html#pathwise-coordinate-descent",
    "href": "posts/elastic_net/index.html#pathwise-coordinate-descent",
    "title": "The Mathematics of the Elastic Net",
    "section": "Pathwise Coordinate Descent",
    "text": "Pathwise Coordinate Descent\nThe elastic net algorithm can compute the coefficient vector for several values of \\(\\lambda\\). Suppose we have a monotonically decreasing sequence for \\(\\lambda\\), \\({\\lambda} = {\\lambda_{max}, \\lambda_2, \\ldots}\\). By definition, the coefficient vector for \\(\\lambda_{max}\\) is the zero vector. The next \\(\\lambda\\) in the sequence will have the update step \\(\\beta^{(\\lambda)}_j = 0\\) as long as \\(|X^Ty[j]| &lt; \\lambda \\alpha n\\). This check is a simple lookup since \\(X^T y\\) is cached, and can bypass several update steps."
  },
  {
    "objectID": "posts/elastic_net/index.html#active-sets",
    "href": "posts/elastic_net/index.html#active-sets",
    "title": "The Mathematics of the Elastic Net",
    "section": "Active Sets",
    "text": "Active Sets\nAfter doing one pass on the outermost loop that iterates on cycles, we check which coefficients are nonzero. In the second cycle, instead of iterating on the \\(p\\) coefficients, we iterate only on the nonzero ones. These are the active sets. Finally, at the end we do one last cycle iterating on all coefficients. If the nonzeros have not changed, we conclude the algorithm."
  },
  {
    "objectID": "posts/elastic_net/index.html#centering-and-scaling",
    "href": "posts/elastic_net/index.html#centering-and-scaling",
    "title": "The Mathematics of the Elastic Net",
    "section": "Centering and Scaling",
    "text": "Centering and Scaling\nMuch of the elastic net algorithm assumes \\(X\\) and \\(y\\) have been centered and scaled. Say we start with a feature matrix \\(\\tilde{X}\\) which is not centered or scaled. Centering \\(\\tilde{X}\\) makes it become dense, and many sparse linear algebra optimizations are lost.\nInstead, we leverage the formula that centering and scaling can be written as\n\\[X = (\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}.\\]\nwith \\(\\mu_\\tilde{x}\\) and \\(\\sigma_\\tilde{x}\\) column vectors containing the column means and column standard deviations of \\(\\tilde{X}\\), and likewise for \\(\\tilde{y}\\).\nThe key computations can be written as:\n\\[\\begin{align}\nX^T y &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T (\\tilde{y} - 1\\mu_\\tilde{y}).\\\\\nX^T X &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} - n (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/elastic_net/index.html#coordinate-descent-with-weights",
    "href": "posts/elastic_net/index.html#coordinate-descent-with-weights",
    "title": "The Mathematics of the Elastic Net",
    "section": "Coordinate Descent with Weights",
    "text": "Coordinate Descent with Weights\nAssume that \\(X\\) and \\(y\\) have been centered and scaled without weights, so that their unweighted means are 0 and unweighted variances are 1. The update step for weighted elastic net is\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}(\\varepsilon_i + x_{i,j}\\beta_j^{(\\lambda)})), \\lambda \\alpha)}{\\sum_i w_i x_{i,j}^2 + \\lambda(1 - \\alpha)}\\]\nThough it looks more complex than before, using \\(w_i = 1/n\\) will reduce the update step to the original unweighted update step.\nNow suppose that \\(X\\) and \\(y\\) were centered and scaled with weights, so that their weighted means are 0 and weighted variances are 1. By taking advantage of the definition \\(\\sum_i w_i x_{i,j}^2 = \\sum_i w_i\\) we can recover the more familiar formula\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}\\varepsilon_i + \\beta_j^{(\\lambda)}), \\lambda \\alpha)}{\\sum_i w_i + \\lambda(1 - \\alpha)}.\\]\nLike before, this update step can use vectorized operations. The key computations can be written as:\n\\[\\begin{align}\nX^T W y &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T \\text{Diagonal}(w) ({\\tilde{y}}) \\\\\n  &= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) ({\\tilde{y}}) -\n     \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\mu_{\\tilde{X}} w^T \\tilde{y}. \\\\\nX^T W X &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [({\\tilde{X}} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T w (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) w^T \\tilde{X}\\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} +\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i. \\\\\n\\lambda_{max} &= \\max \\frac{|X^T W y|}{\\alpha}.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/elastic_net/index.html#vectorizing-for-multiple-outcome-variables",
    "href": "posts/elastic_net/index.html#vectorizing-for-multiple-outcome-variables",
    "title": "The Mathematics of the Elastic Net",
    "section": "Vectorizing for Multiple Outcome Variables",
    "text": "Vectorizing for Multiple Outcome Variables\nMany applications will track multiple outcome variables, so that \\(Y \\in \\mathbb{R}^{n \\times o}\\) is a matrix of \\(o\\) outcomes per observation. When the outcomes are independent, there is a fast way to fit multiple OLS regressions to the same feature matrix. Likewise, there is a fast way to do this for multiple elastic nets.\nThe bulk of the computation for a single \\(y\\) is in the covariance update step\n\\[\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta.\\]\n\\(y\\) and \\(\\beta\\) are column vectors. It is possible to update the j-th coefficient for all outcomes simultaneously. We vectorize over \\(o\\) outcomes to produce and cache the intermediate matrix \\(X^T Y \\in \\mathbb{R}^{p \\times o}\\), and reuse \\(X^T X\\) across outcomes.\nHowever, different outcome variables can reach convergence differently. When updating the j-th coefficient, we would like to subset the columns of \\(X^T Y\\) to those outcomes which have not converged yet. This subsetting creates a deep copy of the matrix, and can be counter productive to the vectorization over multiple outcomes.\nIn practice, it may be easier to implement a job coordinator that computes \\(X^T Y\\) and \\(X^T X\\) apriori. These intermediates are stored in shared memory. Then, the coordinator assigns the task of estimating \\(\\beta\\) for a single outcome to a worker, which reads the intermediates from shared memory."
  },
  {
    "objectID": "posts/elastic_net/index.html#differential-shrinkage",
    "href": "posts/elastic_net/index.html#differential-shrinkage",
    "title": "The Mathematics of the Elastic Net",
    "section": "Differential Shrinkage",
    "text": "Differential Shrinkage\nThe standard description of the elastic net assumes a constant penalty across all coefficients, as seen in\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nSometimes we want to augment the penalty for different coefficients. The library glmnet introduces the parameter penalty.factor, which multiplies the \\(\\lambda\\) term by a \\(\\gamma_j \\geq 0\\) that varies for different coefficients. The algorithm for solving elastic net is flexible for differential shrinkage, where the loop over coefficients scales the \\(\\lambda\\) penalty term by \\(\\gamma_j\\). In addition, the initialization of the \\(\\lambda\\) path should use\n\\[\\lambda_{max} = \\max \\text{Diagonal}(1/\\gamma) \\frac{|X^T W y|}{n \\alpha}.\\]"
  },
  {
    "objectID": "posts/power_noncompliance/index.html",
    "href": "posts/power_noncompliance/index.html",
    "title": "Statistical Power under Noncompliance",
    "section": "",
    "text": "This is an extension of a previous post on statistical power. In that post we derived the formula for statistical power using\n\nThe effect size\nThe standard error of the effect. Alternatively, this can be stated as the variance and the sample size.\n\\(\\alpha\\).\n\nHowever, there is an assumption of perfect compliance. That is, 100% of the treatment units are actually treated, and 100% of the control units are actually withheld.\nIn this post we discuss the case when \\(p_1\\)% of treatment units are treated, and \\(p_0\\)% of control units are withheld.\n\nEffect size\nSay that treatment assignment is determined by the variable \\(Z\\), so \\(Z = 1\\) means we intend to provide treatment, and \\(Z = 0\\) means we intend to withhold. Let \\(X\\) be the treatment that was actually received.\n\\[\\begin{align}\np_1 &= P(X = 1 | Z = 1) \\\\\np_0 &= P(X = 0 | Z = 0)\n\\end{align}\\]\nThe effect size we care about is \\(\\Delta = \\mu_1 - \\mu_0 = E[y | X = 1] - E[y | X = 0]\\). There is a difference between what we care about and what we will observe. From data, we will observe a dilution in the treatment effect. Noncompliance will decrease the effect size and will subsequently decrease the power.\n\\[\\begin{align}\n\\mu_1 | Z = 1 &= p_1 \\mu_1 + (1 - p_1) \\mu_0 \\\\\n\\mu_0 | Z = 0 &= p_0 \\mu_0 + (1 - p_0) \\mu_1 \\\\\n\\mu_1 | Z = 1 - \\mu_0 | Z = 0 &= (p_1 + p_0 - 1) \\Delta\n\\end{align}\\]\n\n\nVariance\nThe variance of \\(y\\) is broken into\n\\[Var(y) = E[var(y  | X)] + Var(E[y | X])\\]\nThe first term can be simplified and reduced to just \\(\\sigma^2\\). We can argue that the variance of \\(y\\) is not a function of the artificially generated assignment variable, \\(Z\\). It is only a function of whether or not the unit actually received the treatment. For simplicity, we say that the variance is independent of the treatment received.\n\\[E[var(y | X = 1)] + E[var(y | X = 0)] = \\sigma^2.\\] The conditioning on a binary X is like a mixture of bernoulli random variables. In this case the mixture has 2 equal components so it reduces to \\(\\sigma^2\\).\nNext, is the variance of a mixture of bernoulli variables: \\(E[y | X = 0] =\\mu_0\\) and \\(E[y | X = 1] = \\mu_1 = \\mu_0 + \\Delta\\). The variance of this mixture is a function of the mixing probabilities and the difference in the individual means, which in this case will be \\(\\Delta\\). Then we have the key pieces we need to measure statistical power based on the observed data\n\\[\\begin{align}\n\\mu_1 | Z = 1 &= p_1 \\mu_1 + (1 - p_1) \\mu_0 \\\\\n\\mu_0 | Z = 0 &= p_0 \\mu_0 + (1 - p_0) \\mu_1 \\\\\n\\mu_1 | Z = 1 - \\mu_0 | Z = 0 &= (p_1 + p_0 - 1) \\Delta \\\\\nVar(y | Z = 1) &= \\sigma^2 + p_1 (1 - p_1) \\Delta^2 \\approx \\sigma^2 \\\\\nVar(y | Z = 0) &= \\sigma^2 + p_0 (1 - p_0) \\Delta^2 \\approx \\sigma^2\n\\end{align}\\]\nThe approximation in \\(Var(y | Z = 1) \\approx \\sigma^2\\) comes from the pattern that variance (not normalized by \\(n\\)) tends to be large while effect size tends to be small. Thus variance under noncompliance is not much different than variance under full compliance. The change in power will largely come from the dilution in the treatment effect.\n\\[\n\\boxed{\\text{Power} = \\Phi(\\delta' - z_{1-\\alpha/2}) + \\Phi(-\\delta' - z_{1-\\alpha/2})}\n\\]\nwhere \\(\\delta' = \\delta (p_1 + p_0 - 1)\\) is a dilution on the treatment effect.\nUsing some simple numbers, if \\(p_1 = p_0 = 0.9\\), then \\(\\delta' = 0.8 \\delta\\). By changing the effect size by 0.8, we change the sample size necessary by \\(\\frac{1}{.8^2} = 56\\%\\)!"
  }
]