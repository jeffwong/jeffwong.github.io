[
  {
    "objectID": "posts/statistical_power/index.html",
    "href": "posts/statistical_power/index.html",
    "title": "Foundations in Statistical Power",
    "section": "",
    "text": "Statistical power is a concept that helps us plan for an effective experiment. Power is typically described in terms of a purely randomized and controlled trial, where we can take advantage of certain properties like independence. It is also typically described in the context of measuring the difference in means, which also has properties like the Central Limit Theorem that it takes advantage of. Finally, an assumption in typical description is that all units that are assigned to the treatment group are successfully treated, and all units assigned to the control are successfully withheld.\nBut not all experiments are so perfectly planned and executed. It is important to understand the history of how statistical power was derived, from first principles, so that we can adapt it and innovate. For example, some experiments do not have perfect compliance. Many experiments will try to give treatment to a subject, but that subject refuses, or forgets, to use the treatment. This complexity can add layers to the question: “what should we measure? The average effect or the average effect among the compliers?” With this complication we will need to revisit our formula for statistical power. We will do this innovation exercise in the next post. For now, let us understand the history of statistical power.\n\nPower is always associated with a hypothesis test. \\[\\text{Power} = 1 - \\beta = P(\\text{Reject }H_0 | H_A)\\] where \\(\\alpha\\) is the type 1 error rate, and \\(\\beta\\) is the type 2 error rate.\nSo to derive power, we must first identify the rejection rules of the hypothesis test, then evaluate the probability of rejection if the alternative is true.\nTo illustrate, we specify a very generic hypothesis test, the difference in means. While frequently associated with the t test, it is not necessarily tied to it. The hypothesis about the difference in means is\n\\[\\begin{align}\nH_0: \\mu_1 - \\mu_0 &= 0 \\\\\nH_A: \\mu_1 - \\mu_0 &\\neq 0\n\\end{align}\\]\n\nCentral Limit Theorem\nFrom the Central Limit Theorem, we know the distribution of a sample mean is \\(\\bar{X} \\sim N(\\mu, \\sigma^2 / n)\\) and the standard error on \\(\\bar{X}\\) is \\(se(\\bar{X}) = \\sigma/\\sqrt{n}\\). The Z statistic is a transformation with \\(Z = \\frac{\\bar X - \\mu}{se(\\bar{X})} \\sim N(0, 1)\\). Now we apply information from the hypothesis test, where the mean in question is the difference in two means.\n\n\nRejection Rule\nIn order to reject the null, we must first assume that the null is true. Then we must show that the sample mean for \\(\\Delta\\) is sufficiently different from 0, even when the true governing parameter is 0. This is conveniently done by working with the normalized Z statistic. In the case of the null, the sample mean \\(\\bar{\\Delta} \\sim N(\\Delta, \\sigma^2)\\) and \\(\\Delta = 0\\). Converting to a normalized Z statistic we have \\(Z = \\frac{\\bar{\\Delta}}{{SE(\\bar{\\Delta})}} \\sim N(0, 1)\\). Now, we can state the rejection rule: reject if \\(|Z| \\geq z_{1-\\alpha/2}\\). Equivalently \\[\\begin{align}\n\\left|\\frac{\\bar{\\mu}_1 - \\bar{\\mu}_0}{se(\\bar{\\Delta})}\\right| &\\geq z_{1-\\alpha/2}\n\\end{align}\\] where \\(se(\\bar{\\Delta}) = \\sqrt{\\frac{\\sigma^2}{n_T} + \\frac{\\sigma^2}{n_C}}\\) under randomization. If \\(n_T = k \\cdot n_C\\), then we can use the reduction\n\\[\\begin{align}\nVar(\\bar{y}_c) &= \\frac{\\sigma^2}{n_C} \\\\\nse(\\bar{\\Delta}) &= \\sqrt{Var(\\bar{y}_c) (1 + \\frac{1}{k})}\n\\end{align}\\]\n\n\nProbability under the Alternative\nPower is the probability of triggering the rejection rules, which were derived under the null \\(H_0: \\Delta = 0\\), when the true data generating process has \\(\\Delta \\neq 0\\). It is important to understand which governing parameter is in play here. The rejection rules will be derived using \\(\\Delta = 0\\), and those rules will be fixed. Then, we toggle the governing parameter to have \\(\\Delta \\neq 0\\), and benchmark the probability that data under this governing parameter will hit the rejection rule.\nSay that \\(H_A\\) is true and there are two distinct means \\(\\mu_0\\) and \\(\\mu_1\\) with \\(\\mu_1 - \\mu_0 = k \\neq 0\\). Under \\(H_A\\) we cannot claim the usual Z statistic is distributed N(0, 1). Instead, we have\n\\[\\begin{align}\nZ | H_A &= \\frac{\\bar{\\Delta}}{se(\\bar{\\Delta})} \\sim N(\\frac{k}{se(\\bar{\\Delta})},1) \\\\\n\\end{align}\\]\nNow we revisit the rejection rule: reject when \\(|Z| \\geq z_{1-\\alpha/2}\\). Power is the probability of triggering the rejection rule when \\(H_A\\) is true, so\n\\[\\begin{align}\nPower &= P(|Z| \\geq z_{1-\\alpha/2} | Z \\sim N(\\frac{k}{se(\\bar{\\Delta})}, 1))\n\\end{align}\\]\nLet \\(\\delta = \\frac{\\Delta | H_A}{se(\\bar{\\Delta})}\\) be the noncentrality parameter (ncp), using the anticipated effect size we would like to detect under \\(H_A\\). The ncp is a parameter we will revisit many times. The final solution for power is \\[\n\\boxed{\\text{Power} = \\Phi(\\delta - z_{1-\\alpha/2}) + \\Phi(-\\delta - z_{1-\\alpha/2})}\n\\]\nUsing first principles, we can implement code as\n\ntreatment_effect = .01\nsigma2_treatment = sigma2_control = sigma2 = 2\nn = 1e3\nalpha = .05\ntreatment_share = .5\ncontrol_share = .5\nn_treatment = n * treatment_share\nn_control = n * control_share\n\npooled_se = sqrt(sigma2_treatment / n_treatment + sigma2_control / n_control)\npooled_ncp = treatment_effect / pooled_se\n\ncrit = qnorm(1-alpha/2)\npnorm(-crit - pooled_ncp) + (pnorm(-crit + pooled_ncp))\n\n[1] 0.05143313\n\n\nThis matches the native implementation in R\n\npower.t.test(\n  n = n * treatment_share,\n  delta = treatment_effect,\n  sd = sqrt(sigma2),\n  strict = TRUE\n)\n\n\n     Two-sample t test power calculation \n\n              n = 500\n          delta = 0.01\n             sd = 1.414214\n      sig.level = 0.05\n          power = 0.05143037\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nHow Power Changes\nA lot of research in experimentation goes into methods to increase statistical power. Using the formula, the variables that drive variance are:\n\nThe treatment effect, \\(\\mu_1 - \\mu_0\\).\nThe critical value, determined by \\(\\alpha\\).\nSample size, \\(n\\).\nVariance, \\(\\sigma^2\\).\n\nBy changing the treatment effect that we want to detect, we can change power. However, that may jeopardize the practical value of an experiment. If we only power our experiment to detect large treatment effects that in practice do not happen, then the experiment is useless. Increasing \\(\\alpha\\) is also a simple change, but it also increases the false positive rate. Increasing sample size decreases the standard error, but it will take more time and resources to accumulate the extra data.\nFinally, the last path to improve power has a lot of subtlety. The role variance plays in the power formula is not fixed, it can vary. Its role is more precisely called “unexplained variance”. If there is a model that relates the metric, \\(y\\), with the treatment variable and exogeneous covariates, \\(X\\), then \\(\\sigma^2\\) is actually \\(\\text{Var}(y | X)\\), or equivalently the variance of the residuals after we use \\(X\\) to explain part of the variance. Using a good set of \\(X\\) variables, we can make the unexplained variance small.\nWhich of these levers should we pursue to get maximal power? How much will power change? To answer this, we combine \\(n\\) and variance into the more general variable, standard error. We merge the standard error and the effect size using the more general variable \\(\\delta\\). Below we plot power as \\(\\delta\\) and \\(\\alpha\\) vary. If we wanted to unpack the specific sensitivity to standard error, we could say the sensitivity of power to standard error is\n\\[\\begin{align}\n\\frac{d \\text{Power}}{dSE} &= \\frac{d \\text{Power}}{d \\delta} \\frac{d \\delta}{dSE} \\\\\n& -\\frac{\\mu_0 - \\mu_1}{SE^2} \\Bigl[ \\phi (\\delta - z_{1-\\alpha/2}) - \\phi(-\\delta - z_{1-\\alpha/2}) \\Bigr]\n\\end{align}\\]\nIt is a function of ncp and \\(\\alpha\\), and the sensitivity is charted below.\n\npower = function(delta, se, alpha = .05) {\n  pooled_ncp = delta / se\n  crit = qnorm(1-alpha/2)\n  pnorm(-crit - pooled_ncp) + (pnorm(-crit + pooled_ncp))\n}\n\n\n\n\n\n\n\n\n\n\n\n\nMDE as the Inverse of Statistical Power\nMDE is the minimum detectable effect. It operates as the inverse to statistical power. Instead of asking: how much power do I have given an effect size and standard error, it asks: what is the effect size I can detect given a fixed amount of power and standard error.\nTo derive the MDE from first principles, we go back to the definition of power and how it is framed in terms of the rejection rule:\n\\[\n\\text{Power} = 1 - \\beta = P(\\delta \\geq z_{1-\\alpha/2} | H_A)\n\\]\nThis equation about the CDF can be simplified. The noncentrality parameter is distributed \\(\\delta \\sim N(0, 1)\\), so it being greater than or equal to \\(z_{1-\\alpha/2}\\) is actually just \\(1 - \\Phi(z_{1-\\alpha/2} - \\delta)\\). Then the derivation for the MDE is\n\\[\\begin{align}\n\\Phi(z_{1-\\alpha/2} - \\delta) &= \\beta \\\\\nz_{1-\\alpha/2} - \\delta &= z_{\\beta} \\\\\n\\delta &= z_{1-\\alpha/2} - z_{\\beta} \\\\\n\\Delta &= \\boxed{(z_{1-\\alpha/2} - z_{\\beta}) \\cdot SE}\n\\end{align}\\]\nUsing common numbers, \\(\\alpha = 0.05\\), \\(\\beta = 0.2\\), the rule of thumb for MDE is\n\\[\nMDE = 2.8 \\cdot SE\n\\] or equivalently we need a ncp of \\(2.8\\).\nLet’s unpack this common rule of thumb explicitly. It says that the smallest effect size we can detect while ensuring an 80% power is \\(2.8 \\cdot se(\\bar{\\Delta})\\). (The SE here is the residual standard error after controlling for other variables.)\nThis looks like a similar rule of thumb related to rejecting the null: reject if the effect size is larger than 1.96 SE. This separate rule of thumb can be very confusing - why are there two different constants of 1.96 and 2.8? This rule describes when can we flag a result as stat sig while ensuring a different property: that under the null hypothesis there is less than a 5% chance of generating this effect by random. This rule of thumb is offering a different guarantee, not one about power. Ensuring 80% power at a specific effect size simply says that there is a good chance we can detect these effects, but it does not make it impossible. Even using power = 50% we will correctly flag some results as stat sig. (See multiple online discussions like this one) 1.96 SE is a rejection rule after the test executes, while 2.8 SE is a rule for planning to ensure 80% power. However, it is noteworthy that the scientific community is advocating for rejecting at 2.8 SE, or equivalently a p value of 0.005. (See Redefine Statistical Significance)\n\n\nSample Size Calculator\nIt’s easier for a business to think about designing an experiment around the MDE. It’s a more natural discussion: what is the smallest effect that the business would still care about?\nGiven an MDE, we pivot the problem again. If \\(se(\\bar{\\Delta}) =  \\sqrt{\\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C}} = \\sqrt{Var(\\bar{y}_c) (1 + \\frac{1}{k})}\\), and \\(n_T = k \\cdot n_C\\), then\n\\[\\begin{align}\nMDE &= (z_{1-\\alpha/2} - z_{\\beta}) \\frac{\\sigma}{\\sqrt{n_C}} \\sqrt{1 + \\frac{1}{k}} \\\\\nn_C &= \\frac{(z_{1-\\alpha/2} - z_{\\beta})^2 \\sigma^2 (\\frac{1}{k} + 1)}{MDE^2} \\\\\nn_T &= k \\cdot n_C\n\\end{align}\\]\nUsing common numbers, \\(\\alpha = 0.05\\), \\(\\beta = 0.2\\), and \\(k = 1\\) the rule of thumb for \\(n_c\\) is\n\\[\n\\boxed{n_C = n_T = \\frac{16 \\sigma^2}{MDE^2}}\n\\]"
  },
  {
    "objectID": "posts/posthoc_mde/index.html",
    "href": "posts/posthoc_mde/index.html",
    "title": "Posthoc MDE",
    "section": "",
    "text": "Credits: This post is a summary of the World Bank blog post written by David McKenzie. There is no additional innovation in the post here. It is a natural follow up to our type S/M discussion in Errors in Experiments\nDuring experiment planning, we say that we need a sample size of \\(n\\) in order to achieve 80% power. We will usually track the number of days it will take to observe \\(n\\) samples. However, in real online environments, we cannot forecast exactly how much traffic we will really get. It is tempting to monitor how much statistical power we have achieved so far. This is a bad practice, and is misleading. However, monitoring the MDE midway through a test is OK. This post will summarize why.\n\nEx post statistical power\nEx post statistical power says to plug in the observed effect size into the power formula, in place of the anticipated effect size. When we do this, ex post power is literally a function of \\(Z\\), the test statistic. At the same time, it is noisy because of the noise on the estimated effect size, as well as type M exaggeration. So stat sig results will have large ex post power, whereas results that are not stat sig will have low ex post power. This is dangerous. It can perpetuate the trap that the “lack of evidence of an effect” is not the same as “evidence for the lack of an effect”, e.g. you can and should expect that high powered tests still do not reveal stat sig results.\n\\[\n\\boxed{\\text{Power} = \\Phi(\\delta - z_{1-\\alpha/2}) + \\Phi(-\\delta - z_{1-\\alpha/2})}\n\\]\n\n\nEx post MDE\nPivoting the problem from power to MDE avoids the need to plug in the estimated effect size for the anticipated effect size. This is the procedure that we want to avoid in order to not inherit noise and type M errors.\nIn ex post MDE, we use the current sample size and the current variance to estimate the current MDE. Unlike power, there is no anticipated effect size or the need to estimate it from observed data, hence ex post MDE is more robust and is not vulnerable to type M exaggeration.\n\\[\\boxed{MDE = (z_{1-\\alpha/2} - z_{\\beta}) \\cdot SE}\\]\n\n\nReferences\n\nhttps://blogs.worldbank.org/en/impactevaluations/why-ex-post-power-using-estimated-effect-sizes-bad-ex-post-mde-not"
  },
  {
    "objectID": "posts/errors_in_experiments/index.html",
    "href": "posts/errors_in_experiments/index.html",
    "title": "Errors in Experiments",
    "section": "",
    "text": "An experiment can make many errors. What error do we care about? How do we design an experiment that can make guarantees that those errors are small? While a randomized and controlled experiment lets us report an unbiased estimate of the treatment effect, the reported effect may still\nWe’ll describe each case from first principles, and how to design an experiment when one of the errors is especially important."
  },
  {
    "objectID": "posts/errors_in_experiments/index.html#traditional-designs",
    "href": "posts/errors_in_experiments/index.html#traditional-designs",
    "title": "Errors in Experiments",
    "section": "Traditional Designs",
    "text": "Traditional Designs\nSay that we want to be able to detect a effect of 0.1. The standard deviation of the outcome is 1. We pin \\(\\alpha = 0.05\\) and \\(\\beta = 0.2\\). Data is randomly split 50/50 between treatment and control.\nThe amount of data needed was described in Foundations of Statistical Power\n\\[n_C = n_T = 2 \\frac{(z_{1-\\alpha/2} - z_{\\beta})^2 \\sigma^2}{MDE^2}\\]\nwhich produces a \\(n_C + n_T \\approx 32 \\frac{1}{(.1)^2} = 3200\\) To confirm this design has 80% power, we can verify in code\n\nn_c = 1600\nk = 1\ndelta = 0.1\nalpha = 0.05\nsigma2 = 1\n\npooled_se = sqrt(sigma2 / n_c * (1 + 1/k))\npooled_ncp = delta / pooled_se\n\ncrit = qnorm(1-alpha/2)\npower = pnorm(-crit - pooled_ncp) + (pnorm(-crit + pooled_ncp))\nprint(power)\n\n[1] 0.8074304\n\n\nWe can learn much more about this particular design other than that it achieves 80% power. It also has a \\(\\approx 0\\)% sign error rate, an exaggeration of \\(1.125\\) and false positive risk of 6% when the prior for the null is 50/50. This is a good design.\n\nexp_design_statistics(ncp = 2.8, alpha = 0.05, prior_h0_true = 0.5)\n\n  ncp alpha     power    sign_risk exaggeration        fpr required_n_c\n1 2.8  0.05 0.7995569 1.210843e-06     1.125219 0.05885421         1568\n\n\nHowever there are other configurations that can also yield 80% power. If we allow \\(\\alpha\\) to vary, then any configuration that yields an ncp of \\((z_{1 - \\alpha/2} - z_\\beta)\\) will also achieve 80%. We want to explore other configurations and see how type S errors, exaggeration, and fpr change."
  },
  {
    "objectID": "posts/errors_in_experiments/index.html#alternative-design",
    "href": "posts/errors_in_experiments/index.html#alternative-design",
    "title": "Errors in Experiments",
    "section": "Alternative Design",
    "text": "Alternative Design\nIn this section we explore alternative configurations to experimental design. We will let \\(\\alpha\\) and the ncp vary, and compute the changes to power, FPR, type S error rates, and exaggeration. We will do this with a 50/50 prior that the null is true. The results are charted below.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are patterns in the charts that are noteworthy:\n\nPower increases as ncp increases. Power also increases as \\(\\alpha\\) increases. Power is very sensitive to ncp in the range \\(ncp \\in [0.5, 2.5]\\), small improvements to the ncp can change power by a lot. In order to achieve 80% statistical power, we must plan for \\(ncp \\geq 1.5\\) or higher depending on \\(\\alpha\\).\\\nOnce \\(ncp \\geq 1.5\\), sign error rates are fairly small, regardless of \\(\\alpha\\).\nFPR decreases as ncp increases. FPR also increases when \\(\\alpha\\) increases, holding ncp constant. When \\(\\alpha\\) is small, the FPR is sensitive to small changes in the ncp, but FPR can be flat when \\(\\alpha\\) is large. To have \\(FPR \\leq 0.2\\), we need \\(ncp \\geq 1.1\\) or higher depending on \\(\\alpha\\). Similarly to have \\(FPR \\leq 0.1\\) we need \\(ncp \\geq 1.9\\) or higher.\nExaggeration decreases as ncp increases. It also decreases when \\(\\alpha\\) increases. Achieving \\(M \\leq 1.1\\) can be very hard, requiring ncp to be between 2.0 and 2.8. At high values of ncp, exaggeration is not sensitive to \\(\\alpha\\).\n\nAll of these charts are nonlinear curves. There are diminishing returns to increasing the ncp. Yet, a doubling of the ncp requires a 4x in the data volume, since \\(ncp = \\frac{\\Delta}{se(\\hat{\\Delta})}\\). Below is a table of configurations that range from “good” to “reasonably acceptable” to “bad”, and how data volume vaaries the quality.\nThe standard practice of \\(\\alpha = 0.05\\), and power = 80%, is a good practice. Many others actually even advocate for more data. However, it is important to understand how we arrived here, and whether there is flexibility when designing your own experiment. Understanding the first principles is crucial.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentation Design Statistics\n\n\n\nDesign Inputs\n\n\nDesign Statistics\n\n\n\nncp\nalpha\nrequired_n_c\npower\nsign_risk\nexaggeration\nfpr\n\n\n\n\n3.4\n0.01\n2312\n0.80\n0.00\n1.11\n0.01\n\n\n2.8\n0.05\n1568\n0.80\n0.00\n1.13\n0.06\n\n\n1.5\n0.05\n450\n0.32\n0.00\n1.74\n0.13\n\n\n1.5\n0.10\n450\n0.44\n0.00\n1.59\n0.18\n\n\n1.5\n0.20\n450\n0.59\n0.00\n1.44\n0.25\n\n\n1.5\n0.50\n450\n0.81\n0.02\n1.24\n0.38\n\n\n2.0\n0.05\n800\n0.52\n0.00\n1.39\n0.09\n\n\n2.0\n0.10\n800\n0.64\n0.00\n1.29\n0.14\n\n\n2.0\n0.20\n800\n0.76\n0.00\n1.20\n0.21\n\n\n2.0\n0.50\n800\n0.91\n0.00\n1.09\n0.35"
  },
  {
    "objectID": "posts/arrival_bias_in_accounting/index.html",
    "href": "posts/arrival_bias_in_accounting/index.html",
    "title": "Aggregations and Arrival Times Affect Convergence",
    "section": "",
    "text": "In this post we study the convergence speed of an experiment. Over time, we accumulate sample size. As that sample size increases, statistical power is supposed to increase and MDE decreases. But over time, what conditions make this guarantee? Can variance over time increase, for example due to nonstationarity, seasonality or trends? And if so, how does that affect the power that is being accumulated over time? How does the aggregation over time affect this process? What if our ability to collect samples decreases over time? We study how aggregations like means and variances, the inputs into statistical tests, are actually stochastic processes and how that affects the standard error, the key input into the MDE.\n\nAccumulating Data and Aggregating it\nDifferent cohorts of subjects enter the experiment at different times. Say that cohort \\(c\\) arrives at the experiment at time \\(A(c) = c\\), so cohort 1 arrives on day 1, cohort 2 arrives on day 2, etc. We can consider this statement widely true in different experiments, so we don’t consider any other form of arrival times. Say the size of the cohort that is arriving on day \\(t\\) is \\(n(t)\\). The experiment ends on day \\(T\\), where \\(N(T) = \\sum_{t=1}^T n(t)\\) subjects have been accumulated.\nLet \\(y_c(t)\\) be the metric value for cohort \\(c\\) at time \\(t\\), and \\(Y_c(T) = \\sum_{t=c}^T y_c(t)\\). In this sum, the cohort that enters on day 1 is observed for \\(T\\) days, while the cohort on day 2 is observed for \\(T-1\\) days and the cohort on day \\(T\\) is observed for 1 day. (Note that this is not an average over time, it is simply a sum; we will discuss how this sum has adverse effects on convergence and shine light on why averages are very useful.)\nSay that \\(y\\) is normally distributed iid so \\(y \\sim N(\\mu, \\sigma^2)\\) and for simplicity is not a function of the cohort. Then the distribution of the sum is \\(Y_c(T) \\sim N((T - t + 1)\\mu, (T - t + 1)\\sigma^2)\\) where we emphasize cohort \\(c\\) arrives at time \\(t\\). By the time the experiment ends on day \\(T\\), the data in \\(Y(T)\\) that we observe follows a mixture distribution. The \\(t^{th}\\) component has mean \\(\\mu_t = (T - t + 1)\\mu\\) and variance \\(\\sigma_t^2 = (T - t + 1)\\sigma^2\\). Using the sizes of the components, \\(n(t)\\), we can compute the overall mean and variance of this mixture distribution.\nGiven \\(T\\) components with mixture weights \\(\\pi_t\\), the mean and variance of the mixture distribution are generically\n\\[\\begin{align}\n\\tilde{\\mu}(T) &= \\sum_{t=1}^T \\pi_t \\mu_t \\\\\n\\tilde{\\sigma}^2(T) &= \\sum_{t=1}^T \\Bigl(\\pi_t (\\sigma_t^2 + \\mu_t^2)\\Bigr) - \\tilde{\\mu}(T)^2\n\\end{align}\\]\nApplying this to our data generating process, we have the system\n\\[\\begin{align}\n\\mu_t &= (T - t + 1) \\mu \\\\\n\\sigma_t^2 &= (T - t + 1) \\sigma^2 \\\\\n\\pi(t) &= \\frac{n(t)}{N(T)} \\\\\n\\tilde{\\mu}(T) &= \\sum_{t=1}^T \\pi(t) \\mu_t \\\\\n\\tilde{\\sigma}^2(T) &= \\sum_{t=1}^T \\pi(t) (\\sigma_t^2 + \\mu_t^2) - \\tilde{\\mu}(T)^2 \\\\\n\\end{align}\\]\n\n\nConvergence\nOther than the effect size, the standard error is the key statistic that drives the accumulation of statistical power. It has a lever that we can control, how much time do we dedicate to accumulate \\(n\\). The standard error process, \\(SE(T) = \\frac{\\tilde{\\sigma}^2(T)}{N(T)}\\), is charted below. We note some interesting patterns:\n\nThe standard error can decrease over time, and then increase. This is a peculiar behavior.\n\\(\\mu\\) and \\(\\sigma^2\\) influence what regime we are in, a decreasing standard error or an increasing standard error. When \\(\\sigma^2\\) is large, a small increment in \\(n\\) can tame the standard error and decrease it. However, eventually the growth in \\(\\tilde{\\sigma}^2\\) outpaces \\(n\\) causing the standard error to increase.\nIf we are accumulating \\(n(t)\\) quickly, in a superlinear way, the standard error process always goes down.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation method is important.\nThe pattern that the standard error can go up as you accumulate \\(n\\) is a strange pattern. As we illustrate, this can happen even when \\(y(t)\\) is a random variable with fixed mean. The core reason for why the SE process can increase is that \\(\\tilde{\\sigma}^2(T)\\) is growing faster than \\(N(T)\\) is growing. This is due to the unbounded growth in \\(Y_c(T)\\), combined with the form of \\(n(t)\\). When \\(n\\) is accumulating from a finite pool, the ability for \\(N(T)\\) to grow slows down whereas \\(\\tilde{\\sigma}^2(T)\\) continues to grow in an unbounded way. If \\(n\\) is accumulating from an infinite pool, or is in a regime with fast growth, it is possible to be in a regime where the standard error process is going down.\nAggregation is important. We need to prevent \\(\\tilde{\\sigma}^2(T)\\) from growing unbounded. There are multiple ways to do this. In the simple case, each component can divide the term \\((T - c + 1)\\), so that all components are identically distributed as \\(N(\\mu, \\sigma^2)\\), collapsing the mixture to a single normal distribution. Using this aggregation, \\(Y_c(t)\\) does not grow as \\(T\\) increases. At the same time, the denominator \\(N(T)\\) increases, even if slowly, creating a guarantee that the SE process decreases.\nAnother way to do this is to aggregate \\(y(t)\\) using a window with a fixed size, \\(S\\), and limiting the analysis of the experiment to subjects who have been in the experiment for at least \\(S\\) days.\n\n\nWhat if y(t) is nonstationary?\nThe above strategies can create guarantees due to the fact that the underlying \\(y(t) \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu\\) and \\(\\sigma^2\\) were constants. This created a guarantee that \\(Y_c(T)\\) would not grow unbounded. However, if \\(y(t)\\) has a trend, it is possible for \\(Y_c(T)\\) to grow unbounded again. That is content for a different post."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a local expert in causal inference and computational methods for causal inference. I have 15+ years experience doing data science research to improve business operations, promotion, and product, as well as writing software for data science systems.\nMy two main research areas are in measuring causal effects in large systems, such as AB testing, as well as optimizing products and systems based on causal effects, for example in personalization and algorithmic decision making. My work has led to two significant changes within the promotion and experimentation industries:\n\n\nIncrementality is the science of measuring the effectiveness of product promotion. This science motivates ROI analysis, audience targeting and recommendation systems. Through well designed experiments and causal effects models, we measure the effect of promotion on conversion, factoring in heterogeneous effects as well as time dynamic effects. For example, different promotions will have different causal effects on different devices, and the causal effect of a promotion can fade away over time. We are able to offer a causal effects solution to the multitouch attribution (MTA) problem that can credit a conversion to different promotion channels from a business.\n\n\n\nCausal Effect Measurement is a series of work aimed to make the measurement of average effects (ATE), heterogeneous effects (HTE), and time-dynamic effects (DTE) scalable enough to analyze a hundred million users across a hundred different experiments. By doing so, companies can gather better insights on how users interact with products, and therefore can design and implement changes to improve user joy. The combination of ATE-HTE-DTE provides deep insights into segmentation and trend analysis, and with roots in causal effects. Causal Effect Measurement is a significant milestone for product analysis. This interdisciplinary work across software engineering, data science research, and computational methods eliminated the stereotype that causal effects models could not scale to large engineering systems.\n\n\n\nI am interested in promoting an interdisciplinary community around causal inference and software for causal inference. I have given featured presentations on the topics of causal inference and attribution for different organizations, including the World Bank, Lyft, Apple, and Instacart. Please reach out to me if there is a chance to discuss these topics with your organization:\n\nIncrementality Bidding and Attribution\nComputational Causal Inference\n\n\n\n\n\nComputational Causal Inference\nIncrementality Bidding and Attribution\nEfficient Computation for Linear Model Treatment Effects\nEngineering for a science-centric experimentation platform\nReimagining Experimentation Analysis at Netflix\nSuccess Stories from a Democratized Experimentation Platform"
  },
  {
    "objectID": "about.html#promotion-incrementality-based-attribution.",
    "href": "about.html#promotion-incrementality-based-attribution.",
    "title": "About Me",
    "section": "",
    "text": "Incrementality is the science of measuring the effectiveness of product promotion. This science motivates ROI analysis, audience targeting and recommendation systems. Through well designed experiments and causal effects models, we measure the effect of promotion on conversion, factoring in heterogeneous effects as well as time dynamic effects. For example, different promotions will have different causal effects on different devices, and the causal effect of a promotion can fade away over time. We are able to offer a causal effects solution to the multitouch attribution (MTA) problem that can credit a conversion to different promotion channels from a business."
  },
  {
    "objectID": "about.html#experimentation-causal-effect-measurement.",
    "href": "about.html#experimentation-causal-effect-measurement.",
    "title": "About Me",
    "section": "",
    "text": "Causal Effect Measurement is a series of work aimed to make the measurement of average effects (ATE), heterogeneous effects (HTE), and time-dynamic effects (DTE) scalable enough to analyze a hundred million users across a hundred different experiments. By doing so, companies can gather better insights on how users interact with products, and therefore can design and implement changes to improve user joy. The combination of ATE-HTE-DTE provides deep insights into segmentation and trend analysis, and with roots in causal effects. Causal Effect Measurement is a significant milestone for product analysis. This interdisciplinary work across software engineering, data science research, and computational methods eliminated the stereotype that causal effects models could not scale to large engineering systems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From First Principles",
    "section": "",
    "text": "Absolute truth in data is impossible. What is important is to build a communal understanding of data with others around us. In doing so, there is communal trust, and a path forward in how to make data driven decisions.\nCommunication is key, and it is crucial to not assume partners have the same knowledge of statistical models as other data scientists. This blog “From First Principles” unpacks concepts in statistics and explains them from their first principles using math and software. By building this background together, we will be well positioned to explain concepts in data, gain trust with partners, understand history, and innovate in algorihms to make intelligent systems."
  },
  {
    "objectID": "index.html#about-this-blog",
    "href": "index.html#about-this-blog",
    "title": "From First Principles",
    "section": "",
    "text": "Absolute truth in data is impossible. What is important is to build a communal understanding of data with others around us. In doing so, there is communal trust, and a path forward in how to make data driven decisions.\nCommunication is key, and it is crucial to not assume partners have the same knowledge of statistical models as other data scientists. This blog “From First Principles” unpacks concepts in statistics and explains them from their first principles using math and software. By building this background together, we will be well positioned to explain concepts in data, gain trust with partners, understand history, and innovate in algorihms to make intelligent systems."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "From First Principles",
    "section": "Recent posts",
    "text": "Recent posts"
  },
  {
    "objectID": "posts/elastic_net/index.html",
    "href": "posts/elastic_net/index.html",
    "title": "The Mathematics of the Elastic Net",
    "section": "",
    "text": "Author’s note: This post is largely a rehash of many of the original elastic net and glmnet papers. I hope that having another voice describe the elegance of the elastic net will help others understand it. I have linked to all of the original documents to the best I can.\nThe elastic net adds L1 and L2 penalties to OLS, and is used to shrink coefficients towards zero. This can help with overfitting, as well as building an interpretive model from many features. When there is structure in coefficient-specific penalties, regularization can mimic a hierarchical model.\nWe start with a feature matrix, \\(X \\in \\mathbb{R}^{n \\times p}\\), a response vector, \\(y \\in \\mathbb{R}^n\\), and a given \\(\\alpha\\). The elastic net formulates the problem\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nThe first term is the usual OLS term and the second term is a combination of L1 and L2 regularization."
  },
  {
    "objectID": "posts/elastic_net/index.html#updates-via-covariance",
    "href": "posts/elastic_net/index.html#updates-via-covariance",
    "title": "The Mathematics of the Elastic Net",
    "section": "Updates via Covariance",
    "text": "Updates via Covariance\nNote that the \\(\\sum_i x_{i,j}\\varepsilon_i\\) term can be decomposed into \\(\\sum_i x_{i,j}(y_{i} - x_{i}^T \\beta)\\). This can be computed very efficiently from a few vectorized operations that are computed just once outside of all of the loops. We first compute and store \\(X^T X\\) and \\(X^T y\\). When \\(X\\) is sparse the linear algebra can be optimized. Then \\(\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta\\), i.e. the j-th component of \\(X^T y\\) and the dot product between the j-th column of \\(X^T X\\) and \\(\\beta\\)."
  },
  {
    "objectID": "posts/elastic_net/index.html#reuse-xt-y-from-searching-lambda",
    "href": "posts/elastic_net/index.html#reuse-xt-y-from-searching-lambda",
    "title": "The Mathematics of the Elastic Net",
    "section": "Reuse \\(X^T y\\) from searching \\(\\lambda\\)",
    "text": "Reuse \\(X^T y\\) from searching \\(\\lambda\\)\nWhen a smart set of \\(\\lambda\\) is initialized, we can store the product \\(X^T y\\), which is then used as part of the covariance update strategy."
  },
  {
    "objectID": "posts/elastic_net/index.html#pathwise-coordinate-descent",
    "href": "posts/elastic_net/index.html#pathwise-coordinate-descent",
    "title": "The Mathematics of the Elastic Net",
    "section": "Pathwise Coordinate Descent",
    "text": "Pathwise Coordinate Descent\nThe elastic net algorithm can compute the coefficient vector for several values of \\(\\lambda\\). Suppose we have a monotonically decreasing sequence for \\(\\lambda\\), \\({\\lambda} = {\\lambda_{max}, \\lambda_2, \\ldots}\\). By definition, the coefficient vector for \\(\\lambda_{max}\\) is the zero vector. The next \\(\\lambda\\) in the sequence will have the update step \\(\\beta^{(\\lambda)}_j = 0\\) as long as \\(|X^Ty[j]| &lt; \\lambda \\alpha n\\). This check is a simple lookup since \\(X^T y\\) is cached, and can bypass several update steps."
  },
  {
    "objectID": "posts/elastic_net/index.html#active-sets",
    "href": "posts/elastic_net/index.html#active-sets",
    "title": "The Mathematics of the Elastic Net",
    "section": "Active Sets",
    "text": "Active Sets\nAfter doing one pass on the outermost loop that iterates on cycles, we check which coefficients are nonzero. In the second cycle, instead of iterating on the \\(p\\) coefficients, we iterate only on the nonzero ones. These are the active sets. Finally, at the end we do one last cycle iterating on all coefficients. If the nonzeros have not changed, we conclude the algorithm."
  },
  {
    "objectID": "posts/elastic_net/index.html#centering-and-scaling",
    "href": "posts/elastic_net/index.html#centering-and-scaling",
    "title": "The Mathematics of the Elastic Net",
    "section": "Centering and Scaling",
    "text": "Centering and Scaling\nMuch of the elastic net algorithm assumes \\(X\\) and \\(y\\) have been centered and scaled. Say we start with a feature matrix \\(\\tilde{X}\\) which is not centered or scaled. Centering \\(\\tilde{X}\\) makes it become dense, and many sparse linear algebra optimizations are lost.\nInstead, we leverage the formula that centering and scaling can be written as\n\\[X = (\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}.\\]\nwith \\(\\mu_\\tilde{x}\\) and \\(\\sigma_\\tilde{x}\\) column vectors containing the column means and column standard deviations of \\(\\tilde{X}\\), and likewise for \\(\\tilde{y}\\).\nThe key computations can be written as:\n\\[\\begin{align}\nX^T y &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T (\\tilde{y} - 1\\mu_\\tilde{y}).\\\\\nX^T X &= [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [(\\tilde{X} - 1\\mu_\\tilde{x}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} - n (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/elastic_net/index.html#coordinate-descent-with-weights",
    "href": "posts/elastic_net/index.html#coordinate-descent-with-weights",
    "title": "The Mathematics of the Elastic Net",
    "section": "Coordinate Descent with Weights",
    "text": "Coordinate Descent with Weights\nAssume that \\(X\\) and \\(y\\) have been centered and scaled without weights, so that their unweighted means are 0 and unweighted variances are 1. The update step for weighted elastic net is\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}(\\varepsilon_i + x_{i,j}\\beta_j^{(\\lambda)})), \\lambda \\alpha)}{\\sum_i w_i x_{i,j}^2 + \\lambda(1 - \\alpha)}\\]\nThough it looks more complex than before, using \\(w_i = 1/n\\) will reduce the update step to the original unweighted update step.\nNow suppose that \\(X\\) and \\(y\\) were centered and scaled with weights, so that their weighted means are 0 and weighted variances are 1. By taking advantage of the definition \\(\\sum_i w_i x_{i,j}^2 = \\sum_i w_i\\) we can recover the more familiar formula\n\\[\\beta_j^{(\\lambda)} = \\frac{S(\\sum_{i=1}^n (w_i x_{i,j}\\varepsilon_i + \\beta_j^{(\\lambda)}), \\lambda \\alpha)}{\\sum_i w_i + \\lambda(1 - \\alpha)}.\\]\nLike before, this update step can use vectorized operations. The key computations can be written as:\n\\[\\begin{align}\nX^T W y &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T \\text{Diagonal}(w) ({\\tilde{y}}) \\\\\n  &= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) ({\\tilde{y}}) -\n     \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\mu_{\\tilde{X}} w^T \\tilde{y}. \\\\\nX^T W X &= [(\\tilde{X} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}]^T [({\\tilde{X}} - 1\\mu_{\\tilde{X}}^T) \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix}] \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T w (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) w^T \\tilde{X}\\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} +\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i \\\\\n&= \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} \\tilde{X}^T \\text{Diagonal}(w) \\tilde{X} \\begin{bmatrix} 1/\\sigma_{\\tilde{x}, 1} & & \\\\ & \\ddots & \\\\ & & 1/\\sigma_{\\tilde{x}, p} \\end{bmatrix} -\n  (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}}) (\\frac{\\mu_\\tilde{x}}{\\sigma_\\tilde{x}})^T \\sum_i w_i. \\\\\n\\lambda_{max} &= \\max \\frac{|X^T W y|}{\\alpha}.\n\\end{align}\\]"
  },
  {
    "objectID": "posts/elastic_net/index.html#vectorizing-for-multiple-outcome-variables",
    "href": "posts/elastic_net/index.html#vectorizing-for-multiple-outcome-variables",
    "title": "The Mathematics of the Elastic Net",
    "section": "Vectorizing for Multiple Outcome Variables",
    "text": "Vectorizing for Multiple Outcome Variables\nMany applications will track multiple outcome variables, so that \\(Y \\in \\mathbb{R}^{n \\times o}\\) is a matrix of \\(o\\) outcomes per observation. When the outcomes are independent, there is a fast way to fit multiple OLS regressions to the same feature matrix. Likewise, there is a fast way to do this for multiple elastic nets.\nThe bulk of the computation for a single \\(y\\) is in the covariance update step\n\\[\\sum_i x_{i,j}\\varepsilon_i = (X^T y)[j] - (X^T X)[,j]^T\\beta.\\]\n\\(y\\) and \\(\\beta\\) are column vectors. It is possible to update the j-th coefficient for all outcomes simultaneously. We vectorize over \\(o\\) outcomes to produce and cache the intermediate matrix \\(X^T Y \\in \\mathbb{R}^{p \\times o}\\), and reuse \\(X^T X\\) across outcomes.\nHowever, different outcome variables can reach convergence differently. When updating the j-th coefficient, we would like to subset the columns of \\(X^T Y\\) to those outcomes which have not converged yet. This subsetting creates a deep copy of the matrix, and can be counter productive to the vectorization over multiple outcomes.\nIn practice, it may be easier to implement a job coordinator that computes \\(X^T Y\\) and \\(X^T X\\) apriori. These intermediates are stored in shared memory. Then, the coordinator assigns the task of estimating \\(\\beta\\) for a single outcome to a worker, which reads the intermediates from shared memory."
  },
  {
    "objectID": "posts/elastic_net/index.html#differential-shrinkage",
    "href": "posts/elastic_net/index.html#differential-shrinkage",
    "title": "The Mathematics of the Elastic Net",
    "section": "Differential Shrinkage",
    "text": "Differential Shrinkage\nThe standard description of the elastic net assumes a constant penalty across all coefficients, as seen in\n\\[\\beta^{(\\lambda)} = \\arg\\min \\sum_{i=1}^n (y_i -\\beta_0 -x_i^T \\beta)^2 + \\lambda \\sum_{j=1}^p (0.5(1-\\alpha)\\beta_j^2 + \\alpha |\\beta_j|).\\]\nSometimes we want to augment the penalty for different coefficients. The library glmnet introduces the parameter penalty.factor, which multiplies the \\(\\lambda\\) term by a \\(\\gamma_j \\geq 0\\) that varies for different coefficients. The algorithm for solving elastic net is flexible for differential shrinkage, where the loop over coefficients scales the \\(\\lambda\\) penalty term by \\(\\gamma_j\\). In addition, the initialization of the \\(\\lambda\\) path should use\n\\[\\lambda_{max} = \\max \\text{Diagonal}(1/\\gamma) \\frac{|X^T W y|}{n \\alpha}.\\]"
  },
  {
    "objectID": "posts/l2_gaussian_prior/index.html",
    "href": "posts/l2_gaussian_prior/index.html",
    "title": "Tikhonov Regularization and Gaussian Priors",
    "section": "",
    "text": "In this post we will show that the maximum a-posteriori (MAP) estimator of a normal-normal is equal to the estimator from Tikhonov regularization.\n\nIntroduction\nThroughout this post we will build on ordinary least squares. First, we will assume that there is a random variable, \\(y\\), that is normally distributed and its mean is a linear combination of features, \\(x\\), so that \\(Y \\sim N(x^T\\beta, \\Sigma)\\).\nOptionally, the parameter vector \\(\\beta\\) can have a prior on it, in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\).\n\n\nMaximum Likelihood for Normally Distributed Data\nIn frequentist statistics, we will write the likelihood of the data, then find an estimate of the parameters that will maximize the likelihood. The likelihood as a function of \\(\\beta\\) is\n\\[ L(\\beta) = \\prod_i N(y_i | x_i, \\beta, \\Sigma) = \\prod_i \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}}\nexp({-\\frac{1}{2} (y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta)}).\\] The MLE estimate for \\(\\beta\\) will maximize the log-likelihood with respect to \\(\\beta\\), by differentiating it and finding its root. This produces the MLE estimate\n\\[\\hat{\\beta}^{MLE} = (X^T X)^{-1} X^T y.\\]\n\n\nMaximum a Posteriori\nWhen there is a gaussian prior in the form \\(\\beta \\sim N(\\mu_0, \\Sigma_0)\\), we use Baye’s rule to multiply the likelihood with the prior to get the posterior probability of \\(\\beta\\). Since we are multiplying two normals, we can add their exponents. The posterior takes the form of another normal distribution.\n\\[\\begin{align}\np(\\beta|y, x, \\Sigma) &= \\prod_i N(y_i | x_i, \\beta, \\Sigma) \\cdot N(\\beta | \\mu_0, \\Sigma_0) \\\\\n  &\\propto\n  \\prod_i \\frac{1}{|\\Sigma|}\n  exp({-\\frac{1}{2} \\big((y_i - x_i^T \\beta)^T \\Sigma^{-1} (y_i - x_i^T \\beta) - (\\beta - \\mu_0)^T \\Sigma_0^{-1} (\\beta - \\mu_0)\\big)}).\n\\end{align}\\]\nThe posterior turns out to be another normal distribution, \\(N(\\mu_1, \\Sigma_1)\\) (wiki), where\n\\[\\begin{align}\n\\Sigma_1 &= (\\Sigma_0^{-1} + n \\Sigma^{-1})^{-1} \\\\\n\\mu_1 &= \\Sigma_1 (\\Sigma_0^{-1} \\mu_0 + \\Sigma^{-1} \\sum_i{y_i})\n\\end{align}\\]\nThe maximum a-posteriori estimator (wiki) estimates the parameter vector as the mode of the posterior distribution. This is done by differentiating the posterior and solvings its root, similar to MLE. Taking the log posterior probability and then maximizing it gives\n\\[\\hat{\\beta}^{MAP} = \\arg max_{\\beta}\n- (y-X\\beta)^T \\Sigma^{-1} (y-X\\beta)\n- (\\beta-\\beta_0)^T \\Sigma_0^{-1} (\\beta-\\beta_0).\\] Recall that \\(\\Sigma\\) is fixed, and \\(\\mu_0\\) and \\(\\Sigma_0\\) are inputs for the prior. Differentiating and solving, we can show the MAP estimator is equal to Tikhonov regularization.\n\\[\\hat{\\beta}^{MAP} = (X^T X + \\Sigma_0)^{-1} (X^T y + \\Sigma_0 \\mu_0).\\]\n\n\nEquivalence between MLE and MAP\nWhen the prior is a constant everywhere, it factors out of the posterior probability as a constant. That means the MLE estimator is a special case of MAP when the prior is a uniform distribution."
  },
  {
    "objectID": "posts/power_noncompliance/index.html",
    "href": "posts/power_noncompliance/index.html",
    "title": "Statistical Power under Noncompliance",
    "section": "",
    "text": "This is an extension of a previous post on statistical power. In that post we derived the formula for statistical power using\n\nThe effect size\nThe standard error of the effect. Alternatively, this can be stated as the variance and the sample size.\n\\(\\alpha\\).\n\nHowever, there is an assumption of perfect compliance. That is, 100% of the treatment units are actually treated, and 100% of the control units are actually withheld.\nIn this post we discuss the case when \\(p_1\\)% of treatment units are treated, and \\(p_0\\)% of control units are withheld.\n\nEffect size\nSay that treatment assignment is determined by the variable \\(Z\\), so \\(Z = 1\\) means we intend to provide treatment, and \\(Z = 0\\) means we intend to withhold. Let \\(X\\) be the treatment that was actually received.\n\\[\\begin{align}\np_1 &= P(X = 1 | Z = 1) \\\\\np_0 &= P(X = 0 | Z = 0)\n\\end{align}\\]\nThe effect size we care about is \\(\\Delta = \\mu_1 - \\mu_0 = E[y | X = 1] - E[y | X = 0]\\). There is a difference between what we care about and what we will observe. From data, we will observe a dilution in the treatment effect. Noncompliance will decrease the effect size and will subsequently decrease the power.\n\\[\\begin{align}\n\\mu_1 | Z = 1 &= p_1 \\mu_1 + (1 - p_1) \\mu_0 \\\\\n\\mu_0 | Z = 0 &= p_0 \\mu_0 + (1 - p_0) \\mu_1 \\\\\n\\mu_1 | Z = 1 - \\mu_0 | Z = 0 &= (p_1 + p_0 - 1) \\Delta\n\\end{align}\\]\n\n\nVariance\nThe variance of \\(y\\) is broken into\n\\[Var(y) = E[var(y  | X)] + Var(E[y | X])\\]\nThe first term can be simplified and reduced to just \\(\\sigma^2\\). We can argue that the variance of \\(y\\) is not a function of the artificially generated assignment variable, \\(Z\\). It is only a function of whether or not the unit actually received the treatment. For simplicity, we say that the variance is independent of the treatment received.\n\\[E[var(y | X = 1)] + E[var(y | X = 0)] = \\sigma^2.\\] The conditioning on a binary X is like a mixture of bernoulli random variables. In this case the mixture has 2 equal components so it reduces to \\(\\sigma^2\\).\nNext, is the variance of a mixture of bernoulli variables: \\(E[y | X = 0] =\\mu_0\\) and \\(E[y | X = 1] = \\mu_1 = \\mu_0 + \\Delta\\). The variance of this mixture is a function of the mixing probabilities and the difference in the individual means, which in this case will be \\(\\Delta\\). Then we have the key pieces we need to measure statistical power based on the observed data\n\\[\\begin{align}\n\\mu_1 | Z = 1 &= p_1 \\mu_1 + (1 - p_1) \\mu_0 \\\\\n\\mu_0 | Z = 0 &= p_0 \\mu_0 + (1 - p_0) \\mu_1 \\\\\n\\mu_1 | Z = 1 - \\mu_0 | Z = 0 &= (p_1 + p_0 - 1) \\Delta \\\\\nVar(y | Z = 1) &= \\sigma^2 + p_1 (1 - p_1) \\Delta^2 \\approx \\sigma^2 \\\\\nVar(y | Z = 0) &= \\sigma^2 + p_0 (1 - p_0) \\Delta^2 \\approx \\sigma^2\n\\end{align}\\]\nThe approximation in \\(Var(y | Z = 1) \\approx \\sigma^2\\) comes from the pattern that variance (not normalized by \\(n\\)) tends to be large while effect size tends to be small. Thus variance under noncompliance is not much different than variance under full compliance. The change in power will largely come from the dilution in the treatment effect.\n\\[\n\\boxed{\\text{Power} = \\Phi(\\delta' - z_{1-\\alpha/2}) + \\Phi(-\\delta' - z_{1-\\alpha/2})}\n\\]\nwhere \\(\\delta' = \\delta (p_1 + p_0 - 1)\\) is a dilution on the treatment effect.\nUsing some simple numbers, if \\(p_1 = p_0 = 0.9\\), then \\(\\delta' = 0.8 \\delta\\). By changing the effect size by 0.8, we change the sample size necessary by \\(\\frac{1}{.8^2} = 56\\%\\)!"
  }
]